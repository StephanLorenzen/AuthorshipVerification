0;In Chapter 1 I defined a database to be “… an organised, machine-readable collection of symbols, to be$NL$interpreted as a true account of some enterprise.” I also gave this example (extracted from Figure 1.1):$NL$I suggested that those green symbols, organised as they are with respect to the blue ones, might be$NL$understood to mean:$NL$“Student S1, named Anne, is enrolled on course C1.”$NL$In this chapter I explain exactly how such an interpretation can be justified. In fact, I describe the general$NL$method under which data organized in the form of relations is to be interpretedto yield information, as$NL$some people say. This method of interpretation is firmly based in the science of logic. Relational database$NL$theory is based very directly on logic. Predicates and propositions are the fundamental concepts that logic$NL$deals with.$NL$Fortunately, we need to understand only the few basic principles on which logic is founded. You may well$NL$already have a good grasp of the principles in question, but even if you do, please do not skip this chapter.$NL$For one thing, the textbooks on logic do not all use exactly the same terminology and I have chosen the$NL$terms and definitions that seem most suitable for the purpose at hand. For another, I do of course$NL$concentrate on the points that are particularly relevant to relational theory you need to know which points$NL$those are and to understand exactly why they are so relevant.$NL$Predicates, one might say, are what logic is all about. And yet the textbooks do not speak with one voice$NL$when it comes to pinning down exactly what the term refers to! I choose the definition that appears to me$NL$to fit best, so to speak, with relational database theory. We start by looking again at that possible$NL$interpretation of the symbols S1, Anne, and C1, placed the way they are in Figure 1.1:$NL$“Student S1, named Anne, is enrolled on course C1.”$NL$This is a sentence. Sentences are what human beings typically use to communicate with each other, using$NL$language. We express our interpretations of the data using sentences in human language and we use$NL$relations to organize the data to be interpreted. Logic bridges the gap between relations and sentences.$NL$Our example sentence can be recast into two simpler sentences, “Student S1 is named Anne” and “Student$NL$S1 is enrolled on course C1”. Let’s focus on the second:$NL$The symbols S1 and C1 appear both in this sentence and in the data whose meaning it expresses. Because$NL$they each designate, or refer to, a particular thingS1 a particular student, C1 a particular coursethey$NL$are called designators. The word Anne is another designator, referring to a particular forename. “An$NL$Introduction to Relational Database Theory” is also a designator, referring to a particular book, and so is,$NL$for example, -7, referring to a particular number.$NL$Now, suppose we replace S1 and C1 in Example 3.1 by another pair of symbols, taken from the same$NL$columns of Figure 1.1 but a different row. Then we might obtain$NL$A pattern is clearly emerging. For every row in Figure 1.1, considering just the columns headed StudentId$NL$and CourseId, we can obtain a sentence in the form of Examples 3.1 and 3.2. The words “Student … is$NL$enrolled on course …” appear in that order in each case and in each case the gaps indicated$NL$by …sometimes called placeholdersare replaced by appropriate designators. If we now replace each$NL$placeholder by the name given in the heading of the column from which the appropriate designator is to be$NL$drawn, we obtain this:$NL$Example 3.3 succinctly expresses the way in which the named columns in each row of Figure 1.1 are$NL$probably to be interpreted. And we now know that those names, StudentId and CourseId, in the column$NL$headings are the names of two of the attributes of the relation that Figure 1.1 depicts in tabular form.$NL$Now, the sentences in Examples 3.1 and 3.2 are in fact statements. They state something of which it can$NL$be said, “That is true”, or “That is not true”, or “I believe that”, or “I don’t believe that”.$NL$Not all sentences are statements. A good informal test, in English, to determine whether a sentence is a$NL$statement is to place “Is it true that” in front of it. If the result is a grammatical English question, then the$NL$original sentence is indeed a statement otherwise it is not. Here are some sentences that are not statements:$NL$•“Let’s all get drunk.”$NL$•“Will you marry me?”$NL$•“Please pass me the salt.”$NL$•“If music be the food of love, play on.”$NL$They each fail the test. In fact one of them is a question itself and the other three are imperatives, but we$NL$have no need of such sentences in our interpretation of relations because we seek only information, in the$NL$form of statementsstatements that we are prepared to believe are statements of fact in other words,$NL$statements we believe to be true. We do not expect a database to be interpreted as asking questions or$NL$giving orders. We expect it to be stating facts (or at least what are believed to be facts). As an aside, I must own up to the fact that some sentences that would be accepted as statements in English$NL$don’t really pass the test as they stand. Here are two cases in point, from Shakespeare:$NL$•“O for a muse of fire that would ascend the highest heaven of invention.”$NL$•“To be or not to bethat is the question.”$NL$The first appears to lack a verb, but we know that “O for a …” is a poetical way of expressing a wish for$NL$something on the part of the speaker, so we can paraphrase it fairly accurately by replacing “O” by “I$NL$wish”, and the sentence thus revised passes the test. In the second case we have only to delete the word$NL$“that”, whose presence serves only for emphasis (and scansion, of course!), and alter the punctuation$NL$slightly: “It is true that ‘to be or not to be?’ is the question.”$NL$Now, a statement is a sentence that is declarative in form: it declares something that is supposed to be true.$NL$Example 3.3, “Student StudentId is enrolled on course CourseId”, is not a statementit does not pass the$NL$test. It does, however, have the grammatical form of a statement. We can say that, like a statement, it is$NL$declarative in form.
0;This chapter gives a very broad overview of$NL$•what a database is$NL$•what a relational database is, in particular$NL$•what a database management system (DBMS) is$NL$•what a DBMS does$NL$•how a relational DBMS does what a DBMS does$NL$We start to familiarise ourselves with terminology and notation used in the remainder of the book, and we$NL$get a brief introduction to each topic that is covered in more detail in later sections.$NL$$NL$You will find many definitions of this term if you look around the literature and the Web. At one time (in$NL$2008), Wikipedia [1] offered this: “A structured collection of records or data.” I prefer to elaborate a little:$NL$$NL$The organized, machine-readable collection of symbols is what you “see” if you “look at” a database at a$NL$particular point in time. It is to be interpreted as a true account of the enterprise at that point in time. Of$NL$course it might happen to be incorrect, incomplete or inaccurate, so perhaps it is better to say that the$NL$account is believed to be true.$NL$The alternative view of a database as a collection of variables reflects the fact that the account of the$NL$enterprise has to change from time to time, depending on the frequency of change in the details we choose$NL$to include in that account.$NL$The suitability of a particular kind of database (such as relational, or object-oriented) might depend to$NL$some extent on the requirements of its user(s). When E.F. Codd developed his theory of relational$NL$databases (first published in 1969), he sought an approach that would satisfy the widest possible ranges of$NL$users and uses. Thus, when designing a relational database we do so without trying to anticipate specific$NL$uses to which it might be put, without building in biases that would favour particular applications. That is$NL$perhaps the distinguishing feature of the relational approach, and you should bear it in mind as we explore$NL$some of its ramifications.$NL$$NL$For example, the table in Figure 1.1 shows an organized collection of symbols.$NL$$NL$Can you guess what this tabular arrangement of symbols might be trying to tell us? What might it mean,$NL$for symbols to appear in the same row? In the same column? In what way might the meaning of the$NL$symbols in the very first row (shown in blue) differ from the meaning of those below them?$NL$Do you intuitively guess that the symbols below the first row in the first column are all student$NL$identifiers, those in the second column names of students, and those in the third course identifiers? Do$NL$you guess that student S1’s name is Anne? And that Anne is enrolled on courses C1 and C2? And that$NL$Cindy is enrolled on neither of those two courses? If so, what features of the organization of the symbols$NL$led you to those guesses?$NL$Remember those features. In an informal way they form the foundation of relational theory. Each of them$NL$has a formal counterpart in relational theory, and those formal counterparts are the only constituents of the$NL$organized structure that is a relational database.$NL$$NL$Perhaps those green symbols, organized as they are with respect to the blue ones, are to be$NL$understood to mean:$NL$“Student S1, named Anne, is enrolled on course C1.”$NL$An important thing to note here is that only certain symbols from the sentence in quotes appear in the$NL$table—S1, Anne, and C1. None of the other words appear in the table. The symbols in the top row of$NL$the table (presumably column headings, though we haven’t actually been told that) might help us to$NL$guess “student”, “named”, and “course”, but nothing in the table hints at “enrolled”. And even if those$NL$assumed column headings had been A, B and C, or X, Y and Z, the given interpretation might still be$NL$the intended one.$NL$Now, we can take the sentence “Student S1, named Anne, is enrolled on course C1” and replace$NL$each of S1, Anne, and C1 by the corresponding symbols taken from some other row in the table, such$NL$as S2, Boris, and C1. In so doing, we are applying exactly the same mode of interpretation to each row.$NL$If that is indeed how the table is meant to be interpreted, then we can conclude that the following$NL$sentences are all true:$NL$Student S1, named Anne, is enrolled on course C1.$NL$Student S1, named Anne, is enrolled on course C2.$NL$Student S2, named Boris, is enrolled on course C1.$NL$Student S3, named Cindy, is enrolled on course C3.$NL$$NL$In Chapter 3, “Predicates and Propositions”, we shall see exactly how such interpretations can be$NL$systematically formalized. In Chapter 4, “Relational AlgebraThe Foundation”, and Chapter 5, “Building$NL$on The Foundation”, we shall see how they help us to formulate correct queries to derive useful$NL$information from a relational database.$NL$$NL$We have added the name, ENROLMENT, above the table, and we have added an extra row.$NL$ENROLMENT is a variable. Perhaps the table we saw earlier was once its value. If so, it (the variable) has$NL$been updated since thenthe row for S4 has been added. Our interpretation of Figure 1.1 now has to be$NL$revised to include the sentence represented by that additional row:$NL$Student S1, named Anne, is enrolled on course C1.$NL$Student S1, named Anne, is enrolled on course C2.$NL$Student S2, named Boris, is enrolled on course C1.$NL$Student S3, named Cindy, is enrolled on course C3.$NL$Student S4, named Devinder, is enrolled on course C1.$NL$Notice that in English we can join all these sentences together to form a single sentence, using$NL$conjunctions like “and”, “or”, “because” and so on. If we join them using “and” in particular, we get a$NL$single sentence that is logically equivalent to the given set of sentences in the sense that it is true if each$NL$one of them is true (and false if any one of them is false). A database, then, can be thought of as a$NL$representation of an account of the enterprise expressed as a single sentence! (But it’s more usual to think$NL$in terms of a collection of individual sentences.)$NL$We might also be able to conclude that the following sentences (for example) are false:$NL$Student S2, named Boris, is enrolled on course C2.$NL$Student S2, named Beth, is enrolled on course C1.
0;In this chapter we look at the four fundamental concepts on which most computer languages are based.$NL$We acquire some useful terminology to help us talk about these concepts in a precise way, and we begin$NL$to see how the concepts apply to relational database languages in particular. It is quite possible that you$NL$are already very familiar with these conceptsindeed, if you have done any computer programming they$NL$cannot be totally new to youbut I urge you to study the chapter carefully anyway, as not everybody$NL$uses exactly the same terminology (and not everybody is as careful about their use of terminology as we$NL$need to be in the present context). And in any case I also define some special terms, introduced by C.J.$NL$Date and myself in the 1990s, which have perhaps not yet achieved wide usagefor example, selector$NL$and possrep.$NL$I wrote “most computer languages” because some languages dispense with variables. Database languages$NL$typically do not dispense with variables because it seems to be the very nature of what we call a database$NL$that it varies over time in keeping with changes in the enterprise. Money changes hands, employees come$NL$and go, get salary rises, change jobs, and so on. A language that supports variables is said to be an$NL$imperative language (and one that does not is a functional language). The term “imperative” appeals to the$NL$notion of commands that such a language needs for purposes such as updating variables. A command is an$NL$instruction, written in some computer language, to tell the system to do something. The terms statement$NL$(very commonly) and imperative (rarely) are used instead of command. In this book I use statement quite$NL$frequently, bowing to common usage, but I really prefer command because it is more appropriate also, in$NL$normal discourse statement refers to a sentence of the very important kind described in Chapter 3 and does$NL$not instruct anybody to do anything.$NL$$NL$Figure 2.1 shows a simple commandthe assignment, Y := X + 1dissected into its component parts.$NL$The annotations show the terms we use for those components.$NL$$NL$It is important to distinguish carefully between the concepts and the language constructs that$NL$represent (denote) those concepts. It is the distinction between what is written and what it meanssyntax$NL$and semantics.$NL$$NL$Each annotated component in Figure 1 is an example of a certain language construct. The annotation$NL$shows the term used for the language construct and also the term for the concept it denotes. Honouring$NL$this distinction at all times can lead to laborious prose. Furthermore, we don’t always have distinct terms$NL$for the language construct and the corresponding concept. For example, there is no single-word term for$NL$an expression denoting an argument. We can write “argument expression” when we need to be absolutely$NL$clear and there is any danger of ambiguity, but normally we would just say, for example, that X+1 is an$NL$argument to that invocation of the operator “:=” shown in Figure 2.1. (The real argument is the result of$NL$evaluating X+1.)$NL$The update operator “:=” is known as assignment. The command Y := X+1 is an invocation of$NL$assignment, often referred to as just an assignment. The effect of that assignment is to evaluate the$NL$expression X+1, yielding some numerical result r and then to assign r to the variable Y. Subsequent$NL$references to Y therefore yield r (until some command is given to assign something else to Y).$NL$Note the two operands of the assignment: Y is the target, X+1 the source. The terms target and source$NL$here are names for the parameters of the operator. In the example, the argument expression Y is$NL$substituted for the parameter target and the argument expression X+1 is substituted for the parameter$NL$source. We say that target is subject to update, meaning that any argument expression substituted for it$NL$must denote a variable. The other parameter, source, is not subject to update, so any argument expression$NL$substituted must denote a value, not a variable. Y denotes a variable and X+1 denotes a value. When the$NL$assignment is evaluated (or, as we sometimes say of commands, executed), the variable denoted by Y$NL$becomes the argument substituted for target, and the current value of X+1 becomes the argument$NL$substituted for source.$NL$Whereas the Y in Y := X + 1 denotes a variable, as I have explained, the X in Y := X + 1 does not,$NL$as I am about to explain. So now let’s analyse the expression X+1. It is an invocation of the read-only$NL$operator +, which has two parameters, perhaps named a and b. Neither a nor b is subject to update. A$NL$read-only operator is one that has no parameter that is subject to update. Evaluation of an invocation of a$NL$read-only operator yields a value and updates nothing. The arguments to the invocation, in this example$NL$denoted by the expressions X and 1, are the values denoted by those two expressions. 1 is a literal,$NL$denoting the numerical value that it always denotes X is a variable reference, denoting the value currently$NL$assigned to X.$NL$A literal is an expression that denotes a value and does not contain any variable references. But we do not$NL$use that term for all such expressions: for example, the expression 1+2, denoting the number 3, is not a$NL$literal. I defer a precise definition of literal to later in the present chapter.$NL$$NL$The following very important distinctions emerge from the previous section and should be firmly$NL$taken on board:$NL$•Syntax versus semantics$NL$•Value versus variable$NL$•Variable versus variable reference$NL$•Update operator versus read-only operator$NL$•Operator versus invocation$NL$•Parameter versus argument$NL$•Parameter subject to update versus parameter not subject to update$NL$Each of these distinctions is illustrated in Figure 2.1, as follows:$NL$•Value versus variable: Y denotes a variable, X denotes the value currently assigned to the$NL$variable X. 1 denotes a value. Although X and Y are both symbols referencing variables, what they$NL$denote depends in the context in which those references appear. Y appears as an update target and$NL$thus denotes the variable of that name, whereas X appears where an expression denoting a value is$NL$expected and that position denotes the current value of the referenced variable. Note that variables,$NL$by definition, are subject to change (in value) from time to time. A value, by contrast, exists$NL$independently of time and space and is not subject to change.
0;I should make it clear right away that to most people the term “normalization”, in the context of databases,$NL$means projection-join normalization specifically. But the term does seem to be equally appropriate in$NL$connection with other kinds of design choice, hence my decision to use it in the first two sections of$NL$this chapter.$NL$I promised a discussion of 1NF in this section. It is here, but later.$NL$Some examples in this chapter refer to the relvars COURSE and EXAM_MARK introduced at the beginning$NL$of Chapter 5. Their current values are repeated here in Figure 8.1.$NL$And here again are the relvar definitions for COURSE and EXAM_MARK:$NL$In Chapter 5, Example 5.12 illustrates the Tutorial D operator GROUP. The expression$NL$EXAM_MARK GROUP ( { StudentId, Mark } AS ExamResult ), operating on the current$NL$value of EXAM_MARK, yields the relation shown in Figure 5.6, repeated here as Figure 8.2 for$NL$convenience:$NL$Recall also that we can reverse the process by use of UNGROUP. Thus, a relvar defined as in Example 8.1$NL$might be considered as a valid alternative to EXAM_MARK, but note the constraints needed to make the$NL$“grouped” design genuinely equivalent.$NL$The first of those two constraints reflects the fact that EXAM_MARK by itself cannot accommodate a$NL$course for which nobody sat the exam. It would probably make better sense to disregard that putative$NL$requirement and drop the constraint. The second constraint is a key constraint on the attribute values for$NL$ExamResult, a logical consequence of KEY { StudentId, CourseId } specified for EXAM_MARK.$NL$The KEY shorthand could be considered for relation-valued attributes of relvars but it is not included in$NL$Tutorial D, one reason being that such a design is in general contraindicated and should be discouraged.$NL$Here are some points against it:$NL$1. The particular grouping chosen is arbitrary. Why not group on { CourseId, Mark } instead?$NL$2. The constraints are more complicated, even if we drop the one requiring ExamResult values$NL$to be nonempty.$NL$3. Updating can be particularly awkward. Consider how to write a Tutorial D UPDATE statement$NL$to change student S1’s mark in course C1. Consider how to write an INSERT statement to record$NL$that mark in the first place.$NL$4. The playing field for queries, as with non-5NF relvars, is not level. True, some aggregation$NL$queries are slightly simplified (try obtaining the average exam mark for each course), but many$NL$others become quite complex unless a preliminary invocation of UNGROUP is injected (try$NL$obtaining all of student S1’s exam marks).$NL$In short, the asymmetric structure illustrated in Example 8.1 leads to asymmetric queries, asymmetric$NL$constraints, and asymmetric updates.$NL$Now, C_ER is in 5NF, as you can easily verify, and it exhibits no redundancy. But see, in Figure 8.3, what$NL$happens if we apply a similar treatment to our original non-5NF relvar, ENROLMENT, having the attribute$NL$Name in place of EXAM_MARK’s Mark, giving the relvar C_ES with relation-valued attribute$NL$EnrolledStudents.$NL$C_ES is in 5NF but exhibits exactly the same redundancy that ENROLMENT exhibits: in the current value$NL$of C_ES, student S1’s name is recorded twice.$NL$For these reasons, perhaps a group-ungroup normal form (GUNF?) could be usefully defined: relvar r is in$NL$GUNF if and only if no attribute of r is relation valued but as far as the present author is aware no such$NL$definition is to be found in the literature (apart from this book). Codd proposed a normal form that he$NL$called first normal form (1NF), and he included a requirement for 1NF in his definitions for 2NF, 3NF,$NL$and subsequently BCNF. Under 1NF as he defined it, relation-valued attributes were “outlawed” that is to$NL$say, a relvar having such an attribute was not in 1NF. However, certain examples do exist where to avoid$NL$a relation valued attribute we have to resort to artifice.$NL$Consider, for example, the relvar, in the catalog, to record the keys of all the relvars in the database. The$NL$straightforward definition, shown in Example 8.2, involves a relation-valued attribute.$NL$We cannot obtain an equivalent design by ungrouping, because a relvar can have several keys. A tuple$NL$appearing in the ungrouping, simply pairing relvar name r with attribute name a tells, us only that a is a$NL$member of some key of r. A truly equivalent design in GUNF is unachievable. The best we can do is$NL$probably as shown in Example 8.3, where we have to introduce an extra attribute. Moreover, this design$NL$does not admit relvars with empty keys (specified by KEY { })those would have to be represented by a$NL$separate relvar in the catalog.$NL$Having to number the keys of each relvar is artificial and burdensome. Most of the noted disadvantages of$NL$relation-valued attributes are not so relevant here because we expect catalog relvars to be maintained by$NL$the DBMS. The natural, non-GUNF design of Example 8.2 is probably preferable.$NL$Now, Codd’s definition of 1NF attempted similarly to outlaw attributes of certain other types too, because$NL$relation types are not the only types that, if used for attributes of relvars, give rise to the problems$NL$identified with relation types, as we shall now see.$NL$Instead of defining C_ES with its relation-valued attribute EnrolledStudents, derived from$NL$ENROLMENT using GROUP, we could apply WRAP to derive the relvar C_EST, as shown in Example 8.4.$NL$Like C_ER, C_EST is in 5NF and yet still exhibits the same redundancy as ENROLMENT. Codd’s$NL$definition of 1NF precluded tuple-typed attributes too. Perhaps a “wrap-unwrap” normal form (WUNF?)$NL$could be usefully defined along similar lines to the GUNF previously mooted. But even outlawing relation$NL$and tuple types wasn’t sufficient for the purpose at hand. A similar effect can be obtained with userdefined$NL$types, as Example 8.5 shows.$NL$Codd attempted to preclude the use of such types in his definition of 1NF, but unfortunately this definition$NL$appealed to an unclear notion of atomicity. To be in 1NF, a relvar’s attributes all had to be of types$NL$consisting of “atomic” values only. However, he did not give a clear definition of what it means for a$NL$value to be atomic and we now believe the notion has no absolute meaning. Suffice it just to say that the$NL$relational database designer should generally avoid futile attempts, such as those shown in this section, to$NL$obtain 5NF without achieving the elimination of redundancy that 5NF is supposed to achieve. In particular,$NL$stick, where possible, to GUNF and WUNF.
1;In order to understand the TR approach to implementing the relational model, it’s necessary to be very clear over three distinct levels of the system, which I’ll refer to as the three levels of abstraction (since each level is an abstraction of the one below, loosely speaking). The three levels, or layers, are: $NL$1. The relational (or user) level $NL$2. The file level $NL$3. The TR level$NL$■Level 1, which corresponds to the database as seen by the user, is the relational level. At this level, the data is perceived as relations, including, perhaps, the suppliers relation S discussed in Section 2.1 (and illustrated in Fig. 2.1) in the previous chapter. $NL$■■Level 3 is the fundamental TR implementation level. At this level, data is represented by means of a variety of internal structures called tables. Please note immediately that those TR tables are NOT tables in the SQL sense and do NOT correspond directly to relations at the user level.$NL$■Level 2 is a level of indirection between the other two. Relations at the user or relational level are mapped to files at this level, and those files are then mapped to tables at the TR level. Of course, the mappings go both ways that is, tables at the TR level map to files at the next level up, and those files then map to relations at the top level. Note: As I’m sure you know, map is a synonym for transform (and I’ll be using the term in that sense throughout this book) thus, we’re already beginning to touch on the TR transforms that were mentioned in Chapter 1. However, there’s a great deal more to it, as we’ll soon see. $NL$Please now observe that each level has its own terminology: relational terms at the user level, file terms at the file level, and table terms at the TR level. Using different terms should, I hope, help you keep the three levels distinct and separate in your mind for that reason, I plan to use the three sets of terms consistently and systematically throughout the rest of this book. $NL$Having said that, I now need to say too that I’m well aware that some readers might object to my choice of terms—perhaps even find them confusing—for at least the following two reasons: $NL$■■First, the industry typically uses the terminology of tables, not relations, at the user level—almost exclusively so, in fact. But I’ve already explained some of my rationale for wanting to use relational terms at that level (see the previous chapter, Section 2.1), and I’m going to give some additional reasons in the next section.$NL$■Second, the industry also typically tends to think of files as a fairly “physical” construct. In fact, I did the same thing myself in the previous chapter, somewhat, though I was careful in that chapter always to be quite clear that the files I was talking about were indeed physically stored files specifically. By contrast, the files I’ll be talking about in the rest of the book are not physically stored instead, they’re an abstraction of what’s physically stored, and hence a “logical” construct, not a physical one. (Though it wouldn’t be wrong to think of them as “slightly more physical” than the user-level relations, if you like.) $NL$If you still think my terms are confusing, then I’m sorry, but for better or worse they’re the terms I’m going to use. $NL$One final point: When I talk of three levels, or layers, of abstraction, I don’t mean that each of those levels is physically materialized in any concrete sense—of course not. The relational level is only a way of looking at the file level, a way in which certain details are ignored (that’s what “level of abstraction” means). Likewise, the file level in turn is only a way of looking at the TR level. Come to that, the TR level in turn is only a way of looking at the bits and bytes that are physically stored that is, the TR level is itself—as already noted in Chapter 1, Section 1.2—still somewhat abstract. In a sense, the bits-and-bytes level is the only level that’s physically materialized.1$NL$Since the focus of this book is on the use of TR technology to implement the relational model specifically, the topmost (user) level is relational by definition. In other words, the user sees the database as a set of relations, made up of attributes and tuples as explained in Chapter 2. For simplicity, I’m going to assume those relations are all base relations specifically (again, see Chapter 2) that is, I’ll simply assume, barring explicit statements to the contrary, that any relation that’s named and is included in the database is in fact a base relation specifically, and I won’t usually bother to use the “base” qualifier. $NL$Also, of course, the user at the relational level has available a set of relational operators—restrict, project, join, and so forth—for querying the relations in the database, as well as the usual INSERT, DELETE, and UPDATE operators for updating them. Note: If I wanted to be more precise here, I’d have to get into the important distinction between relation values and relation variables. Relational operators like join operate on relation values, while update operators like INSERT operate on relation variables. Informally, however, it’s usual to call them all just relations, and—somewhat against my better judgment—I’ve decided to follow that common usage (for the most part) in the present book. For further discussion of such matters, see either reference [32] or reference [40]. $NL$Now, given the current state of the IT industry, the user level in a real database system will almost certainly be based on SQL, not on the relational model. As a consequence, users will typically tend to think, not in terms of relational concepts as such, but rather in terms of SQL analogs of those concepts. For example, there isn’t any explicit project operator, as such, in SQL instead, such an operation has to be formulated in terms of SQL’s SELECT and FROM operators, and the user has to think in terms of those SQL operators, as in this example (“Project suppliers over supplier number and city name”):
1;This chapter continues our examination of the core constructs of the TR model (principally the Field Values and Record Reconstruction Tables). However, the chapter is rather more of a potpourri than the previous one. Its structure is as follows. Following this short introductory section, Section 5.2 offers some general observations regarding performance. Section 5.3 then briefly surveys the TR operators, and Sections 5.4 and 5.5 take another look at how the Record Reconstruction Table is built and how record reconstruction is done. Sections 5.6 and 5.7 describe some alternative perspectives on certain of the TR constructs introduced in Chapter 4. Finally, Section 5.6 takes a look at some alternative ways of implementing some of the TR structures and algorithms also first described in that previous chapter.$NL$It seems to me undeniable that the mechanisms described in the previous chapter for representing and reconstructing records and files are vastly different from those found in conventional DBMSs, and I presume you agree with this assessment. At the same time, however, they certainly look pretty complicated ... How does all of that complexity square with the claims I made in Chapter 1 regarding good performance? Let me remind you of some of the things I said there:$NL$Well, let me say a little more now regarding query performance specifically (I haven’t really discussed updates yet, so I’ll have to come back to the question of update performance later—actually in the next chapter). Now, any given query involves two logically distinct processes: $NL$a) Finding the data that’s required, and then $NL$b) Retrieving that data. $NL$TR is designed to exploit this fact. Precisely because it separates field value information and linkage information, it can treat these two processes more or less independently. To find the data, it uses the Field Values Table to retrieve it, it uses the Record Reconstruction Table. (These characterizations aren’t 100 percent accurate, but they’re good to a first approximation—good enough for present purposes, at any rate.) And the Field Values Table in particular is designed to make the finding of data very efficient (for example, via binary search), as we saw in Chapter 4. Of course, it’s true that subsequent retrieval of that data then involves the record reconstruction process, and this latter process in turn involves a lot of pointer chasing, but:$NL$■■Even in a disk-based implementation, the system will do its best to ensure that pertinent portions of both the Field Values Table and the Record Reconstruction Table are kept in main memory at run time, as we’ll see in Part III. Assuming this goal is met, the reconstruction will be done at main-memory speeds. $NL$■■The “frills” to be discussed in Chapters 7 9 (as well as others that are beyond the scope of this book) have the effect, among other things, of dramatically improving the performance of various aspects of the reconstruction process. $NL$■■Most important of all: Almost always, finding the data that’s wanted is a much bigger issue than returning that data to the user is. In a sense, the design of the TR internal structures is biased in favor of the first of these issues at the expense of the second. Observe the implication: The more complex the query, the better TR will perform—in comparison with traditional approaches, that is. (Of course, I don’t mean to suggest by these remarks that record reconstruction is slow or inefficient—it isn’t—nor that TR performs well on complex queries but not on simple ones. I just want to stress the relative importance of finding the data in the first place, that’s all.)$NL$I’d like to say more on this question of query performance. In 1969, in his very first paper on the relational model [5], Codd had this to say:$NL$Once aware that a certain relation exists, the user will expect to be able to exploit that relation using any combination of its attributes as “knowns” and the remaining attributes as “unknowns,” because the information (like Everest) is there. This is a system feature (missing from many current information systems) which we shall call (logically) symmetric exploitation of relations. Naturally, symmetry in performance is not to be expected. $NL$—E. F. Codd$NL$Note: I’ve reworded Codd’s remarks just slightly here. In particular, the final sentence (the caveat concerning performance) didn’t appear in the original 1969 paper [5] but was added in the expanded 1970 version [6]. $NL$Anyway, the point I want to make is that the TR approach gives us symmetry in performance, too—or, at least, it comes much closer to doing so than previous approaches ever did. This is because, as we saw in Chapter 4, the separation of field values from linkage information effectively allows the data to be physically stored in several different sort orders simultaneously. When Codd said “symmetry in performance is not to be expected,” he was tacitly assuming a direct-image style of implementation, one involving auxiliary structures like those described in Chapter 2. However, as I said in that chapter: $NL$[Auxiliary structures such as pointer chains and] indexes can be used to impose different orderings on a given file and thus (in a sense) “level the playing field” with respect to different processing sequences all of those sequences are equally good from a logical point of view. But they certainly aren’t equally good from a performance point of view. For example, even if there’s a city index, processing suppliers in city name sequence will involve (in effect) random accesses to storage, precisely because the supplier records aren’t physically stored in city name sequence but are scattered all over the disk. $NL$—from Chapter 2$NL$As we’ve seen, however, these remarks simply don’t apply to the TR data representation. $NL$And now I can address another issue that might possibly have been bothering you. We’ve seen that the TR model relies heavily on pointers. Now, the CODASYL “network model” [14,25] also relies heavily on pointers—as the “object model” [3,4,28,29] and “hierarchic model” [25,56] both do also, as a matter of fact—and I and many other writers have criticized it vigorously in the past on exactly that score (see, for example, references [10], [21], and [37]). So am I arguing out of both sides of my mouth here? How can TR pointers be good while CODASYL pointers are bad?
1;There’s an old joke, well known in database circles, to the effect that what users really want (and always have wanted, ever since database systems were first invented) is for somebody to implement the go faster! command. Well, I’m glad to be able to tell you that, as of now, somebody finally has ... This book is all about a radically new database implementation technology, a technology that lets us build database management systems (DBMSs) that are “blindingly fast”—certainly orders of magnitude faster than any previous system. As explained in the preface, that technology is known as The TransRelationaltm Model, or the TR model for short (the terms TR technology and, frequently, just TR are also used). As also explained in the preface, the technology is the subject of a United States patent (U.S. Patent No. 6,009,432, dated December 28th, 1999), listed as reference [63] in Appendix B at the back of this book however, that reference is usually known more specifically as the Initial Patent, because several follow-on patent applications have been applied for at the time of writing. This book covers material from the Initial Patent and from certain of those follow-on patents as well.$NL$The TR model really is a breakthrough. To say it again, it allows us to build DBMSs that are orders of magnitude faster than any previous system. And when I say “any previous system,” I don’t just mean previous relational systems. It’s an unfortunate fact that many people still believe that the fastest relational system will never perform as well as the fastest nonrelational system. Indeed, it’s exactly that belief that accounts in large part for the continued existence and use of older, nonrelational systems such as IMS [25,57] and IDMS [14,25], despite the fact that—as is well known—relational systems are far superior from the point of view of usability, productivity, and the like. However, a relational system implemented using TR technology should dramatically outperform even the fastest of those older nonrelational systems, finally giving the lie to those old performance arguments and making them obsolete (not before time, either).$NL$I must also make it clear that I don’t just mean that queries should be faster under TR (despite the traditional emphasis in relational systems on queries in particular)—updates should be faster as well. Nor do I mean that TR is suitable only for decision support systems—it’s eminently suitable for transaction processing systems, too (though it’s probably fair to say that TR is particularly suitable for systems in which read-only operations predominate, such as data warehouse and data mining systems).$NL$And one last preliminary remark: You’re probably thinking that the performance advantages I’m claiming must surely come at a cost: perhaps poor usability, or less functionality, or something (there’s no free lunch, right?). Well, I’m pleased to be able to tell you that such is not the case. The fact is, TR actually provides numerous additional benefits, over and above the performance benefit—for example, in the areas of database and system administration. Thus, I certainly don’t want you to think that performance is the only argument in favor of TR. We’ll take a look at some of those additional benefits in Chapters 2 and 15, and elsewhere in passing. (In fact, a detailed summary of all of the TR benefits appears in Chapter 15, in Section 15.4. You might like to take a quick look at that section right now, just to get an idea of how much of a breakthrough the TR model truly is.)$NL$As I said in the preface, I believe TR technology is one of the most significant advances—quite possibly the most significant advance—in the data management field since E. F. Codd first invented the relational model (which is to say, since the late 1960s and early 1970s see references [5 7], also reference [35]). As I also said in the preface, TR represents among other things a highly effective way to implement the relational model, as I hope to show in this book. In fact, the TR model—or, rather, the more general technology of which the TR model is just one specific but important manifestation—represents an effective way to implement data management systems of many different kinds, including but not limited to the following:$NL$■■SQL DBMSs 	■■Data warehouse systems$NL$■■Information access tools	■■Data mining tools$NL$■■Object/relational DBMSs	■■Web search engines$NL$■■Main-memory DBMSs	■■Temporal DBMSs$NL$■■Business rule systems	■■Repository managers$NL$■■XML document storage and retrieval systems	■■Enterprise resource planning tools$NL$as well as relational DBMSs in particular. Informally, we could say we’re talking about a backend technology that’s suitable for use with many different frontends. In planning this book, however, I quickly decided that my principal focus should be on the application of the technology to implementing the relational model specifically. Here are some of my reasons for that decision:$NL$■Concentrating on one particular application should make the discussions and examples more concrete and therefore, I hope, easier to follow and understand.$NL$■■More significantly, the relational model is of fundamental importance it’s rock solid, and it will endure. After all, it really is the best contender, so far as we know, for the role of “proper theoretical foundation” for the entire data management field. One hundred years from now, I fully expect database systems still to be firmly based on Codd’s relational model—even if they’re advertised as “object/relational,” or “temporal,” or “spatial,” or whatever. See Chapter 15 for further discussion of such matters. $NL$■■If your work involves data management in any of its aspects, then you should already have at least a nodding acquaintance with the basic ideas of the relational model. Though I feel bound to add that if that “nodding acquaintance” is based on a familiarity with SQL specifically, then you might not know as much as you should about the model as such, and you might know “some things that ain’t so.” I’ll come back to this point in a few moments. $NL$■■The relational model is an especially good fit with TR ideas I mean, it’s a very obvious candidate for implementation using those ideas. Why? Because the relational model is at a uniform, and high, level of abstraction it’s concerned purely with what a database system is supposed to look like to the user, and has absolutely nothing to say about what the system might look like internally. As many people would put it, the relational model is logical, not physical.
1;By now I hope it’s clear that, even without the refinements to be discussed in later chapters, the TR model is certainly good for retrieval. (At least in principle! I’ll describe in more detail how retrievals are actually implemented in Chapter 10.) But what about updates?1 Conventional wisdom has always been that a given data structure can be good for either retrieval or update, but not both. In a direct-image implementation, for example, indexes are generally held to be good for retrieval but bad for update. So what about TR? How are updates done in TR? This chapter examines this question. $NL$To repeat from Chapter 5, then, the operators we need to consider are as follows (see Section 5.3): $NL$■■INSERT: Insert a new record. $NL$■■DELETE: Delete the record “passing through” cell [i,j] of the Record Reconstruction Table. $NL$■■UPDATE: Update the record “passing through” cell [i,j] of the Record Reconstruction Table. $NL$Note: The notion of a record “passing through” some cell of the Record Reconstruction Table was also explained in Section 5.3.$NL$Section 6.2 immediately following discusses the three update operators in general terms Sections 6.3 then presents a detailed example, and Section 6.4 discusses the swap algorithm. Section 6.5 briefly describes an alternative implementation technique that makes use of an overflow structure. Finally, Section 6.6 offers some observations regarding the performance aspects of TR update operations.$NL$It’s convenient to begin by discussing the INSERT operator specifically. Consider the suppliers file shown in Fig. 6.1 (it’s the same as the one shown in Fig. 4.1 in Chapter 4, except that the last record, the one for supplier S3, has been omitted). Figs. 6.2 and 6.3 show the corresponding Field Values Table and a corresponding Record Reconstruction Table, respectively. Exercise 6: Check that these tables are correct.$NL$Now suppose the user asks the system to insert the following tuple into the suppliers relation:$NL$In terms of the file of Fig. 6.1, of course, we can imagine a new record corresponding to this tuple simply being appended at the end, in position 5 (since record ordering within files is arbitrary). If we now rebuild the Field Values Table, it’ll appear as shown on the left-hand side of Fig. 6.4 (a copy of the Field Values Table from Fig. 4.3 in Chapter 4). And if we then build a corresponding Record Reconstruction Table, it might appear as shown on the right-hand side of Fig. 6.4$NL$As you can see by comparing Fig. 6.4 with Figs. 6.2 and 6.3, respectively, inserting supplier S3 has caused both the Field Values Table and the Record Reconstruction Table to change dramatically. It follows that INSERT operations have the potential to be quite disruptive, and hence (possibly) to display very poor performance. What can be done about this problem? $NL$Well, let me say right away that the effect on the Field Values Table is actually not as dramatic as it might appear. Although I’ve been calling it a table and showing it as a table in figures like Fig. 6.4, the Field Values Table doesn’t necessarily have to be physically stored as a table in fact, it almost certainly won’t be. Much more likely, it’ll be stored “column-wise” as a set of vectors (one-dimensional arrays), or possibly as a set of chained lists, one such vector or list for each column. Indeed, such an implementation is virtually certain to be used in practice if the refinements to be discussed in Chapters 8 and 9 are adopted, as we’ll see.2 $NL$For definiteness, let’s assume a vector implementation. Of course, those vectors will be kept in the sort orders associated with the corresponding columns of the Field Values Table. As a consequence, the insert point in each such vector for the pertinent field value from the new record is easily determined—for example, by binary search—and the vectors themselves, and hence the overall Field Values Table, are thus easily maintained.$NL$$NL$The Record Reconstruction Table is another matter, however. Is there a way to avoid rebuilding the entire table every time a new record is inserted into the user file? The answer, of course, is yes. One possible approach is as follows (the details are a little complicated, but the fundamental idea is straightforward): When a record is deleted from the user file, we3 don’t physically remove the corresponding entries from the Field Values and Record Reconstruction Tables, we just flag those entries as “logically deleted.” Those flagged cells can then be regarded as free space in each of the two tables. Then, when we subsequently insert a new record, it might be possible to use such flagged cells for the record in question (removing the flags, of course), thereby avoiding the overhead of completely rebuilding the Record Reconstruction Table (and the overhead of completely rebuilding the Field Values Table also, as a matter of fact). Detailed examples illustrating this process are given in Sections 6.3 and 6.4 below. $NL$I should immediately add that the scheme just described in outline makes considerably more sense if the refinements to be discussed in Chapters 8 and 9 are adopted. If they are—and in practice it’s virtually certain they will be—then it becomes possible for distinct records at the file level to share entries in the Field Values Table. For example, the supplier records for suppliers S2 and S3 might share the entry in that table that contains the city name Paris. Thus, when a new record is inserted, it might well be the case that most if not all of the field values in that record already exist in the Field Values Table—perhaps logically deleted, perhaps not—and such values can simply be shared by that new record with previously existing records. In effect, the ability to share field values in this way means that INSERT operations work at the field level instead of the usual record level—yet another significant difference between the TR approach and conventional implementation technology. Of course, analogous remarks apply to DELETE and UPDATE operations also, as you’d surely expect.
1;The main purpose of this chapter is to explain in more detail some of the problems that arise in connection with what the lawyers call “prior art”—meaning, in the case at hand, systems that use the traditional direct-image approach to implementation. Of course, you can skip this material if you’re already familiar with conventional implementation technology. However, this first section does also introduce a few simple relational ideas, and you might at least want to make sure you’re familiar with those and fully understand them. $NL$Consider Fig. 2.1, which depicts a relation called S (“suppliers”). Observe that each supplier has a supplier number (S#), unique to that supplier1 a supplier name (SNAME), not necessarily unique (though in fact the sample names shown in the figure do happen to be unique) a rating or status value (STATUS) and a location (CITY). I’ll use this example to remind you of a few of the most fundamental relational terms and concepts.$NL$■First of all, a relation can, obviously enough, be pictured as a table. However, a relation is not a table.2 A picture of a thing isn’t the same as the thing! In fact, the difference between a thing and a picture of that thing is another of the great logical differences (see the remarks on this latter notion in Chapter 1, near the beginning of Section 1.3). One problem with thinking of a relation as a table is that it suggests that certain properties of tables—for example, the property that the rows are in a certain top-to-bottom order—apply to relations too, when in fact they don’t (see below). $NL$■■Each of the five suppliers is represented by a tuple (pronounced as noted in Chapter 1 to rhyme with “couple”). Tuples are depicted as rows in figures like Fig. 2.1, but tuples aren’t rows. $NL$■■Each supplier tuple contains four values, called attribute values that is, the suppliers relation involves four attributes, called S#, SNAME, STATUS, and CITY. Attributes are depicted as columns in figures like Fig. 2.1, but attributes aren’t columns.$NL$■■Attributes are defined over data types (types for short, also known as domains), meaning that every value of the attribute in question is required to be a value of the type in question. Types can be either system-defined (built in) or user-defined. For example, attribute STATUS might be defined over the system-defined type INTEGER (STATUS values are integers), while attribute SNAME might be defined over the user-defined type NAME (SNAME values are names). Note: For definiteness, I’ll assume these specific types throughout what follows, where it makes any difference. I’ll also assume that attribute S# is defined over a user-defined type with the same name (that is, S#), and attribute CITY is defined over the system-defined type CHAR (meaning character strings of arbitrary length). $NL$■■The tuples of a relation are all distinct. In fact, relations never contain duplicate tuples—the tuples of a relation form a mathematical set, and sets in mathematics don’t contain duplicate elements. Note: People often complain about this aspect of the relational model, but in fact there are good practical reasons for not permitting duplicate tuples. A detailed discussion of the point is beyond the scope of this book see any of references [13], [20], or [33] if you want to pursue the matter. $NL$■■There’s no top-to-bottom ordering to the tuples of a relation. Although figures like Fig. 2.1 clearly suggest there is such an ordering, there really isn’t—to say it again, the tuples of a relation form a mathematical set, and sets in mathematics have no ordering to their elements. Note: It follows from this point that we could draw several different pictures that would all represent the same relation. An analogous remark applies to the point immediately following.$NL$■There’s no left-to-right ordering to the attributes of a relation. Again, figures like Fig. 2.1 clearly suggest there is such an ordering, but there really isn’t like the tuples, the attributes of a relation form a set, and thus have no ordering. (By the same token, there’s no left-to-right ordering to the components of a tuple, either.) No relation can have two or more attributes with the same name. $NL$■■The suppliers relation is in fact a base relation specifically. In general, we distinguish between base and derived relations a derived relation is one that is derived from, or defined in terms of, other relations, and a base relation is one that isn’t derived in this sense. Loosely speaking, in other words, the base relations are the “given” ones—they’re the ones that make up the actual database—while the derived ones are views, snapshots, query results, and the like [33]. For example, given the base relation of Fig. 2.1, the result of the query “Get suppliers in London” is a derived relation that looks like this:$NL$Another way to think about the distinction is that base relations exist in their own right, while derived ones don’t—they’re existence-dependent on the base relations. $NL$■■Every relation has at least one candidate key (or just key for short), which serves as a unique identifier for the tuples of that relation. In the case of the suppliers relation (and the derived relation just shown as well), there’s just one key, namely {S#}, but relations can have any number of keys, in general. Note: It’s important to understand that keys are always sets of attributes (though the set in question might well contain just a single attribute). For this reason, in this book I’ll always show key attributes enclosed in braces, as in the case at hand—braces being used by convention to bracket the elements that make up a set. $NL$■■As you probably know, it’s customary (though not obligatory) to choose, for any given relation, one of that relation’s candidate keys—possibly its sole candidate key—as primary thus, for example, we might say in the case of the suppliers relation that {S#} is not just a key but the “primary” key. In figures like Fig. 2.1, I’ll follow the convention of identifying primary key attributes by double underlining. $NL$■■Finally, relations can be operated on by a variety of relational operators. In general, a relational operator is an operator that takes zero or more relations as input and produces a relation as output. Examples include the well-known operators restrict, project, join, and so on.
1;Now (at last) I can begin to explain the TR model in detail. As I mentioned several times in Part I, TR is indeed still a model, and thus, like the relational model, still somewhat abstract. At the same time, however, it’s at a much lower level of abstraction than the relational model it can be thought of as being closer to the physical implementation level (“closer to the metal”), and accordingly more oriented toward issues of performance. In particular, it relies heavily on the use of pointers—a concept deliberately excluded from the relational model, of course, for reasons discussed in references [9], [30], [40], and many other places—and its operators are much more procedural in nature than those of the relational model. (What I mean by this latter remark is that code that makes use of those operators is much more procedural than relational code is, or is supposed to be.) What’s more, reference [63] includes detailed, albeit still somewhat abstract, algorithms for implementing those operators. Note: These remarks aren’t meant to be taken as criticisms, of course I’m just trying to capture the essence of the TR model by highlighting some of its key features. $NL$Despite its comparatively low-level nature, the fact remains that, to say it again, TR is indeed a model, and thus capable of many different physical realizations. In what follows, I’ll talk for much of the time in terms of just one possible realization—it’s easier on the reader to be concrete and definite—but I’ll also mention some alternative implementation schemes on occasion. Note that the alternatives in question have to do with the implementation of both data structures and corresponding access algorithms. In particular, bear in mind that both main-memory and secondary-storage implementations are possible. $NL$Now, this book is meant to be a tutorial accordingly, I want to focus on showing the TR model in action (as it were)—that is, showing how it works in terms of concrete examples—rather than on describing the abstract model as such. Also, many TR features are optional, in the sense that they might or might not be present in any given implementation or application of the model, and it’s certainly not worth getting into all of those optional features in a book of this kind. Nor for the most part is it worth getting into the optionality or otherwise of those features that are discussed—though I should perhaps at least point out that options do imply a need for decisions: Given some particular option X, some agency, at some time, has to decide whether or not X should be exercised. For obvious reasons, I don’t want to get into a lot of detail on this issue here, either. Suffice it to say that I don’t think many of those decisions, if any at all, should have to be made at database design time (by some human being) or at run time (by the system itself) in fact, I would expect most of them to be made during the process of designing the DBMS that is the specific TR implementation in question. In other words, I don’t think the fact that those decisions do have to be made implies that a TR implementation will therefore suffer from the same kinds of problems that arise in connection with direct-image systems, as discussed in Chapter 2. $NL$It follows from all of the above that this book is meant as an introduction only many topics are omitted and others are simplified, and I make no claims of completeness of any kind.$NL$Now let’s get down to business. In this chapter and the next,1 we’ll be looking at what are clearly the most basic TR constructs of all: namely, the Field Values Table and the Record Reconstruction Table, both of which were mentioned briefly in the final section of the previous chapter. These two constructs are absolutely fundamental—everything else builds on them, and I recommend as strongly as I can that you familiarize yourself with their names and basic purpose before you read much further. Just to remind you: $NL$■■The Field Values Table contains the field values from a given file, rearranged in a way to be explained in Section 4.3. $NL$■■The Record Reconstruction Table contains information that allows records of the given file to be reconstructed from the Field Values Table, in a way to be explained in Section 4.4. $NL$In subsequent chapters I’ll consider various possible refinements of those core concepts. Note: Those refinements might be regarded in some respects as “optional extras” or “frills,” but some of them are very important—so much so, that they’ll almost certainly be included in any concrete realization of the TR model, as we’ll see.$NL$Let r be some given record within some given file at the file level. Then the crucial insight underlying the TR model can be characterized as follows: $NL$The stored form of r involves two logically distinct pieces, a set of field values and a set of “linkage” information that ties those field values together, and there’s a wide range of possibilities for physically storing each piece. $NL$In direct-image systems, the two pieces (the field values and the linkage information) are kept together, of course in other words, the linkage information in such systems is represented by physical contiguity. In TR, by contrast, the two pieces are kept separate to be specific, the field values are kept in the Field Values Table, and the linkage information is kept in the Record Reconstruction Table. That separation makes TR strikingly different from virtually all previous approaches to implementing the relational model (see Chapters 1 and 2), and is the fundamental source of the numerous benefits that TR technology is capable of providing. In particular, it means that TR data representations are categorically not a direct image of what the user sees at the relational level. $NL$Note: One immediate advantage of the separation is that the Field Values Table and the Record Reconstruction Table can both be physically stored in a way that is highly efficient in terms of storage space and access time requirements. However, we’ll see many additional advantages as well, both in this chapter and in subsequent ones.
1;This is the last chapter in this part of the book. In it, I want to describe a rather different approach to the problem of implementing the TR model on disk: more specifically, to the problem of minimizing disk seeks. Note immediately, therefore, that the approach in question can be regarded in part as an alternative to file banding as discussed in Chapter 13—but only in part, because in fact file banding can be used in combination with the approach to be described, as we’ll see in Section 14.4. Note too that, as with the discussion of file banding in Chapter 13, we’re primarily concerned here with how to deal with the “large file” that remains after file factoring has been used to get all of the “small files” into memory. But first things first. $NL$As we know, the basic problem with TR on the disk is that if we’re not careful, the zigzags can splay out all over the disk. Well, if the splay problem is caused by the zigzags, then let’s get rid of the zigzags! Recall from Chapter 5 (Section 5.8) that the linkage information that lets us reconstruct records doesn’t have to be implemented as zigzags specifically—other possibilities exist, with (of course) different performance characteristics. The approach to be described in this chapter exploits this idea essentially, what it does is replace the zigzags by a different kind of structure called a star.$NL$$NL$Let me illustrate this idea right away. Fig. 14.1 shows the Field Values Table and corresponding Record Reconstruction Table from Figs. 13.2 and 13.3 in Chapter 13—except that, for pedagogic reasons, I’ve shown the Field Values Table in uncondensed form. Fig. 14.2 then highlights one particular zigzag from Fig. 14.1 (actually the one for part P7), and Fig. 14.3 shows what happens if we replace that zigzag by a star.$NL$$NL$As you can see, where Fig. 14.2 has a ring of pointers (implemented within the Record Reconstruction Table and conceptually superimposed on the Field Values Table), Fig. 14.3 has a star of pointers instead. Cell [7,1], which corresponds to the P# value P7, serves as the center or core of that star. Three pointers emanate from that core and point to cells [6,2], [8,3], and [4,4], respectively those cells correspond to the PNAME value Nut, the WEIGHT value 19.0, and the CC# value cc1, respectively. Those three pointers, which (as Fig. 14.3 indicates) are all two-way and can therefore be traversed in either direction, serve as the spokes or rays of the star. $NL$Now, the star in the figure clearly does support reconstruction of the record for the part in question (part P7). To be specific: $NL$a) If we start at the core, we can simply follow the three spoke pointers outward to obtain the other three field values. $NL$b) If we start at any other point, we can follow the corresponding spoke pointer inward to the core and then proceed as under a) above—with the exception that, if we get to the core by following spoke pointer sp inward, then of course there’s no need to follow that particular spoke sp outward again. Note: As a matter of fact, we never need to follow a spoke outward from the core within the Record Reconstruction Table as such we only need to be able to go from the core outward to cells within the Field Values Table. $NL$Now, you might have already realized that, for any given zigzag, there are several distinct but equivalent stars—it just depends on which field we choose as the core. I’ll return to this point in Section 14.3. You might also have realized that the record reconstruction algorithm as just outlined displays asymmetric performance—access via the core field will be faster than access via any other field, because stars (unlike zigzags) are an inherently asymmetric structure—and I’ll return to this point in Section 14.5. $NL$The structure of the chapter is as follows. Following this introductory section, Section 14.2 gives a simple example to illustrate the basic ideas behind star structures. Section 14.3 elaborates on and generalizes that example. Section 14.4 shows how the ideas from the first three sections work on the disk (those previous sections are principally concerned with a memory-based implementation only). Finally, Section 14.5 discusses the use of controlled redundancy in connection with star structures.$NL$$NL$As in the previous chapter, the basic problem we’re trying to deal with is how to get the best possible performance out of the “large” Record Reconstruction Table in a disk-based system. So I’ll base my discussions on the same running example as in that previous chapter to be specific, I’ll assume once again that we’ve factored the parts file into large and small files that look like this:$NL$$NL$However, we’re interested here in the large file exclusively. Fig. 14.4 shows a sample value for that file (extracted from Fig. 13.1 in Chapter 13). And we’ve already seen a Field Values Table and a zigzag-based Record Reconstruction Table for that file in Fig. 14.1 above. Note: While the file shown in Fig. 14.4 is obviously not very large, let me remind you that we’re really supposed to be dealing with files of millions or even billions of records, and the data in those files isn’t supposed to display any “statistical clumpiness” at all.$NL$$NL$Now, despite the fact that we’re really supposed to be talking about a disk implementation, it’s convenient to pretend for the time being that everything’s in memory, and I’ll adopt that pretense until further notice. So how do we proceed? Well, since (as we’ve already seen) stars are asymmetric, the first thing we have to do is decide what the core’s going to be in other words, we first have to choose a core field (much as we had to choose a characteristic field in connection with with banding in the previous chapter).1 Suppose we choose field P#. Then Fig. 14.5 shows a corresponding star-based Record Reconstruction Table for the file of Fig. 14.4. Note: From this point forward, for convenience, I’ll abbreviate the term “star-based Record Reconstruction Table” to just star table, and similarly for zigzag table.
2;As mentioned previously a major reason for wishing to obtain a mathematical model of a device$NL$is to be able to evaluate the output in response to a given input. Using the transfer function and$NL$Laplace transforms provides a particularly elegant way of doing this. This is because for a block$NL$with input U(s) and transfer function G(s) the output Y(s) = G(s)U(s). When the input, u(t), is a$NL$unit impulse which is conventionally denoted by 􀄯(t), U(s) = 1 so that the output Y(s) = G(s).$NL$Thus in the time domain, y(t) = g(t), the inverse Laplace transform of G(s), which is called the$NL$impulse response or weighting function of the block. The evaluation of y(t) for any input u(t) can$NL$be done in the time domain using the convolution integral (see Appendix A, theorem (ix))$NL$but it is normally much easier to use the transform relationship Y(s) = G(s)U(s). To do this one$NL$needs to find the Laplace transform of the input u(t), form the product G(s)U(s) and then find its$NL$inverse Laplace transform. G(s)U(s) will be a ratio of polynomials in s and to find the inverse$NL$Laplace transform, the roots of the denominator polynomial must be found to allow the$NL$expression to be put into partial fractions with each term involving one denominator root (pole).$NL$Assuming, for example, the input is a unit step so that U(s) = 1/s then putting G(s)U(s) into$NL$partial fractions will result in an expression for Y(s) of the form$NL$where in the transfer function G(s) = B(s)/A(s), the n poles of G(s) [zeros of A(s)] are 􀄮i, i = 1…n$NL$and the coefficients C0 and Ci, i = 1…n, will depend on the numerator polynomial B(s), and are$NL$known as the residues at the poles. Taking the inverse Laplace transform yields$NL$The first term is a constant C0, sometimes written C0u0(t) because the Laplace transform is$NL$defined for t 􀂕 0, where u0(t) denotes the unit step at time zero. Each of the other terms is an$NL$exponential, which provided the real part of 􀄮i is negative will decay to zero as t becomes large.$NL$In this case the transfer function is said to be stable as a bounded input has produced a bounded$NL$output. Thus a transfer function is stable if all its poles lie in the left hand side (lhs) of the s plane$NL$zero-pole plot illustrated in Figure 2.1. The larger the negative value of 􀄮i the more rapidly the$NL$contribution from the ith term decays to zero. Since any poles which are complex occur in$NL$complex pairs, say of the form 􀄮1,􀄮2 = 􀄱 ± j􀈦, then the corresponding two residues C1 and C2 will$NL$be complex pairs and the two terms will combine to give a term of the form Ce􀁖t sin(􀁚t 􀀎􀁍) .$NL$This is a damped oscillatory exponential term where 􀄱, which will be negative for a stable$NL$transfer function, determines the damping and 􀈦 the frequency [strictly angular frequency] of the$NL$oscillation. For a specific calculation most engineers, as mentioned earlier, will leave a complex$NL$pair of roots as a quadratic factor in the partial factorization process, as illustrated in the Laplace$NL$transform inversion example given in Appendix A. For any other input to G(s), as with the step$NL$input, the poles of the Laplace transform of the input will occur in a term of the partial fraction$NL$expansion (3.2), [as for the C0/s term above], and will therefore produce a bounded output for a$NL$bounded input.$NL$In control engineering the major deterministic input signals that one may wish to obtain$NL$responses to are a step, an impulse, a ramp and a constant frequency input. The purpose of this$NL$section is to discuss step responses of specific transfer functions, hopefully imparting an$NL$understanding of what can be expected from a knowledge of the zeros and poles of the transfer$NL$function without going into detailed mathematics.$NL$A transfer function with a single pole is$NL$s a$NL$G s K$NL$􀀎$NL$( ) 􀀠1 , which may also be written in the socalled$NL$time constant form$NL$sT$NL$G s K$NL$􀀎$NL$􀀠$NL$1$NL$( ) , where K K / a 1 􀀠and T 􀀠1/ a The steady state$NL$gainG(0) 􀀠K , that is the final value of the response, and T is called the time constant as it$NL$determines the speed of the response. K will have units relating the input quantity to the output$NL$quantity, for example °C/V, if the input is a voltage and the output temperature. T will have the$NL$same units of time as s-1, normally seconds. The output, Y(s), for a unit step input is given by$NL$Taking the inverse Laplace transform gives the result$NL$The larger the value of T (i.e. the smaller the value of a), the slower the exponential response. It$NL$can easily be shown that y(T) 􀀠0.632K , T$NL$dt$NL$dy(0) 􀀠$NL$and y(5T) 􀀠0.993K or in words, the$NL$output reaches 63.2% of the final value after a time T, the initial slope of the response is T and$NL$the response has essentially reached the final value after a time 5T. The step response in$NL$MATLAB can be obtained by the command step(num,den). The figure below shows the step$NL$response for the transfer function with K = 1 on a normalised time scale.$NL$Here the transfer function G(s) is often assumed to be of the form$NL$It has a unit steady state gain, i.e G(0) = 1, and poles at 􀀠􀀐􀁝􀁚􀁲􀁚1􀀐􀁝2 o o s j , which are$NL$complex when 􀁝􀀟1. For a unit step input the output Y(s), can be shown after some algebra,$NL$which has been done so that the inverse Laplace transforms of the second and third terms are$NL$damped cosinusoidal and sinusoidal expressions, to be given by$NL$Taking the inverse Laplace transform it yields, again after some algebra,$NL$where 􀁍􀀠cos􀀐1􀁝. 􀁝is known as the damping ratio. It can also be seen that the angle to the$NL$negative real axis from the origin to the pole with positive imaginary part is$NL$tan􀀐1 (1􀀐􀁝2 )1/ 2 /􀁝􀀠cos􀀐1􀁝􀀠􀁍.
2;As its name implies control engineering involves the design of an engineering product or system$NL$where a requirement is to accurately control some quantity, say the temperature in a room or the$NL$position or speed of an electric motor. To do this one needs to know the value of the quantity$NL$being controlled, so that being able to measure is fundamental to control. In principle one can$NL$control a quantity in a so called open loop manner where ‘knowledge’ has been built up on what$NL$input will produce the required output, say the voltage required to be input to an electric motor$NL$for it to run at a certain speed. This works well if the ‘knowledge’ is accurate but if the motor is$NL$driving a pump which has a load highly dependent on the temperature of the fluid being pumped$NL$then the ‘knowledge’ will not be accurate unless information is obtained for different fluid$NL$temperatures. But this may not be the only practical aspect that affects the load on the motor and$NL$therefore the speed at which it will run for a given input, so if accurate speed control is required$NL$an alternative approach is necessary.$NL$This alternative approach is the use of feedback whereby the quantity to be controlled, say C, is$NL$measured, compared with the desired value, R, and the error between the two,$NL$E = R - C used to adjust C. This gives the classical feedback loop structure of Figure 1.1.$NL$In the case of the control of motor speed, where the required speed, R, known as the reference is$NL$either fixed or moved between fixed values, the control is often known as a regulatory control, as$NL$the action of the loop allows accurate speed control of the motor for the aforementioned situation$NL$in spite of the changes in temperature of the pump fluid which affects the motor load. In other$NL$instances the output C may be required to follow a changing R, which for example, might be the$NL$required position movement of a robot arm. The system is then often known as a$NL$servomechanism and many early textbooks in the control engineering field used the word$NL$servomechanism in their title rather than control.$NL$The use of feedback to regulate a system has a long history [1.1, 1.2], one of the earliest concepts,$NL$used in Ancient Greece, was the float regulator to control water level, which is still used today in$NL$water tanks. The first automatic regulator for an industrial process is believed to have been the$NL$flyball governor developed in 1769 by James Watt. It was not, however, until the wartime period$NL$beginning in 1939, that control engineering really started to develop with the demand for$NL$servomechanisms for munitions fire control and guidance. With the major improvements in$NL$technology since that time the applications of control have grown rapidly and can be found in all$NL$walks of life. Control engineering has, in fact, been referred to as the ‘unseen technology’ as so$NL$often people are unaware of its existence until something goes wrong. Few people are, for$NL$instance, aware of its contribution to the development of storage media in digital computers$NL$where accurate head positioning is required. This started with the magnetic drum in the 50’s and$NL$is required today in disk drives where position accuracy is of the order of 1μm and movement$NL$between tracks must be done in a few ms.$NL$Feedback is, of course, not just a feature of industrial control but is found in biological, economic$NL$and many other forms of system, so that theories relating to feedback control can be applied to$NL$many walks of life.$NL$The book is concerned with theoretical methods for continuous linear feedback control system$NL$design, and is primarily restricted to single-input single-output systems. Continuous linear time$NL$invariant systems have linear differential equation mathematical models and are always an$NL$approximation to a real device or system. All real systems will change with time due to age and$NL$environmental changes and may only operate reasonably linearly over a restricted range of$NL$operation. There is, however, a rich theory for the analysis of linear systems which can provide$NL$excellent approximations for the analysis and design of real world situations when used within$NL$the correct context. Further simulation is now an excellent means to support linear theoretical$NL$studies as model errors, such as the affects of neglected nonlinearity, can easily be assessed.$NL$There are total of 11 chapters and some appendices, the major one being Appendix A on Laplace$NL$transforms. The next chapter provides a brief description of the forms of mathematical model$NL$representations used in control engineering analysis and design. It does not deal with$NL$mathematical modelling of engineering devices, which is a huge subject and is best dealt with in$NL$the discipline covering the subject, since the devices or components could be electrical,$NL$mechanical, hydraulic etc. Suffice to say that one hopes to obtain an approximate linear$NL$mathematical model for these components so that their effect in a system can be investigated$NL$using linear control theory. The mathematical models discussed are the linear differential$NL$equation, the transfer function and a state space representation, together with the notations used$NL$for them in MATLAB.$NL$Chapter 3 discusses transfer functions, their zeros and poles, and their responses to different$NL$inputs. The following chapter discusses in detail the various methods for plotting steady state$NL$frequency responses with Bode, Nyquist and Nichols plots being illustrated in MATLAB.$NL$Hopefully sufficient detail, which is brief when compared with many textbooks, is given so that$NL$the reader clearly understands the information these plots provide and more importantly$NL$understands the form of frequency response expected from a specific transfer function.$NL$The material of chapters 2-4 could be covered in other courses as it is basic systems theory, there$NL$having been no mention of control, which starts in chapter 5. The basic feedback loop structure$NL$shown in Figure 1.1 is commented on further, followed by a discussion of typical performance$NL$specifications which might have to be met in both the time and frequency domains. Steady state$NL$errors are considered both for input and disturbance signals and the importance and properties of$NL$an integrator are discussed from a physical as well as mathematical viewpoint. The chapter$NL$concludes with a discussion on stability and a presentation of several results including the$NL$Mikhailov criterion, which is rarely mentioned in English language texts.
2;Control systems exist in many fields of engineering so that components of a control system may$NL$be electrical, mechanical, hydraulic etc. devices. If a system has to be designed to perform in a$NL$specific way then one needs to develop descriptions of how the outputs of the individual$NL$components, which make up the system, will react to changes in their inputs. This is known as$NL$mathematical modelling and can be done either from the basic laws of physics or from$NL$processing the input and output signals in which case it is known as identification. Examples of$NL$physical modelling include deriving differential equations for electrical circuits involving$NL$resistance, inductance and capacitance and for combinations of masses, springs and dampers in$NL$mechanical systems. It is not the intent here to derive models for various devices which may be$NL$used in control systems but to assume that a suitable approximation will be a linear differential$NL$equation. In practice an improved model might include nonlinear effects, for example Hooke’s$NL$Law for a spring in a mechanical system is only linear over a certain range or account for time$NL$variations of components. Mathematical models of any device will always be approximate, even$NL$if nonlinear effects and time variations are also included by using more general nonlinear or time$NL$varying differential equations. Thus, it is always important in using mathematical models to have$NL$an appreciation of the conditions under which they are valid and to what accuracy.$NL$Starting therefore with the assumption that our model is a linear differential equation then in$NL$general it will have the form:-$NL$where D denotes the differential operator d/dt. A(D) and B(D) are polynomials in D with$NL$Di 􀀠d i / dt i , the ith derivative, u(t) is the model input and y(t) its output. So that one can write$NL$where the a and b coefficients will be real numbers. The orders of the polynomials A and B are$NL$assumed to be n and m, respectively, with n 􀂕 m.$NL$Thus, for example, the differential equation$NL$with the dependence of y and u on t assumed can be written $NL$$NL$In order to solve an nth order differential equation, that is determine the output y for a given input$NL$u, one must know the initial conditions of y and its first n-1 derivatives. For example if a$NL$projectile is falling under gravity, that is constant acceleration, so that D2y= constant, where y is$NL$the height, then in order to find the time taken to fall to a lower height, one must know not only$NL$the initial height, normally assumed to be at time zero, but the initial velocity, dy/dt, that is two$NL$initial conditions as the equation is second order (n = 2). Control engineers typically study$NL$solutions to differential equations using either Laplace transforms or a state space representation.$NL$A short introduction to the Laplace transformation is given in Appendix A for the reader who is$NL$not familiar with its use. It is an integral transformation and its major, but not sole use, is for$NL$differential equations where the independent time variable t is transformed to the complex$NL$variable s by the expression$NL$Since the exponential term has no units the units of s are seconds-1, that is using mks notation s$NL$has units of s-1. If denotes the Laplace transform then one may write$NL$[f(t)] = F(s) and -1[F(s)] = f(t). The relationship is unique in that for every f(t), [F(s)], there is a$NL$unique F(s), [f(t)]. It is shown in Appendix A that when the n-1 initial conditions, Dn-1y(0) are$NL$zero the Laplace transform of Dny(t) is snY(s). Thus the Laplace transform of the differential$NL$equation (2.1) with zero initial conditions can be written$NL$with the assumed notation that signals as functions of time are denoted by lower case letters and$NL$as functions of s by the corresponding capital letter.$NL$If equation (2.8) is written$NL$then this is known as the transfer function, G(s), between the input and output of the ‘system’,$NL$that is whatever is modelled by equation (2.1). B(s), of order m, is referred to as the numerator$NL$polynomial and A(s), of order n, as the denominator polynomial and are from equations (2.2) and$NL$(2.3)$NL$Since the a and b coefficients of the polynomials are real numbers the roots of the polynomials$NL$are either real or complex pairs. The transfer function is zero for those values of s which are the$NL$roots of B(s), so these values of s are called the zeros of the transfer function. Similarly, the$NL$transfer function will be infinite at the roots of the denominator polynomial A(s), and these values$NL$are called the poles of the transfer function. The general transfer function (2.9) thus has m zeros$NL$and n poles and is said to have a relative degree of n-m, which can be shown from physical$NL$realisation considerations cannot be negative. Further for n > m it is referred to as a strictly$NL$proper transfer function and for n 􀂕 m as a proper transfer function.$NL$When the input u(t) to the differential equation of (2.1) is constant the output y(t) becomes$NL$constant when all the derivatives of the output are zero. Thus the steady state gain, or since the$NL$input is often thought of as a signal the term d.c. gain (although it is more often a voltage than a$NL$current!) is used, and is given by$NL$If the n roots of A(s) are 􀄮i , i = 1….n and of B(s) are 􀈕j, j = 1….m, then the transfer function may$NL$be written in the zero-pole form$NL$When the transfer function is known in the zero-pole form then the location of its zeros and poles$NL$can be shown on an s plane zero-pole plot, where the zeros are marked with a circle and the poles$NL$by a cross. The information on this plot then completely defines the transfer function apart from$NL$the gain K. In most instances engineers prefer to keep any complex roots in quadratic form, thus$NL$for example writing
2;The frequency response of a transfer function G(j􀈦) was introduced in the last chapter. As G(j􀈦)$NL$is a complex number with a magnitude and argument (phase) if one wishes to show its behaviour$NL$over a frequency range then one has 3 parameters to deal with the frequency, 􀈦, the magnitude,$NL$M, and the phase 􀄳. Engineers use three common ways to plot the information, which are known$NL$as Bode diagrams, Nyquist diagrams and Nichols diagrams in honour of the people who$NL$introduced them. All portray the same information and can be readily drawn in MATLAB for a$NL$system transfer function object G(s).$NL$One diagram may prove more convenient for a particular application, although engineers often$NL$have a preference. In the early days when computing facilities were not available Bode diagrams,$NL$for example, had some popularity because of the ease with which they could, in many instances,$NL$be rapidly approximated. All the plots will be discussed below, quoting many results without$NL$going into mathematical detail, in the hope that the reader will obtain enough knowledge to know$NL$whether MATLAB plots obtained are of the general shape expected.$NL$A Bode diagram consists of two separate plots the magnitude, M, as a function of frequency and$NL$the phase 􀄳 as a function of frequency. For both plots the frequency is plotted on a logarithmic$NL$(log) scale along the x axis. A log scale has the property that the midpoint between two$NL$frequencies 􀈦1 and 􀈦2 is the frequency 1 2 􀁚􀀠􀁚􀁚. A decade of frequency is from a value to$NL$ten times that value and an octave from a value to twice that value. The magnitude is plotted$NL$either on a log scale or in decibels (dB), where dB M 10 􀀠20log . The phase is plotted on a linear$NL$scale. Bode showed that for a transfer function with no right hand side (rhs) s-plane zeros the$NL$phase is related to the slope of the magnitude characteristic by the relationship$NL$It can be further shown from this expression that a relatively good approximation is that the$NL$phase at any frequency is 15° times the slope of the magnitude curve in dB/octave. This was a$NL$useful concept to avoid drawing both diagrams when no computer facilities were available.$NL$For two transfer functions G1 and G2 in series the resultant transfer function, G, is their product,$NL$this means for their frequency response$NL$which in terms of their magnitudes and phases can be written$NL$Thus since a log scale is used on the magnitude of a Bode diagram this means Bode magnitude$NL$plots for two transfer functions in series can be added, as also their phases on the phase diagram.$NL$Hence a transfer function in zero-pole form can be plotted on the magnitude and phase Bode$NL$diagrams simple by adding the individual contributions from each zero and pole. It is thus only$NL$necessary to know the Bode plots of single roots and quadratic factors to put together Bode plots$NL$for a complicated transfer function if it is known in zero-pole form.$NL$The single pole transfer function is normally considered in time constant form with unit steady$NL$state gain, that is$NL$It is easy to show that this transfer function can be approximated by two straight lines, one$NL$constant at 0 dB, as G(0) = 1, until the frequency, 1/T, known as the break point, and then from$NL$that point by a line with slope -6dB/octave. The actual curve and the approximation are shown in$NL$Figure 4.1 together with the phase curve. The differences between the exact magnitude curve and$NL$the approximation are symmetrical, that is a maximum at the breakpoint of 3dB, 1dB one octave$NL$each side of the breakpoint, 0.3 dB two octaves away etc. The phase changes between 0° and -$NL$90° again with symmetry about the breakpoint phase of -45°. Note a steady slope of -6 dB/octave$NL$has a corresponding phase of -90°$NL$The Bode magnitude plot of a single zero time constant, that is$NL$is simply a reflection in the 0 dB axis of the pole plot. That is the approximate magnitude curve is$NL$flat at 0 dB until the break point frequency, 1/T, and then increases at 6 dB/octave. Theoretically$NL$as the frequency tends to infinity so does its gain so that it is not physically realisable. The phase$NL$curve goes from 0° to +90°$NL$The transfer function of an integrator, which is a pole at the origin in the zero-pole plot, is 1/s. It$NL$is sometimes taken with a gain K, i.e.K/s. Here K will be replaced by 1/T to give the transfer$NL$function$NL$On a Bode diagram the magnitude is a constant slope of -6 dB/octave passing through 0 dB at the$NL$frequency 1/T. Note that on a log scale for frequency, zero frequency where the integrator has$NL$infinite gain (the transfer function can only be produced electronically by an active device) is$NL$never reached. The phase is -90° at all frequencies. A differentiator has a transfer function of sT$NL$which gives a gain characteristic with a slope of 6 dB/octave passing through 0dB at a frequency$NL$of 1/T. Theoretically it produces infinite gain at infinite frequency so again it is not physically$NL$realisable. It has a phase of +90° at all frequencies.$NL$The quadratic factor form is again taken for two complex poles with 􀈗 < 1 as in equation (3.7),$NL$that is$NL$Again G(0) = 1 so the response starts at 0 dB and can be approximated by a straight line at 0 dB$NL$until 􀈦o and by a line from 􀈦o at -12 dB/octave. However, this is a very coarse approximation as$NL$the behaviour around 􀈦o is highly dependent on 􀈗. It can be shown that the magnitude reaches a$NL$maximum value of$NL$which is approximately 1/2􀈗 for small 􀈗, at a frequency$NL$of$NL$This frequency is thus always less than 􀈦o and only exists for 􀈗 < 0.707.$NL$The response with 􀈗 = 0.707 always has magnitude, M < 1. The phase curve goes from 0° to -$NL$180° as expected from the original and final slopes of the magnitude curve, it has a phase shift of$NL$-90° at the frequency 􀈦o independent of 􀈗 and changes more rapidly near 􀈦o for smaller 􀈗, as$NL$expected due to the more rapid change in the slope of the corresponding magnitude curve.
3;The Media Browser appears in many of Apple’s application software. It is a system wide utility that is accessed from within$NL$programs such as Keynote. It has a distinct icon displaying a frame of film, a picture frame and two musical notes. This$NL$icon appears in different places in different applications.$NL$The Media Browser can be launched from the Toolbar or View > Media Browser. It contains three tabs, Audio, Photos$NL$and Movies.$NL$Audio – This contains tracks from the current user’s iTune account. Garage band projects, these might include voice–overs$NL$or podcasts and audio from other applications such as Aperture or Final Cut.$NL$Photos – This contains images stored in iPhoto or Aperture is it is installed$NL$Movies – This contains video files stored in iMovie, the current user’s Movies Folder, or in iTunes. If Final Cut or Aperture$NL$are installed video files from those applications will also appear here.$NL$The bottom pane of the Media Browser contains thumbnails of the respective media files. Double–clicking the thumbnail$NL$previews the file. In the case of Photos it enlarges the thumbnail to fill the Media Browser pane.$NL$As with all iWork applications, Keynote stores graphics and other media within the file itself, with the exception of Fonts.$NL$This makes for easy transfer of files from workstation to laptop et cetera. A consequence of this is that iWork documents$NL$may have large files sizes though there is an easy remedy in the Reduce File Size command. This will be illustrated in$NL$Section 16 — Sharing Your Work.$NL$To insert a media file or files select them in the Media Browser and drag them onto the Keynote Slide Canvas.$NL$When images are placed into Keynote they may look too dark or lack contrast. Like all iWork applications there is an$NL$Adjust Image window that can be used to adjust the brightness, contrast, saturation and other image parameters directly$NL$within Keynote. There are several image parameters that can be used to finesse imported photographs though often just$NL$clicking the Enhance button will improve a picture.$NL$Image Adjust is an icon in the Toolbar, or can be launched from View > Adjust.$NL$When you have taken the time to format a picture frame or text box there is no need to repeat all the steps taken when$NL$you wish to apply that style to subsequent objects. Styles can be copied and pasted. Start this process by creating two text$NL$boxes. Use the Graphics Inspector to stylize one box. Ensure that Text Box is selected then go to Format > Copy Style.$NL$Select the second text box and go to Format > Paste Style. The second Text Box will take on the general appearance of$NL$the stylized one.$NL$There are several methods for preparing presentations. With presentations planning and rehearsal are essential. As$NL$previously stated this guide does not aim to explain the art of presentation. Information on presentation creation and$NL$delivery can be found on the web with several sites offering Keynote tips and techniques. Suffice to say planning is required$NL$before opening Keynote.$NL$Keynote can help structure planning processes, once you have a concept in mind. Outline mode is a particularly useful$NL$way to list bullet points as hierarchies and sequences when creating a presentation from scratch.$NL$There are five stages in the life of a presentation:$NL$1. Planning – What you need to do before opening Keynote and using Keynote in Outline mode.$NL$2. Construction – Using Outline, Navigator and Light Table modes to build and sequence the presentation.$NL$3. Rehearsal — For presentations that are to be delivered to an audience rehearsal is very important. Keynote has$NL$a rehearsal mode that can be used to check timings.$NL$4. Delivery — After rehearsals comes the main event often in front of an audience. Being able to stop and jump to$NL$different slides is a useful skill. Keynote presentations don’t have to be delivered in person. Using Kiosk mode$NL$allows users to navigate through presentation material on their own.$NL$5. Legacy – Making a presentation may be sufficient, though legacy items can be created, such as handouts or$NL$narrated versions of the presentation in video form that can be accessed on-demand, by whoever.$NL$Keynote ships with forty-four Apple designed themes. Themes are collections of Master Slides, for example, titles slides,$NL$bullets, bullets with photos, and so on. Themes have a coordinated design for fonts, colours and style across their respective$NL$Master Slides.$NL$An Apple Designed Theme may be a perfect starting point for a presentation, though users can generate and save their$NL$own themes, or purchase extra themes online. Defining Master Slides and creating custom themes will be explored later.$NL$Some Keynote Themes only have two resolutions, 800 x 600 pixels and 1024 x 768 pixels. Newer Themes come in three$NL$additional sizes 1280 x 720, 1920 x 1080†, and 1680 x 1050. Although presentations can be set to playback so that they$NL$fill the screen, choosing an option with sufficient resolution at an appropriate aspect ratio is very important.$NL$The Theme Chooser contains all the themes currently available including any custom ones you might make. To see$NL$previews of all the Master Slides in a Theme, skim the cursor over the Theme’s thumbnail. Before selecting a slide select$NL$the correct slide resolution from the Slide SIze Pop–up menu.$NL$Master Slides are preset layouts for title slides, bullet point slides, bullets with a picture and so on. Themes are a collection$NL$of Master Slides. The Master icon in the toolbar can be used to change one or more slides to a different master layout.$NL$The View icon in the toolbar can be used to Show Master Slides. Click dragging Master Slide thumbnails onto existing$NL$slides thumbnails changes their master layout.$NL$Although many users begin making their Keynote, or Powerpoint, presentations in slide view, Outline view is a great way$NL$to structure ideas. iWork’s Pages has several Outline templates to help with this task.$NL$Outline view is one of several options that can be found under the View menu. If the default Outline view is not suitable it$NL$can be adjusted. To make the Slide pane areas bigger move the cursor over the resize handle and click drag to adjust its size.
3;This guide explains the processes used to make Keynote documents. Keynote is Apple Inc.’s equivalent to Microsoft’s$NL$Powerpoint. Keynote’s strength is its ease of use and its ability to handle a variety of media types, including HD Video.$NL$This guide to Keynote is one of three books I have written for Bookboon on iWork. My books on Pages and Numbers$NL$complement this one, with some areas of repetition, each guide is designed to stand alone.$NL$A great way to learn is to experiment and play. Use this guide to focus your learning on specific areas of Keynote before$NL$taking a broad view of the myriad of possibilities for this software.$NL$This guide describes ways to assemble and edit content. It does not seek to give advice on presentation methodology, too$NL$often business presentations suffer from densely packed slides, with too much text and statistical information squeezed$NL$into them information that is best left for a report. Rather than cram the contents of a report onto a handful of slides,$NL$judicious planning will make for compelling presentations. Keynote is not a place to copy and paste all the text of a report.$NL$It is far better to highlight and illustrate important points using Keynote’s excellent graphic and animation capabilities. That$NL$said, animation and graphic elements should be used sparingly to aid the communication, and not as a distraction to it.$NL$Keynote can be used to create dynamic and engaging presentations. Text, images and charts can be arranged with ease.$NL$Keynote makes it easy to add audio and video, add transitions between slides, animate data, and then share presentations$NL$in a variety of ways.$NL$The guide describes software functions and outlines generic examples of the software in use. Further information can be$NL$found on Apple’s web pages, or via Apple’s Certified Training Scheme.$NL$Regarding keyboard shortcuts. The keyboard shortcuts mentioned in this book will work on International English$NL$QWERTY keyboards. For US keyboards the only difference is that Alt key (􀀂 ) is called Option (􀀂 ). For AZERTY and$NL$other language keywords please try the shortcuts, they will probably work.$NL$For seasoned Mac users please note that the Apple key is now referred to as the Command key. It is labelled cmd , not .$NL$There are three ways to launch Keynote.$NL$• Go to your Applications folder. In Finder choose Go > Applications. Open the iWork ’09 folder and double–$NL$click the Keynote icon. (Unless you have done a customized iWork installation the iWork folder will be in the$NL$Application folder found in the root of your primary hard drive.)$NL$• In the Dock, click the Keynote icon. (In Apple’s latest operating system named Lion, a Keynote icon will appear$NL$in Launchpad.)$NL$• Double–click any Keynote document.$NL$Every time you launch Keynote or try to create a new document, Keynote’s Template Chooser appears. The Template$NL$Chooser contains Apple designed templates and any Templates created by you. Apple’s Templates can be customized to$NL$suit your tastes or to comply with a business’s graphic identity. There will be more on Themes later. The White and Black$NL$Themes are good options to experiment with.$NL$Keynote has features that are shared with, or are similar to, features found in other Apple applications such as Numbers and$NL$Pages. This section lists these features. Understanding features including Inspectors and the Media Browser are essential$NL$when learning about any Apple software.$NL$To explore the features described in Section 1. launch Keynote and open any Template. When launching Keynote the$NL$following message may appear.$NL$By default Keynote’s window contains a customizable Tool Bar, a Format Bar, a Slides Pane, and the Slide Canvas. Other$NL$panes may be opened, including Master Slides Pane, and Presenter Notes Pane.$NL$The Tool Bar contains several icons. These control common functions and will be described later. Note that some Tool$NL$Bar Icons are greyed-out meaning they cannot be used. They become active once an Object is selected. Also note that the$NL$Tool Bar can be customized to display buttons for commands based on user preference. From left to right:$NL$• New – This adds a new slide. The keyboard shortcut is Command – Shift – N.$NL$• Play – This starts the presentation in Slideshow, at the selected slide, the keyboard shortcut is Command–Alt–P.$NL$To start a Slideshow from the first slide hold down the Alt key and click the Play button in the Tool Bar.$NL$• View – changes the View Mode. Options include Navigator, Outline, Slide Only and Light Table. Of these$NL$Navigator and Light Table are the most useful when constructing documents. Outline Mode will be discussed$NL$further in Section 5.4. View also controls the display of Rulers, Format Bat, Presenter Notes and Master Slides.$NL$When developing a presentation to support a speech or lecture using Presenter Notes can prove a great aid to memory.$NL$Keynote can be configured to display on two displays simultaneously, one showing slides to an audience and a comprehensive$NL$display of Presenter Notes, Next Slide and Time Elapsed or Remaining on the other.$NL$Comments can be temporarily hidden from the View Menu. Comments are like virtual sticky notes and can be found in$NL$all the iWork applications.$NL$• Guides – By default Keynote displays alignment guides to help layout. These appear as yellow lines that indicate$NL$whether an object is aligned to the top, bottom, centre or side of another object, or where it is in relation to$NL$the canvas. Choosing all four options from Show Guides at Object Center to Show Relative Sizes will give$NL$maximum layout feedback help.$NL$• Themes – On launching Keynote a Theme Chooser appears. Users can create their own Themes. Having a$NL$Theme button in the Tool Bar allows Themes to be swapped in an open document. This function is useful for$NL$companies wishing to update presentations to meet their latest visual identity guidelines. A presentation using$NL$their properly applied 2008 theme can be updated to their 2012 theme in a single click.$NL$• Masters – Themes contain several Master Slides Title & Subtitle, Title & Bullets, Title & Bullets – 2 Column…$NL$et cetera. Use this menu to change slides to different Masters.$NL$• Text Box – This adds a simple text box to the slide canvas. It is better to use a slide Master with a Text Box and$NL$modify it. This makes the Theme change function work speedily. It isn’t wrong, nor does it expose weaknesses$NL$in the Keynote software, to add Text Boxes. Additional Text Boxes cannot be automatically changed when$NL$changing Themes.
3;When the View is set to Navigator, the pane to the left of the Slide Canvas is labelled Slides. Change the View to Outline$NL$and the pane is labelled Outline. The other view modes include Slide Only and Light Table. The Navigator View is probably$NL$the best option for creating and editing presentations, as thumbnails of the slide appear in the Slides Pane. This allows$NL$thumbnails to be click–dragged into new positions and slides to be duplicated and deleted with ease.$NL$Click-dragging a slide in Navigator View will reposition it in the running order. Holding down the Alt key whilst doing$NL$this will copy the slide to a new location. As with nearly all keyboard short–cuts the modifier key, in this case the Alt$NL$Key, should only be released after the mouse button is released. Other operations can be accessed using right–click on$NL$the Slides Pane.$NL$Other operations include the ability to skip slides. This is useful when shorter versions of a presentation are required$NL$perhaps a thirty minute slot has been cut to twenty. The Skip function is a safe way to trim a presentation without deleting$NL$any data. Skipped slides won’t display as part of a Slide Show, but can be easily un–skipped too the context menu command$NL$for this is Don’t Skip Slide.$NL$In teaching scenarios a Keynote can be prepared with extra sections that might be useful in class, but can be set to Skip$NL$by default, and only revealed if the teacher thinks it prudent to explore such areas. Skipping slides avoids the need to have$NL$several similar Keynote documents on the same subject. So rather than having to update several Keynotes as data changes,$NL$just the one document needs updating and the Skip function helps tailor the presentation for a variety of audiences and$NL$durations.$NL$Only revealed when the View is set to Show Master Slides, the Master Slide Pane appears above the Slides or Outline$NL$Pane to the left of the Slide Canvas. From here the current slide’s format can be changed from one Master to another.$NL$Simply click the slide or slides to be changed and click on the desired Master. This operation can also be achieved using$NL$the Master button in the Tool Bar.$NL$The Presenter Notes Pane appears under the Slide Canvas when set from the View menu in the Tool Bar.$NL$Presenter notes can be copied and pasted from other applications such as Pages or typed directly into the Presenter Notes$NL$Pane. They can be a great aid to memory when making presentations. Presenter Notes can also be included in handouts$NL$printed from Keynote.$NL$The Inspector is a key feature in Apple’s iLife and iWork software. Users new to Apple Mac software need to embrace the$NL$use of the Inspector. Inspectors control nearly all the parameters in iWork software. Numbers, Pages and Keynote have$NL$some unique Inspectors and some in common. For instance all the applications have a Document Inspector containing$NL$slightly different parameters in each.$NL$The Inspector is launched by either clicking the Inspector icon in the Toolbar or View > Show Inspector.$NL$Keynote has ten inspector tabs. They are:$NL$• Document Inspector –$NL$The Document Inspector should be used when first setting up a document. Divided into three tabs. The first tab contains$NL$slideshow preferences with options for Looping the slideshow. From here the Presentation mode can be changed, along$NL$with the Slide Size.$NL$The bottom section is Require Password To Open. For additional security a password can be entered to lock a Keynote$NL$document. The password would then be required every-time the document is opened.$NL$The next tab is Audio. There are several ways to add audio to a presentation, and using this tab this may not be the most$NL$flexible option.$NL$The last tab is Spotlight. Completing the fields for Author, Title, Keywords, and Comments is recommended practice. It$NL$makes documents easier to search for, especially when using the Mac Spotlight search engine. All of these fields are for$NL$metadata. Author and Title are self-explanatory. Adding Keywords helps classify a document. For example a Keynote$NL$document designed for a seminar on Apple’s Aperture could have the keywords, ‘Apple’, ‘Aperture’, ‘photography’, ‘digital$NL$imaging’ and ’45 minutes’. The list could go on. There are five keywords here. Keywords are denoted by commas, ‘Apple,$NL$Aperture, photography’ ‘digital imaging’ and ‘45 minutes’ are all single keywords.$NL$Searches can be case-sensitive, but using ‘apple’ and ‘Apple’ as keywords is not strictly necessary. When choosing keywords$NL$it is helpful to invoke the spirit of the librarian. A less rigorous form of applying metadata is using the Comments field. Here$NL$paragraphs of descriptive text can add be added. In the example of the Apple Aperture Keynote document, the comments$NL$field could read, ‘Seminar Presentation for Apple’s Aperture, aimed at the serious amateur and professional audience…’$NL$• Slide Inspector –$NL$The Slide Inspector has two tabs, Transition and Appearance. Transitions can be applied to one or more slides at a time.$NL$There are currently forty-eight transition effects and something known as Magic Move, located in the Effect menu of the$NL$Transition tab. Experiment with the transitions to learn more, try changing the transition duration as this may enhance the$NL$effect. Some transitions have a direction, moving from left to right, or top to bottom. When this is the case the Direction$NL$menu becomes active. The Start Transition menu is set to On Click by default, meaning that to move the the next slide a$NL$mouse click or an alternative to click has to be made†. Transitions can be triggered in other ways, as listed in the Start$NL$Transition menu.$NL$† Alternatives to a mouse click, include clicking a trackpad, tapping the Spacebar, using a keyboard’s forward arrow key,$NL$using a remote control device
3;Spreadsheet tables can be created, complete with calculations, directly in Keynote. Clicking the Table icon in the Toolbar$NL$places a basic three column by three row table onto a slide. The Table Inspector can be used to modify that Table adding$NL$extra rows and columns as needed. The Table Inspector be used to format the Table. However in most circumstances$NL$spreadsheet data is best copied and pasted into slides from iWork’s Numbers.$NL$As Numbers is a dedicated spreadsheet package it offers far more data editing and function control than Keynote. Having$NL$Table creation control in Keynote is great though the primary reason for having this capability is to allow table and chart$NL$data to be modified within Keynote. For instance, if sales figures change immediately before a business presentation, the$NL$table and chart data can be changed in Keynote without the need to locate the original spreadsheet data.$NL$Clicking on any Table Cell that contains a formula invokes the Formula Editor. Once active the Formula Editor can be$NL$used to modify equations. If Numbers is not available, create a simple table, select a cell and press the equals (=) key.$NL$In common with the other iWork applications Keynote runs automatic spell checking. To switch this off go to Edit >$NL$Spelling > Check Spelling As You Type and mark sure it is un–ticked. When on potentially misspelt words are automatically$NL$underlined with a red dotted line. Using the context menu spelling suggestions are offered. The context menu also allows$NL$words to be learnt. To invoke the Context Menu use Right Click or Control Click. This context menu also launches the$NL$Mac’s dictionary and thesaurus, and even links to Wikipedia, or the Google search engine.$NL$To run spell check across a document go to Edit > Spelling >Spelling… The spell check dialogue box appears.$NL$Automatic Corrections can be controlled from Keynotes Preferences. For example typing ‘teh’ will automatically change$NL$to ‘the’. Also, scientific terms or business names can be abbreviated. Here typing ‘mwp’ will automatically change to$NL$Mark Wood | Photography. Genuine fractions can be achieved but only with certain font types. Once enabled Automatic$NL$Corrections, Symbol and Text Substitution will replace 1/3 with ⅓, but only if the chosen font contains glyphs for fractions.$NL$If the chosen font does not contain the required glyph an alternative font is used.$NL$Photographs, Movies and Sound can be added to a Keynote document in one of four different ways.$NL$1. The Media Browser$NL$2. The Insert menu and Choose…$NL$3. Dragging files from Finder$NL$4. Copying and Pasting from other applications$NL$The hassle free method is to use Media Browser, as it connects to other Apple software including iTunes. The available$NL$media will be in a Keynote compatible format. To include files from the Media Browser locate the correct tab for the$NL$media type, either Photos, Movies, Audio, then find the desired file and drag it on to a Keynote slide.$NL$Using the Insert command from the application menu bypasses the need to have files included in the Media Browser, but$NL$be aware that not all picture, movie and audio formats are supported by iWork and Keynote. If a file appears greyed out$NL$in the Finder Import window it means that file format is not supported.$NL$Supported file formats are:$NL$For Pictures – all QuickTime-supported formats, including TIFF, GIF, JPEG, PDF, PSD, EPS, PICT$NL$For Movies and Audio - any QuickTime or iTunes file type, including MOV, MP3, MPEG-4, AIFF and AAC$NL$Supported files can be simply dragged from a Finder window onto a Slide. If the contents of an entire folder are required,$NL$drag the folder from its Finder window onto the Media Browser. For a folder of mixed media, being Pictures, Movies and$NL$Audio this operation will have to be executed three times. Once for each tab of the Media Browser.$NL$$NL$If images placed onto slides look too dark or lack contrast Keynote has an Adjust Image control. This is a window that$NL$can be used to adjust the brightness, contrast, saturation and other image parameters directly within Keynote. There$NL$are several image parameters that can be used to finesse imported photographs, though often just clicking the Enhance$NL$button will improve a picture.$NL$Adjust Image is an icon in the Toolbar, or it can be launched from View > Adjust.$NL$By default Keynote’s preferences are set to include audio and movies in the document file. This option, found in the General$NL$tab of Keynote’s Preferences, should be left ticked its default. This ensures media is saved within Keynote documents.$NL$There is no mechanism for excluding picture data from automatically being saved in Keynote.$NL$Because Keynote embeds media files, presentations can become rather bloated, filling hard drive space. On modern$NL$workstations this is not a big problem but for the fast transmission of a Keynote via a network, file size can jeopardize$NL$smooth playback.$NL$All iWork applications have a Reduce File Size command found in the File menu. Use this to resample media files contained$NL$in Keynote. Caution: do not overwrite, that is save over, the original Keynote file. Using Reduce File Size may reduce the$NL$fidelity of media during resampling. This may not be an issue, but keeping the higher quality Keynote is a sensible precaution.$NL$Global Transitions can be added to slides. They help to indicate a change is taking place. The Dissolve Effect set to two$NL$or three seconds makes a pleasing transition from one picture slide to another. With so many transitions it is all too easy$NL$to overload a presentation with effects, and so obscure the intended message.$NL$Often transitions are triggered by a click, though they can also be set to change after a time delay. Mixing click-triggered$NL$transitions with time-delayed transitions can be confusing to a presenter. It is probably best to use click-triggering and$NL$no delays because often when a computer appears unresponsive users start clicking freely and frustratedly.
4;In Chapter Two, we see that class attributes are implemented in Java programmes as variables, whose$NL$values determine the state of an object. To some extent Chapter Two addresses the question of how we$NL$name variables this question is explored further in this chapter.$NL$Chapter Three explores some of the basic elements of the Java language. Given the nature of this guide, it$NL$is not the intention to make this chapter exhaustive with respect to all of the basic elements of the Java$NL$language. Further details can be found in the on-line Java tutorial.$NL$We see in Chapter Two that the two broad categories of Java types are primitives and classes. There are$NL$eight of the former and a vast number of classes, including several thousand classes provided with the$NL$Java language development environment and an infinitude of classes written by the worldwide community$NL$of Java developers. This chapter examines aspects of both categories of types.$NL$An identifier is a meaningful name given to a component in a Java programme. Identifiers are used to$NL$name the class itself – where the name of the class starts with an upper case letter – and to name its$NL$instances, its methods and their parameters. While class identifiers always – by convention – start with an$NL$upper case letter, everything else is identified with a word (or compound word) that starts with a lower$NL$case letter. Identifiers should be made as meaningful as possible, in the context of the application$NL$concerned. Thus compound words or phrases are used in practice.$NL$Referring to elements of the themed application, we can use the following identifiers for variables in the$NL$Member class:$NL$because we wouldn’t name a class membershipCard and spaces are not permitted in identifiers.$NL$We could have declared other variables in the class definition as follows:$NL$We cannot use what are known as keywords for identifiers. These words are reserved and cannot be$NL$used solely as an identifier, but can be used as part of an identifier. Thus we cannot identify a variable$NL$as follows:$NL$// not permitted because int is a keyword$NL$but we could write$NL$The table below lists the keywords in the Java language.$NL$Java is case-sensitive: this means that we cannot expect the following statement to compile:$NL$if we have not previously declared the identifier newint. On the other hand, if we write$NL$as the last statement of the getNewInt method, it will compile because the identifier named newInt has$NL$been declared previously.$NL$Similarly we cannot expect the compiler to recognise identifiers such as the following$NL$if they have not been declared before we refer to them later in our code.$NL$In one of the declarations in Section 3.2, we declared a variable with the identifier newInt to be of the int$NL$type, in the following statement:$NL$Let us deconstruct this simple statement from right to left: we declare that we are going to use an$NL$identifier named newInt to refer to integer values and ensure that access to this variable is private.$NL$This kind of declaration gives rise to an obvious question: what primitive data types are there in the Java$NL$language? The list on the next page summarises the primitive data types supported in Java.$NL$Before we move on to discuss assignment of actual values to variables, it will be instructive to find out if$NL$Java can convert between types automatically or whether this is left to the developer and if compile-time$NL$and run-time rules for conversion between types are different.$NL$In some situations, the JRE implicitly changes the type without the need for the developer to do this. All$NL$conversion of primitive data types is checked at compile-time in order to establish whether or not the$NL$conversion is permissible.$NL$Consider, for example, the following code snippet:$NL$A value of 10.0 is displayed when d is output.$NL$Evidently the implicit conversion from an int to a double is permissible.$NL$Consider this code snippet:$NL$The first statement compiles this means that the implicit conversion from an int to a double is permissible$NL$when we assign a literal integer value to a double. However the second statement does not compile: the$NL$compiler tells us that there is a possible loss of precision. This is because we are trying to squeeze, as it$NL$were, an eight byte value into a four byte value (see Table 3.2) the compiler won’t let us carry out such a$NL$narrowing conversion.$NL$On the other hand, if we write:$NL$// the cast ( int ) forces d to be an int we will examine the concept of casting$NL$// or explicit conversion later in this section$NL$Both statements compile and a value of 10 is displayed when i is output.$NL$The general rules for implicit assignment conversion are as follows:$NL$􀁸a boolean cannot be converted to any other type$NL$􀁸a non-boolean type can be converted to another non-boolean type provided that$NL$the conversion is a widening conversion$NL$􀁸a non-boolean type cannot be converted to another non-boolean type if the$NL$conversion is a narrowing conversion.$NL$Another kind of conversion occurs when a value is passed as an argument to a method when the method$NL$defines a parameter of some other type.$NL$For example, consider the following method declaration:$NL$The method is expecting a value of a double to be passed to it when it is invoked. If we pass a float to the$NL$method when it is invoked, the float will be automatically converted to a double.$NL$Fortunately the rules that govern this kind of conversion are the same as those for implicit assignment$NL$conversion listed above.$NL$The previous sub-section shows that Java is willing to carry out widening conversions implicitly. On the$NL$other hand, a narrowing conversion generates a compiler error. Should we actually intend to run the risk of$NL$the possible loss of precision when carrying out a narrowing conversion, we must make what is known as$NL$an explicit cast. Let us recall the following code snippet from the previous sub-section:$NL$Casting means explicitly telling Java to force a conversion that the compiler would otherwise not carry out$NL$implicitly. To make a cast, the desired type is placed between brackets, as in the second statement above,$NL$where the type of d – a double - is said to be cast (i.e. flagged by the compiler to be converted at run-time)$NL$into an int type.
4;There are several examples in previous chapters that illustrate how constructors are used to instantiate$NL$objects of a class. Let us recall the overall technique before we bring together a number of features of$NL$constructors in this chapter.$NL$One of the constructors for Member objects in the themed application is as follows:$NL$An object’s constructors have the same name as the class they$NL$instantiate.$NL$To access an object of the class Member in an application, we first declare a variable of the Member type$NL$in a main method in a test class as follows:$NL$The statement above does not create a Member object it merely declares a variable of the required type$NL$that can subsequently be initialised to refer to an instance of the Member type. The variable that refers to$NL$an object is known as its object reference. The object that an object reference refers to must be created$NL$explicitly, in a statement that instantiates a Member object as follows.$NL$The two statements above can be combined as follows.$NL$When the Member object is created by using ‘new’, the type of object required to be constructed is$NL$specified and the required arguments are passed to the constructor. The JRE allocates sufficient memory$NL$to store the fields of the object and initialises its state. When initialisation is complete, the JRE returns a$NL$reference to the new object. Thus, we can regard a constructor as returning an object reference to the$NL$object stored in memory.$NL$While objects are explicitly instantiated using ‘new’, as shown above for a Member object, there is no$NL$need to explicitly destroy them (as is required in some OO run-time systems). The Java Virtual Machine$NL$(JVM) manages memory on behalf of the developer so that memory for objects that is no longer used in an$NL$application is automatically reclaimed without the intervention of the developer.$NL$In general, an object’s fields can be initialised when they are declared or they can be declared without$NL$being initialised. For example, the code snippet on the next page shows part of the class declaration for a$NL$version of the Member class:$NL$The code snippet illustrates an example where some of the instance variables are initialised and some are$NL$only declared. In the case of the latter type of declaration, the instance variable is initialised to its default$NL$value when the constructor returns an object reference to the newly-created object. For example, the$NL$instance variable noOfCards is initialised to 0 when the object is created.$NL$Declaring and initialising none, some or all instance variables in this way if often sufficient to establish$NL$the initial state of an object. On the other hand, where more than simple initialisation to literals or default$NL$values is required and where other tasks are required to be performed, the body of a constructor can be$NL$used to do the work of establishing the initial state of an object. Consider the following part of the$NL$constructor for the Member class.$NL$This constructor is used when simple initialisation of Member objects is insufficient. Thus, in the code$NL$block of the constructor above, the arguments passed to the constructor are associated with four of the$NL$fields of the Member class. The effect of the four statements inside the constructor’s code block is to$NL$initialise the four fields before the constructor returns a reference to the object.$NL$Constructors can, like methods, generate or throw special objects that represent error conditions. These$NL$special objects are instances of Java’s in-built Exception class. We will explore how to throw and detect$NL$Exception objects in Chapter Four in An Introduction to Java Programming 2: Classes in Java$NL$Applications.$NL$It is worthwhile being reminded at this point in the discussion about constructors that the compiler inserts$NL$a default constructor if the developer has not defined any constructors for a class.$NL$The default constructor takes no arguments and contains no code. It$NL$is provided automatically only if the developer has not provided any$NL$constructors in a class definition.$NL$We saw in the previous chapter that methods can be overloaded. Constructors can be similarly overloaded$NL$to provide flexibility in initialising the state of objects of a class. For example, the following class$NL$definition includes more than one constructor.$NL$The example class – SetTheTime – is a simple illustration of a class which provides more than one$NL$constructor. The example also shows that a constructor can be called from the body of another constructor$NL$by using the ‘this’ invocation as the first executable statement in the constructor. Thus, in the example$NL$above, the two argument constructor is called in the first statement of the three argument constructor.$NL$Complex initialisation of fields can be achieved by using what is known as an initialisation block. An$NL$initialisation block is a block of statements, delimited by braces, that appears near the beginning of a class$NL$definition outside of any constructor definitions. The position of such a block can be generalised in the$NL$following simple template for a typical class definition:$NL$An initialisation block is executed as if it were placed at the beginning of every constructor of a class. In$NL$other words, it represents a common block of code that every constructor executes.$NL$Thus far, in this study guide, we have only been able to work with single values of primitive data types$NL$and object references. In the next chapter, we will find out how we can associate multiple values of types$NL$with a single variable so that we can work with multiple values of primitives or object references in$NL$an application.
4;While there is a study guide (available from Ventus) that focuses largely on objects and their$NL$characteristics, it will be instructive to the learner (of the Java programming language) to understand how$NL$the concept of an object is applied to their construction and use in Java applications. Therefore, Chapter$NL$One (of this guide) introduces the concept of an object from a language-independent point of view and$NL$examines the essential concepts associated with object-oriented programming (OOP) by briefly comparing$NL$how OOP and non-OOP approach the representation of data and information in an application. The$NL$chapter goes on to explain classes, objects and messages and concludes with an explanation of how a class$NL$is described with a special diagram known as a class diagram.$NL$$NL$Despite the wide use of OOP languages such as Java, C++ and C#, non-OOP languages continue to be$NL$used in specific domains such as for some categories of embedded applications. In a conventional,$NL$procedural language such as C, data is sent to a procedure for processing this paradigm of information$NL$processing is illustrated in Figure 1.1 below.$NL$$NL$The figure shows that the number 4 is passed to the function (SQRT) which is ‘programmed’ to calculate$NL$the result and output it (to the user of the procedure). In general, we can think of each procedure in an$NL$application as ready and waiting for data items to be sent to them so that they can do whatever they are$NL$programmed to do on behalf of the user of the application. Thus an application written in C will typically$NL$comprise a number of procedures along with ways and means to pass data items to them.$NL$$NL$The way in which OOP languages process data, on the other hand, can be thought of as the inverse of the$NL$procedural paradigm. Consider Figure 1.2 below.$NL$$NL$In the figure, the data item – the number 4 – is represented by the box (with the label ‘4’ on its front face).$NL$This representation of the number 4 can be referred to as the object of the number 4. This simple object$NL$doesn’t merely represent the number 4, it includes a button labeled sqrt which, when pressed, produces$NL$the result that emerges from the slot labeled return.$NL$Whilst it is obvious that the object-oriented example is expected to produce the same result as that for the$NL$procedural example, it is apparent that the way in which the result is produced is entirely different when$NL$the object-oriented paradigm considered. In short, the latter approach to producing the result 2 can be$NL$expressed as follows.$NL$$NL$A message is sent to the object to tell it what to do. Other messages might press other buttons associated$NL$with the object. However for the present purposes, the object that represents the number 4 is a very simple$NL$one in that it has only one button associated with it. The result of sending a message to the object to press$NL$its one and only button ‘returns’ another object. Hence in Figure 1.2, the result that emerges from the$NL$‘return’ slot - the number 2 – is an object in its own right with its own set of buttons.$NL$Despite the apparent simplicity of the way in which the object works, the question remains: how does it$NL$calculate the square root of itself? The answer to this question enshrines the fundamental concept$NL$associated with objects, which is to say that objects carry their programming code around with them.$NL$Applying this concept to the object shown in Figure 1.2, it has a button which gives access to the$NL$programming code which calculates the square root (of the number represented by the object). This$NL$amalgam of data and code is further illustrated by an enhanced version of the object shown in Figure$NL$1.3 below.$NL$$NL$The enhanced object (representing the number 4) has two buttons: one to calculate the square root of itself$NL$– as before - and a second button that adds a number to the object. In the figure, a message is sent to the$NL$object to press the second button – the button labeled ‘+’ – to add the object that represents the number 3$NL$to the object that represents the number 4. For the ‘+’ button to work, it requires a data item to be sent to it$NL$as part of the message to the object. This is the reason why the ‘+’ button is provided with a slot into$NL$which the object representing the number 3 is passed. The format of the message shown in the figure can$NL$be expressed as follows.$NL$$NL$When this message is received and processed by the object, it returns an object that represents the number$NL$7. In this case, the message has accessed the code associated with the ‘+’ button. The enhanced object can$NL$be thought of as having two buttons, each of which is associated with its own programming code that is$NL$available to users of the object.$NL$$NL$Extrapolating the principal of sending messages to the object depicted in Figure 1.3 gives rise to the$NL$notion that an object can be thought of as comprising a set of buttons that provide access to operations$NL$which are carried out depending on the details in the messages sent to that object.$NL$$NL$In summary:$NL$􀁸in procedural programming languages, data is sent to a procedure$NL$􀁸in an object-oriented programming language, messages are sent to an object$NL$􀁸an object can be thought of as an amalgam of data and programming code: this is known as$NL$encapsulation.$NL$$NL$Whilst the concept of encapsulation is likely to appear rather strange to learners who are new to OOP,$NL$working with objects is a much more natural way of designing applications compared to designing them$NL$with procedures. Objects can be constructed to represent anything in the world around us and, as such,$NL$they can be easily re-used or modified. Given that we are surrounded by things or objects in the world$NL$around us, it seems natural and logical that we express this in our programming paradigm.$NL$$NL$The next section takes the fundamental concepts explored in this section and applies them to a simple$NL$object.
4;The aim of Chapter Two is to take the simple class diagram shown at the end of Chapter One and explain$NL$how it is translated into Java source code. The code is explained in terms of its attributes, constructor and$NL$behaviour and a test class is used to explain how its constructor and behaviour elements are used.$NL$Before we embark on our first Java programme, let us recall the class diagram with which we concluded$NL$Chapter One. The class diagram is reproduced in Figure 2.1 below, with the omission of the constructor:$NL$this is to keep the code simple to begin with. We will replace the constructor in the class diagram and$NL$provide code for it later in this chapter.$NL$In Figure 2.1, let us be reminded that the qualifier ‘-‘ means private and the qualifier ‘+’ means public.$NL$The purpose of these qualifiers will be revealed when we write the code for the class.$NL$The next section explains how the information in the class diagram shown in Figure 2.1 is translated into$NL$Java source code.$NL$Remember that, in general, a class definition declares attributes and defines constructors and behaviour.$NL$The Java developer concentrates on writing types called classes, as a result of interpreting class diagrams$NL$and other elements of the OOA & D of an application’s domain. The Java developer also makes extensive$NL$use of the thousands of classes provided by the originators of the Java language (Sun Microsystems Inc.)$NL$that are documented in the Java Applications Programming Interface (API).$NL$We have established that classes typically comprise attributes and the behaviour that is used to manipulate$NL$these data. Attributes are implemented, in Java, as variables, whose value determines the condition or$NL$state of an object of that class and behaviour elements are implemented using a construct known as a$NL$method. When a method is executed, it is said to be called or invoked.$NL$As has been mentioned earlier, an instance of a class is also called an object, such that, perhaps somewhat$NL$confusingly, the terms instance and object are interchangeable in Java. The requirement to create an$NL$instance of a class from the definition of the class gives rise to a fundamental question: how do we actually$NL$create an instance of a class so that its methods can be executed? We will address this question in$NL$this section.$NL$One of the components of a class, which we haven’t explained fully so far in the discussion of the Member$NL$class, is its constructor. A constructor is used to create or construct an instance of that class. Object$NL$construction is required so that the Java run-time environment (JRE) can respond to a call to an object’s$NL$constructor to create an actual object and store it in memory. An instance does not exist in memory until$NL$its constructor is called only its class definition is loaded by the (JRE). We will meet the constructor for$NL$the Member class later.$NL$Broadly, then, we can think of the Java developer as writing Java classes, from which objects can be$NL$constructed (by calling their constructors). Classes are to objects as an architect’s plan is to a house, i.e.$NL$we can produce many houses from a single plan and we can construct or instantiate many instances from a$NL$single template known as a class. Given that objects can communicate with other objects, this gives the$NL$developer the means to re-use classes from one application in another application. Therefore, with Java$NL$object technology, we can build software applications by combining re-useable and interchangeable$NL$objects, some of which can be standardised in terms of their interface. This is probably the single-most$NL$important advantage of object-oriented programming (OOP) compared with non-OOP in$NL$application development.$NL$We are now at the stage when we can translate the class diagram for the Member class into Java source$NL$code, often shortened to ‘code’. The code that follows is the class definition of the class named Member$NL$but includes only some of the attributes and methods that do not involve object types: this is to keep the$NL$example straightforward. The reason for this restriction is that if we were to declare attributes or$NL$parameters of the MembershipCard class type in the class Member, as required by the class diagram, the$NL$Java compiler would look for the class definition of the class MembershipCard. In order to keep the$NL$example straightforward, we will only write the class definition for the class Member for the time being$NL$we will refer to the class definition of the class MembershipCard in a later chapter. Thus, in this section,$NL$we will work with a single class that includes only primitive data types there are no class types included$NL$in the simplified class diagram.$NL$In order to make the example code even more straightforward, the class diagram is further simplified as$NL$shown in the next diagram. The class diagram that we will translate into Java code declares two variables$NL$and their corresponding ‘setter’ (or mutator) and ‘getter’ (or accessor) methods, as follows.$NL$The reason for the simplification (of the full class diagram) is so that the class definition can be more$NL$easily understood, compared to its full definition. In short, we well keep our first Java programme as$NL$simple as possible.$NL$In the class definition that follows below, ‘ // ‘ is a single-line comment and ‘ /** … */ ‘ is a block$NL$comment and, as such, are ignored by the Java compiler. For the purposes of the example, Java statements$NL$are written in bold and comments in normal typeface.$NL$// Class definition for the class diagram shown in Figure 2.2. Note that the name of$NL$// the class starts, by convention, with a capital letter and that it is declared as public.$NL$// The first Java statement is the class declaration. Note that the words public and$NL$// class must begin with a lower case letter.$NL$public class Member { // The class declaration.$NL$// Declare instance variables first. Things to note:$NL$// String types in Java are objects and are declared as ‘String’, not ‘string’.$NL$// The qualifier 'private' is used for variables.$NL$// 'String' is a type and 'userName' and ‘password’ are variable names, also$NL$// known as identifiers. Thus, we write the following:
4;By now the learner will be familiar, to some extent, with method invocation from earlier chapters, when$NL$objects of the Member class in the themed application are used to give some examples of passing$NL$arguments to methods. Chapter Four goes into more detail about methods and gives a further explanation$NL$about how methods are defined and used. Examples from the themed application are used to illustrate the$NL$principal concepts associated with an object’s methods.$NL$Chapter Three examines an object’s variables, i.e. its state or what it knows what its values are. An$NL$object’s methods represent the behaviour of an object, or what is knows what it can do, and surround, or$NL$encapsulate, an object’s variables. This section answers the question about how we get computable values$NL$into methods.$NL$As we know from previous chapters, a method is invoked by selecting the object reference for the instance$NL$required. The general syntax of a method invocation can be summarised as follows.$NL$Referring, again, to the Member class of the themed application, we could instantiate a number of Member$NL$objects (in a main method) and call their methods as in the following code snippet.$NL$// Instantiate three members call the no-arguments constructor for the Member class.$NL$// Call one of the set methods of these objects.$NL$// Call one of the get methods of these objects in a print statement.$NL$The screen output from executing this fragment of main is:$NL$In short, we must ensure that we know which method we are calling on which object and in which order.$NL$In the code snippet above, it is evident that setUserName expects a String argument to be passed to it this$NL$is because its definition is written as:$NL$The single parameter is replaced by a computable value, i.e. an argument, when the method is invoked.$NL$􀁸The general syntax of a method’s declaration is modifier return_type$NL$method_name( parameter_list ) exception_list$NL$􀁸The method’s definition is its declaration, together with the body of the method’s$NL$implementation between braces, as follows:$NL$􀁸The method’s signature is its name and parameter list.$NL$It is in the body of a method where application logic is executed, using statements such as:$NL$􀁸invocations: calls to other methods$NL$􀁸assignments: changes to the values of fields or local variables$NL$􀁸selection: cause a branch$NL$􀁸repetition: cause a loop$NL$􀁸detect exceptions, i.e. error conditions.$NL$If the identifier of a parameter is the same as that of an instance variable, the former is said to hide the$NL$latter. The compiler is able to distinguish between the two identifiers by the use of the keyword ‘this’, as$NL$in the following method definition that we met in Chapter One:$NL$If, on the other hand, we wish to avoid hiding, we could write the method definition as follows:$NL$where the identifier of the parameter is deliberately chosen to be different from that of the instance$NL$variable. In this case, the keyword ‘this’ can be included but it is not necessary to do so.$NL$In both versions of the method setUserName, the value of the parameter’s argument has scope only within$NL$the body of the method. Thus, in general, arguments cease to exist when a method completes its execution.$NL$A final point to make concerning arguments is that a method cannot be passed as an argument to another$NL$method or a constructor. Instead, an object reference is passed to the method or constructor so that the$NL$object reference is made available to that method or constructor or to other members of the class that$NL$invoke that method. For example, consider the following code snippet from the graphical version of the$NL$themed application shown on the next page.$NL$The examples and discussion in this section are meant to raise a question in the mind of the learner: are$NL$arguments passed by value or by reference? This question is addressed in the next sub-section.$NL$All arguments to methods (and constructors) are, in Java, passed by value. This means that a copy of the$NL$argument is passed in to a method (or a constructor) call.$NL$The example that follows aims to illustrate what pass by value semantics means in practice: detailed code$NL$documentation is omitted for the sake of clarity.$NL$The method changeValue changes the value of the argument passed to it – a copy of x – but it does not$NL$change the original value of x, as shown by the output. Thus the integer values 1235 and 1234 are output$NL$according to the semantics of pass by value as they apply to arguments.$NL$When a parameter is an object reference, it is a copy of the object reference that is passed to the method.$NL$You can change which object the argument refers to inside the method, without affecting the original$NL$object reference that was passed. However if the body of the method calls methods of the original object –$NL$via the copy of its reference - that change the state of the object, the object’s state is changed for the$NL$duration of its scope in a programme.$NL$Thus, in the example above, the strings “Bonjour” and “Hello there!” are output according to the$NL$semantics of pass by value as they apply to object references.$NL$A common misconception about passing object references to methods or constructors is that Java uses$NL$pass by reference semantics. This is incorrect: pass by reference would mean that if used by Java, the$NL$original reference to the object would be passed to the method or constructor, rather than a copy of the$NL$reference, as is the case in Java. The Java language passes object references by value, in that a copy of the$NL$object reference is passed to the method or constructor.$NL$The statement in the box isn’t true when objects are passed amongst objects in a distributed application.$NL$However, such applications are beyond the scope of this guide. For the purposes of the present guide,$NL$the learner should use the examples above to understand the consequences of Java’s use of pass by$NL$value semantics.$NL$In previous chapters, we have encountered a number of references to a method’s return type. In the$NL$definition of a method, the return type is declared as part of the method’s declaration and its value is$NL$returned by the final statement of the method.
4;The Java language provides a number of constructs that enable the developer to control the sequence of$NL$execution of Java statements. Chapter Two provides examples of how these constructs are used to control$NL$the flow of execution through a block of code that is typically contained in the body of a method.$NL$Sequential flow of execution of statements is the execution of Java source code in a statement-bystatement$NL$sequence in the order in which they are written, with no conditions. Most of the examples of$NL$methods that are discussed in previous chapters exhibit sequential flow. In general terms, such a method is$NL$written as follows.$NL$A number of the main methods, presented in previous chapters, are structured in this sequential way in$NL$order to satisfy straightforward testing criteria.$NL$While sequential flow is useful, it is likely to be highly restrictive in terms of its logic. Executing$NL$statements conditionally gives the developer a mechanism to control the flow of execution in order to$NL$repeat the execution of one or more statements or change the normal, sequential flow of control.$NL$Constructs for conditional flow control in Java are very similar to those provided by other programming$NL$languages. Table 2.1 on the next page identifies the flow control constructs provided by the Java language.$NL$The sub-sections that follow show, by example, how these constructs are used.$NL$Using a decision-making construct allows the developer to execute a block of code only if a condition is$NL$true. The sub-sections that follow illustrate how decision-making constructs are used.$NL$The if … then construct is the most basic of the decision-making constructs provided by the Java language.$NL$If a condition is true, the block of code is executed: otherwise, control skips to the first statement after the$NL$if block. The following code snippet illustrates a simple use of the if … then construct.$NL$When the code snippet is run (in a main method), the output when age = 20 is:$NL$You can drink legally.$NL$The rest of the programme is next.$NL$and when age = 17, the output is:$NL$The rest of the programme is next.$NL$In some programming languages, the word ‘then’ is included in the then clause. As the code snippet$NL$above shows, this is not the case in Java.$NL$An example taken from the themed application shows an if … then construct in action in one of the$NL$methods of the Member class. The method adds a member to the array of members only if there is room in$NL$the array of (arbitrary) size 6.$NL$If there is no room in the array because noOfMembers is equal to or greater than 6, control skips to the$NL$print statement that outputs the message “No room for another member.”$NL$The if … else construct (sometimes known as the if … then … else construct) provides an alternative path$NL$of execution if the if condition evaluates to false. Figure 2.1 illustrates, diagrammatically, the logic of the$NL$if … else construct.$NL$Flow of control enters the if clause and the if condition is tested. The result of evaluating the if condition$NL$returns either true or false and one or other of the paths of execution are followed depending on this value.$NL$The else block is executed if the if condition is false.$NL$The next code snippet illustrates a simple use of the if … else construct by modifying the first code snippet$NL$in Section 2.4.1.$NL$When the code snippet is run (in a main method), the output when age = 20 is:$NL$You can drink legally.$NL$The rest of the programme is next.$NL$and when age = 17, the output is:$NL$You are too young to drink alcohol!$NL$The rest of the programme is next.$NL$Another example taken from the themed application shows an if … else construct in action in another of$NL$the methods of the Member class. The setCard method is used to associate a member of the Media Store$NL$with a virtual membership card. Each member may have up to two cards, so the method checks whether$NL$another card can be allocated to a member.$NL$The if … else construct in the method is used to return either true or false, depending upon the result of$NL$evaluating the if condition that determined whether or not the member has fewer than two cards.$NL$There is another form of the else part of the if .. else construct: else … if. This form of compound or$NL$cascading construct executes a code block depending on the evaluation of an if condition immediately$NL$after the initial if condition. The compound if … else construct is illustrated diagrammatically in Figure$NL$2.2 below.$NL$The figure shows that any number of else … if statements can follow the initial if statement.$NL$The example on the next page illustrates how the if .. else construct is used to identify the classification$NL$for degrees awarded by universities in the United Kingdom, based on the average mark achieved in the$NL$final year.$NL$Running the code with an average of 30 % produces the following output:$NL$Your result is: You are going to have to tell your mother about this!$NL$and with an average of 65 %, the output is as follows:$NL$Your result is: Upper Second$NL$When the value of average is equal to 65, this satisfies more than one of the else … if statements in the$NL$code above. However, the output confirms that the first time that a condition is met – when average >= 60$NL$– control passes out of the initial if statement without evaluating the remaining conditions. When a$NL$condition is met in the code above, the output shows that control skips to the first statement after the initial$NL$if statement, i.e. to the statement$NL$It is worthwhile alerting learners to the use of braces in compound else … if constructs. Care must be taken$NL$when coding compound else .. if constructs due to the number of pairs of brackets involved: a common$NL$error is to omit one or more of these brackets. In cases where there is only one statement in an if block, it$NL$is good practice to include braces – as shown in the example above – in anticipation of if blocks that$NL$include more than one statement.$NL$The final example in this sub-section shows a compound else … if construct in action in the Member class$NL$of the themed application. The method scans the array of (virtual) cards held by a member and outputs$NL$some information that is stored against each card. (for loops are discussed in a later section of this chapter.)
5;In order to understand the TR approach to implementing the relational model, it’s necessary to be very clear over three distinct levels of the system, which I’ll refer to as the three levels of abstraction (since each level is an abstraction of the one below, loosely speaking). The three levels, or layers, are: $NL$1. The relational (or user) level $NL$2. The file level $NL$3. The TR level$NL$■Level 1, which corresponds to the database as seen by the user, is the relational level. At this level, the data is perceived as relations, including, perhaps, the suppliers relation S discussed in Section 2.1 (and illustrated in Fig. 2.1) in the previous chapter. $NL$■■Level 3 is the fundamental TR implementation level. At this level, data is represented by means of a variety of internal structures called tables. Please note immediately that those TR tables are NOT tables in the SQL sense and do NOT correspond directly to relations at the user level.$NL$■Level 2 is a level of indirection between the other two. Relations at the user or relational level are mapped to files at this level, and those files are then mapped to tables at the TR level. Of course, the mappings go both ways that is, tables at the TR level map to files at the next level up, and those files then map to relations at the top level. Note: As I’m sure you know, map is a synonym for transform (and I’ll be using the term in that sense throughout this book) thus, we’re already beginning to touch on the TR transforms that were mentioned in Chapter 1. However, there’s a great deal more to it, as we’ll soon see. $NL$Please now observe that each level has its own terminology: relational terms at the user level, file terms at the file level, and table terms at the TR level. Using different terms should, I hope, help you keep the three levels distinct and separate in your mind for that reason, I plan to use the three sets of terms consistently and systematically throughout the rest of this book. $NL$Having said that, I now need to say too that I’m well aware that some readers might object to my choice of terms—perhaps even find them confusing—for at least the following two reasons: $NL$■■First, the industry typically uses the terminology of tables, not relations, at the user level—almost exclusively so, in fact. But I’ve already explained some of my rationale for wanting to use relational terms at that level (see the previous chapter, Section 2.1), and I’m going to give some additional reasons in the next section.$NL$■Second, the industry also typically tends to think of files as a fairly “physical” construct. In fact, I did the same thing myself in the previous chapter, somewhat, though I was careful in that chapter always to be quite clear that the files I was talking about were indeed physically stored files specifically. By contrast, the files I’ll be talking about in the rest of the book are not physically stored instead, they’re an abstraction of what’s physically stored, and hence a “logical” construct, not a physical one. (Though it wouldn’t be wrong to think of them as “slightly more physical” than the user-level relations, if you like.) $NL$If you still think my terms are confusing, then I’m sorry, but for better or worse they’re the terms I’m going to use. $NL$One final point: When I talk of three levels, or layers, of abstraction, I don’t mean that each of those levels is physically materialized in any concrete sense—of course not. The relational level is only a way of looking at the file level, a way in which certain details are ignored (that’s what “level of abstraction” means). Likewise, the file level in turn is only a way of looking at the TR level. Come to that, the TR level in turn is only a way of looking at the bits and bytes that are physically stored that is, the TR level is itself—as already noted in Chapter 1, Section 1.2—still somewhat abstract. In a sense, the bits-and-bytes level is the only level that’s physically materialized.1$NL$Since the focus of this book is on the use of TR technology to implement the relational model specifically, the topmost (user) level is relational by definition. In other words, the user sees the database as a set of relations, made up of attributes and tuples as explained in Chapter 2. For simplicity, I’m going to assume those relations are all base relations specifically (again, see Chapter 2) that is, I’ll simply assume, barring explicit statements to the contrary, that any relation that’s named and is included in the database is in fact a base relation specifically, and I won’t usually bother to use the “base” qualifier. $NL$Also, of course, the user at the relational level has available a set of relational operators—restrict, project, join, and so forth—for querying the relations in the database, as well as the usual INSERT, DELETE, and UPDATE operators for updating them. Note: If I wanted to be more precise here, I’d have to get into the important distinction between relation values and relation variables. Relational operators like join operate on relation values, while update operators like INSERT operate on relation variables. Informally, however, it’s usual to call them all just relations, and—somewhat against my better judgment—I’ve decided to follow that common usage (for the most part) in the present book. For further discussion of such matters, see either reference [32] or reference [40]. $NL$Now, given the current state of the IT industry, the user level in a real database system will almost certainly be based on SQL, not on the relational model. As a consequence, users will typically tend to think, not in terms of relational concepts as such, but rather in terms of SQL analogs of those concepts. For example, there isn’t any explicit project operator, as such, in SQL instead, such an operation has to be formulated in terms of SQL’s SELECT and FROM operators, and the user has to think in terms of those SQL operators, as in this example (“Project suppliers over supplier number and city name”):
5;This chapter continues our examination of the core constructs of the TR model (principally the Field Values and Record Reconstruction Tables). However, the chapter is rather more of a potpourri than the previous one. Its structure is as follows. Following this short introductory section, Section 5.2 offers some general observations regarding performance. Section 5.3 then briefly surveys the TR operators, and Sections 5.4 and 5.5 take another look at how the Record Reconstruction Table is built and how record reconstruction is done. Sections 5.6 and 5.7 describe some alternative perspectives on certain of the TR constructs introduced in Chapter 4. Finally, Section 5.6 takes a look at some alternative ways of implementing some of the TR structures and algorithms also first described in that previous chapter.$NL$It seems to me undeniable that the mechanisms described in the previous chapter for representing and reconstructing records and files are vastly different from those found in conventional DBMSs, and I presume you agree with this assessment. At the same time, however, they certainly look pretty complicated ... How does all of that complexity square with the claims I made in Chapter 1 regarding good performance? Let me remind you of some of the things I said there:$NL$Well, let me say a little more now regarding query performance specifically (I haven’t really discussed updates yet, so I’ll have to come back to the question of update performance later—actually in the next chapter). Now, any given query involves two logically distinct processes: $NL$a) Finding the data that’s required, and then $NL$b) Retrieving that data. $NL$TR is designed to exploit this fact. Precisely because it separates field value information and linkage information, it can treat these two processes more or less independently. To find the data, it uses the Field Values Table to retrieve it, it uses the Record Reconstruction Table. (These characterizations aren’t 100 percent accurate, but they’re good to a first approximation—good enough for present purposes, at any rate.) And the Field Values Table in particular is designed to make the finding of data very efficient (for example, via binary search), as we saw in Chapter 4. Of course, it’s true that subsequent retrieval of that data then involves the record reconstruction process, and this latter process in turn involves a lot of pointer chasing, but:$NL$■■Even in a disk-based implementation, the system will do its best to ensure that pertinent portions of both the Field Values Table and the Record Reconstruction Table are kept in main memory at run time, as we’ll see in Part III. Assuming this goal is met, the reconstruction will be done at main-memory speeds. $NL$■■The “frills” to be discussed in Chapters 7 9 (as well as others that are beyond the scope of this book) have the effect, among other things, of dramatically improving the performance of various aspects of the reconstruction process. $NL$■■Most important of all: Almost always, finding the data that’s wanted is a much bigger issue than returning that data to the user is. In a sense, the design of the TR internal structures is biased in favor of the first of these issues at the expense of the second. Observe the implication: The more complex the query, the better TR will perform—in comparison with traditional approaches, that is. (Of course, I don’t mean to suggest by these remarks that record reconstruction is slow or inefficient—it isn’t—nor that TR performs well on complex queries but not on simple ones. I just want to stress the relative importance of finding the data in the first place, that’s all.)$NL$I’d like to say more on this question of query performance. In 1969, in his very first paper on the relational model [5], Codd had this to say:$NL$Once aware that a certain relation exists, the user will expect to be able to exploit that relation using any combination of its attributes as “knowns” and the remaining attributes as “unknowns,” because the information (like Everest) is there. This is a system feature (missing from many current information systems) which we shall call (logically) symmetric exploitation of relations. Naturally, symmetry in performance is not to be expected. $NL$—E. F. Codd$NL$Note: I’ve reworded Codd’s remarks just slightly here. In particular, the final sentence (the caveat concerning performance) didn’t appear in the original 1969 paper [5] but was added in the expanded 1970 version [6]. $NL$Anyway, the point I want to make is that the TR approach gives us symmetry in performance, too—or, at least, it comes much closer to doing so than previous approaches ever did. This is because, as we saw in Chapter 4, the separation of field values from linkage information effectively allows the data to be physically stored in several different sort orders simultaneously. When Codd said “symmetry in performance is not to be expected,” he was tacitly assuming a direct-image style of implementation, one involving auxiliary structures like those described in Chapter 2. However, as I said in that chapter: $NL$[Auxiliary structures such as pointer chains and] indexes can be used to impose different orderings on a given file and thus (in a sense) “level the playing field” with respect to different processing sequences all of those sequences are equally good from a logical point of view. But they certainly aren’t equally good from a performance point of view. For example, even if there’s a city index, processing suppliers in city name sequence will involve (in effect) random accesses to storage, precisely because the supplier records aren’t physically stored in city name sequence but are scattered all over the disk. $NL$—from Chapter 2$NL$As we’ve seen, however, these remarks simply don’t apply to the TR data representation. $NL$And now I can address another issue that might possibly have been bothering you. We’ve seen that the TR model relies heavily on pointers. Now, the CODASYL “network model” [14,25] also relies heavily on pointers—as the “object model” [3,4,28,29] and “hierarchic model” [25,56] both do also, as a matter of fact—and I and many other writers have criticized it vigorously in the past on exactly that score (see, for example, references [10], [21], and [37]). So am I arguing out of both sides of my mouth here? How can TR pointers be good while CODASYL pointers are bad?
5;There’s an old joke, well known in database circles, to the effect that what users really want (and always have wanted, ever since database systems were first invented) is for somebody to implement the go faster! command. Well, I’m glad to be able to tell you that, as of now, somebody finally has ... This book is all about a radically new database implementation technology, a technology that lets us build database management systems (DBMSs) that are “blindingly fast”—certainly orders of magnitude faster than any previous system. As explained in the preface, that technology is known as The TransRelationaltm Model, or the TR model for short (the terms TR technology and, frequently, just TR are also used). As also explained in the preface, the technology is the subject of a United States patent (U.S. Patent No. 6,009,432, dated December 28th, 1999), listed as reference [63] in Appendix B at the back of this book however, that reference is usually known more specifically as the Initial Patent, because several follow-on patent applications have been applied for at the time of writing. This book covers material from the Initial Patent and from certain of those follow-on patents as well.$NL$The TR model really is a breakthrough. To say it again, it allows us to build DBMSs that are orders of magnitude faster than any previous system. And when I say “any previous system,” I don’t just mean previous relational systems. It’s an unfortunate fact that many people still believe that the fastest relational system will never perform as well as the fastest nonrelational system. Indeed, it’s exactly that belief that accounts in large part for the continued existence and use of older, nonrelational systems such as IMS [25,57] and IDMS [14,25], despite the fact that—as is well known—relational systems are far superior from the point of view of usability, productivity, and the like. However, a relational system implemented using TR technology should dramatically outperform even the fastest of those older nonrelational systems, finally giving the lie to those old performance arguments and making them obsolete (not before time, either).$NL$I must also make it clear that I don’t just mean that queries should be faster under TR (despite the traditional emphasis in relational systems on queries in particular)—updates should be faster as well. Nor do I mean that TR is suitable only for decision support systems—it’s eminently suitable for transaction processing systems, too (though it’s probably fair to say that TR is particularly suitable for systems in which read-only operations predominate, such as data warehouse and data mining systems).$NL$And one last preliminary remark: You’re probably thinking that the performance advantages I’m claiming must surely come at a cost: perhaps poor usability, or less functionality, or something (there’s no free lunch, right?). Well, I’m pleased to be able to tell you that such is not the case. The fact is, TR actually provides numerous additional benefits, over and above the performance benefit—for example, in the areas of database and system administration. Thus, I certainly don’t want you to think that performance is the only argument in favor of TR. We’ll take a look at some of those additional benefits in Chapters 2 and 15, and elsewhere in passing. (In fact, a detailed summary of all of the TR benefits appears in Chapter 15, in Section 15.4. You might like to take a quick look at that section right now, just to get an idea of how much of a breakthrough the TR model truly is.)$NL$As I said in the preface, I believe TR technology is one of the most significant advances—quite possibly the most significant advance—in the data management field since E. F. Codd first invented the relational model (which is to say, since the late 1960s and early 1970s see references [5 7], also reference [35]). As I also said in the preface, TR represents among other things a highly effective way to implement the relational model, as I hope to show in this book. In fact, the TR model—or, rather, the more general technology of which the TR model is just one specific but important manifestation—represents an effective way to implement data management systems of many different kinds, including but not limited to the following:$NL$■■SQL DBMSs 	■■Data warehouse systems$NL$■■Information access tools	■■Data mining tools$NL$■■Object/relational DBMSs	■■Web search engines$NL$■■Main-memory DBMSs	■■Temporal DBMSs$NL$■■Business rule systems	■■Repository managers$NL$■■XML document storage and retrieval systems	■■Enterprise resource planning tools$NL$as well as relational DBMSs in particular. Informally, we could say we’re talking about a backend technology that’s suitable for use with many different frontends. In planning this book, however, I quickly decided that my principal focus should be on the application of the technology to implementing the relational model specifically. Here are some of my reasons for that decision:$NL$■Concentrating on one particular application should make the discussions and examples more concrete and therefore, I hope, easier to follow and understand.$NL$■■More significantly, the relational model is of fundamental importance it’s rock solid, and it will endure. After all, it really is the best contender, so far as we know, for the role of “proper theoretical foundation” for the entire data management field. One hundred years from now, I fully expect database systems still to be firmly based on Codd’s relational model—even if they’re advertised as “object/relational,” or “temporal,” or “spatial,” or whatever. See Chapter 15 for further discussion of such matters. $NL$■■If your work involves data management in any of its aspects, then you should already have at least a nodding acquaintance with the basic ideas of the relational model. Though I feel bound to add that if that “nodding acquaintance” is based on a familiarity with SQL specifically, then you might not know as much as you should about the model as such, and you might know “some things that ain’t so.” I’ll come back to this point in a few moments. $NL$■■The relational model is an especially good fit with TR ideas I mean, it’s a very obvious candidate for implementation using those ideas. Why? Because the relational model is at a uniform, and high, level of abstraction it’s concerned purely with what a database system is supposed to look like to the user, and has absolutely nothing to say about what the system might look like internally. As many people would put it, the relational model is logical, not physical.
5;By now I hope it’s clear that, even without the refinements to be discussed in later chapters, the TR model is certainly good for retrieval. (At least in principle! I’ll describe in more detail how retrievals are actually implemented in Chapter 10.) But what about updates?1 Conventional wisdom has always been that a given data structure can be good for either retrieval or update, but not both. In a direct-image implementation, for example, indexes are generally held to be good for retrieval but bad for update. So what about TR? How are updates done in TR? This chapter examines this question. $NL$To repeat from Chapter 5, then, the operators we need to consider are as follows (see Section 5.3): $NL$■■INSERT: Insert a new record. $NL$■■DELETE: Delete the record “passing through” cell [i,j] of the Record Reconstruction Table. $NL$■■UPDATE: Update the record “passing through” cell [i,j] of the Record Reconstruction Table. $NL$Note: The notion of a record “passing through” some cell of the Record Reconstruction Table was also explained in Section 5.3.$NL$Section 6.2 immediately following discusses the three update operators in general terms Sections 6.3 then presents a detailed example, and Section 6.4 discusses the swap algorithm. Section 6.5 briefly describes an alternative implementation technique that makes use of an overflow structure. Finally, Section 6.6 offers some observations regarding the performance aspects of TR update operations.$NL$It’s convenient to begin by discussing the INSERT operator specifically. Consider the suppliers file shown in Fig. 6.1 (it’s the same as the one shown in Fig. 4.1 in Chapter 4, except that the last record, the one for supplier S3, has been omitted). Figs. 6.2 and 6.3 show the corresponding Field Values Table and a corresponding Record Reconstruction Table, respectively. Exercise 6: Check that these tables are correct.$NL$Now suppose the user asks the system to insert the following tuple into the suppliers relation:$NL$In terms of the file of Fig. 6.1, of course, we can imagine a new record corresponding to this tuple simply being appended at the end, in position 5 (since record ordering within files is arbitrary). If we now rebuild the Field Values Table, it’ll appear as shown on the left-hand side of Fig. 6.4 (a copy of the Field Values Table from Fig. 4.3 in Chapter 4). And if we then build a corresponding Record Reconstruction Table, it might appear as shown on the right-hand side of Fig. 6.4$NL$As you can see by comparing Fig. 6.4 with Figs. 6.2 and 6.3, respectively, inserting supplier S3 has caused both the Field Values Table and the Record Reconstruction Table to change dramatically. It follows that INSERT operations have the potential to be quite disruptive, and hence (possibly) to display very poor performance. What can be done about this problem? $NL$Well, let me say right away that the effect on the Field Values Table is actually not as dramatic as it might appear. Although I’ve been calling it a table and showing it as a table in figures like Fig. 6.4, the Field Values Table doesn’t necessarily have to be physically stored as a table in fact, it almost certainly won’t be. Much more likely, it’ll be stored “column-wise” as a set of vectors (one-dimensional arrays), or possibly as a set of chained lists, one such vector or list for each column. Indeed, such an implementation is virtually certain to be used in practice if the refinements to be discussed in Chapters 8 and 9 are adopted, as we’ll see.2 $NL$For definiteness, let’s assume a vector implementation. Of course, those vectors will be kept in the sort orders associated with the corresponding columns of the Field Values Table. As a consequence, the insert point in each such vector for the pertinent field value from the new record is easily determined—for example, by binary search—and the vectors themselves, and hence the overall Field Values Table, are thus easily maintained.$NL$$NL$The Record Reconstruction Table is another matter, however. Is there a way to avoid rebuilding the entire table every time a new record is inserted into the user file? The answer, of course, is yes. One possible approach is as follows (the details are a little complicated, but the fundamental idea is straightforward): When a record is deleted from the user file, we3 don’t physically remove the corresponding entries from the Field Values and Record Reconstruction Tables, we just flag those entries as “logically deleted.” Those flagged cells can then be regarded as free space in each of the two tables. Then, when we subsequently insert a new record, it might be possible to use such flagged cells for the record in question (removing the flags, of course), thereby avoiding the overhead of completely rebuilding the Record Reconstruction Table (and the overhead of completely rebuilding the Field Values Table also, as a matter of fact). Detailed examples illustrating this process are given in Sections 6.3 and 6.4 below. $NL$I should immediately add that the scheme just described in outline makes considerably more sense if the refinements to be discussed in Chapters 8 and 9 are adopted. If they are—and in practice it’s virtually certain they will be—then it becomes possible for distinct records at the file level to share entries in the Field Values Table. For example, the supplier records for suppliers S2 and S3 might share the entry in that table that contains the city name Paris. Thus, when a new record is inserted, it might well be the case that most if not all of the field values in that record already exist in the Field Values Table—perhaps logically deleted, perhaps not—and such values can simply be shared by that new record with previously existing records. In effect, the ability to share field values in this way means that INSERT operations work at the field level instead of the usual record level—yet another significant difference between the TR approach and conventional implementation technology. Of course, analogous remarks apply to DELETE and UPDATE operations also, as you’d surely expect.
5;The main purpose of this chapter is to explain in more detail some of the problems that arise in connection with what the lawyers call “prior art”—meaning, in the case at hand, systems that use the traditional direct-image approach to implementation. Of course, you can skip this material if you’re already familiar with conventional implementation technology. However, this first section does also introduce a few simple relational ideas, and you might at least want to make sure you’re familiar with those and fully understand them. $NL$Consider Fig. 2.1, which depicts a relation called S (“suppliers”). Observe that each supplier has a supplier number (S#), unique to that supplier1 a supplier name (SNAME), not necessarily unique (though in fact the sample names shown in the figure do happen to be unique) a rating or status value (STATUS) and a location (CITY). I’ll use this example to remind you of a few of the most fundamental relational terms and concepts.$NL$■First of all, a relation can, obviously enough, be pictured as a table. However, a relation is not a table.2 A picture of a thing isn’t the same as the thing! In fact, the difference between a thing and a picture of that thing is another of the great logical differences (see the remarks on this latter notion in Chapter 1, near the beginning of Section 1.3). One problem with thinking of a relation as a table is that it suggests that certain properties of tables—for example, the property that the rows are in a certain top-to-bottom order—apply to relations too, when in fact they don’t (see below). $NL$■■Each of the five suppliers is represented by a tuple (pronounced as noted in Chapter 1 to rhyme with “couple”). Tuples are depicted as rows in figures like Fig. 2.1, but tuples aren’t rows. $NL$■■Each supplier tuple contains four values, called attribute values that is, the suppliers relation involves four attributes, called S#, SNAME, STATUS, and CITY. Attributes are depicted as columns in figures like Fig. 2.1, but attributes aren’t columns.$NL$■■Attributes are defined over data types (types for short, also known as domains), meaning that every value of the attribute in question is required to be a value of the type in question. Types can be either system-defined (built in) or user-defined. For example, attribute STATUS might be defined over the system-defined type INTEGER (STATUS values are integers), while attribute SNAME might be defined over the user-defined type NAME (SNAME values are names). Note: For definiteness, I’ll assume these specific types throughout what follows, where it makes any difference. I’ll also assume that attribute S# is defined over a user-defined type with the same name (that is, S#), and attribute CITY is defined over the system-defined type CHAR (meaning character strings of arbitrary length). $NL$■■The tuples of a relation are all distinct. In fact, relations never contain duplicate tuples—the tuples of a relation form a mathematical set, and sets in mathematics don’t contain duplicate elements. Note: People often complain about this aspect of the relational model, but in fact there are good practical reasons for not permitting duplicate tuples. A detailed discussion of the point is beyond the scope of this book see any of references [13], [20], or [33] if you want to pursue the matter. $NL$■■There’s no top-to-bottom ordering to the tuples of a relation. Although figures like Fig. 2.1 clearly suggest there is such an ordering, there really isn’t—to say it again, the tuples of a relation form a mathematical set, and sets in mathematics have no ordering to their elements. Note: It follows from this point that we could draw several different pictures that would all represent the same relation. An analogous remark applies to the point immediately following.$NL$■There’s no left-to-right ordering to the attributes of a relation. Again, figures like Fig. 2.1 clearly suggest there is such an ordering, but there really isn’t like the tuples, the attributes of a relation form a set, and thus have no ordering. (By the same token, there’s no left-to-right ordering to the components of a tuple, either.) No relation can have two or more attributes with the same name. $NL$■■The suppliers relation is in fact a base relation specifically. In general, we distinguish between base and derived relations a derived relation is one that is derived from, or defined in terms of, other relations, and a base relation is one that isn’t derived in this sense. Loosely speaking, in other words, the base relations are the “given” ones—they’re the ones that make up the actual database—while the derived ones are views, snapshots, query results, and the like [33]. For example, given the base relation of Fig. 2.1, the result of the query “Get suppliers in London” is a derived relation that looks like this:$NL$Another way to think about the distinction is that base relations exist in their own right, while derived ones don’t—they’re existence-dependent on the base relations. $NL$■■Every relation has at least one candidate key (or just key for short), which serves as a unique identifier for the tuples of that relation. In the case of the suppliers relation (and the derived relation just shown as well), there’s just one key, namely {S#}, but relations can have any number of keys, in general. Note: It’s important to understand that keys are always sets of attributes (though the set in question might well contain just a single attribute). For this reason, in this book I’ll always show key attributes enclosed in braces, as in the case at hand—braces being used by convention to bracket the elements that make up a set. $NL$■■As you probably know, it’s customary (though not obligatory) to choose, for any given relation, one of that relation’s candidate keys—possibly its sole candidate key—as primary thus, for example, we might say in the case of the suppliers relation that {S#} is not just a key but the “primary” key. In figures like Fig. 2.1, I’ll follow the convention of identifying primary key attributes by double underlining. $NL$■■Finally, relations can be operated on by a variety of relational operators. In general, a relational operator is an operator that takes zero or more relations as input and produces a relation as output. Examples include the well-known operators restrict, project, join, and so on.
5;Now (at last) I can begin to explain the TR model in detail. As I mentioned several times in Part I, TR is indeed still a model, and thus, like the relational model, still somewhat abstract. At the same time, however, it’s at a much lower level of abstraction than the relational model it can be thought of as being closer to the physical implementation level (“closer to the metal”), and accordingly more oriented toward issues of performance. In particular, it relies heavily on the use of pointers—a concept deliberately excluded from the relational model, of course, for reasons discussed in references [9], [30], [40], and many other places—and its operators are much more procedural in nature than those of the relational model. (What I mean by this latter remark is that code that makes use of those operators is much more procedural than relational code is, or is supposed to be.) What’s more, reference [63] includes detailed, albeit still somewhat abstract, algorithms for implementing those operators. Note: These remarks aren’t meant to be taken as criticisms, of course I’m just trying to capture the essence of the TR model by highlighting some of its key features. $NL$Despite its comparatively low-level nature, the fact remains that, to say it again, TR is indeed a model, and thus capable of many different physical realizations. In what follows, I’ll talk for much of the time in terms of just one possible realization—it’s easier on the reader to be concrete and definite—but I’ll also mention some alternative implementation schemes on occasion. Note that the alternatives in question have to do with the implementation of both data structures and corresponding access algorithms. In particular, bear in mind that both main-memory and secondary-storage implementations are possible. $NL$Now, this book is meant to be a tutorial accordingly, I want to focus on showing the TR model in action (as it were)—that is, showing how it works in terms of concrete examples—rather than on describing the abstract model as such. Also, many TR features are optional, in the sense that they might or might not be present in any given implementation or application of the model, and it’s certainly not worth getting into all of those optional features in a book of this kind. Nor for the most part is it worth getting into the optionality or otherwise of those features that are discussed—though I should perhaps at least point out that options do imply a need for decisions: Given some particular option X, some agency, at some time, has to decide whether or not X should be exercised. For obvious reasons, I don’t want to get into a lot of detail on this issue here, either. Suffice it to say that I don’t think many of those decisions, if any at all, should have to be made at database design time (by some human being) or at run time (by the system itself) in fact, I would expect most of them to be made during the process of designing the DBMS that is the specific TR implementation in question. In other words, I don’t think the fact that those decisions do have to be made implies that a TR implementation will therefore suffer from the same kinds of problems that arise in connection with direct-image systems, as discussed in Chapter 2. $NL$It follows from all of the above that this book is meant as an introduction only many topics are omitted and others are simplified, and I make no claims of completeness of any kind.$NL$Now let’s get down to business. In this chapter and the next,1 we’ll be looking at what are clearly the most basic TR constructs of all: namely, the Field Values Table and the Record Reconstruction Table, both of which were mentioned briefly in the final section of the previous chapter. These two constructs are absolutely fundamental—everything else builds on them, and I recommend as strongly as I can that you familiarize yourself with their names and basic purpose before you read much further. Just to remind you: $NL$■■The Field Values Table contains the field values from a given file, rearranged in a way to be explained in Section 4.3. $NL$■■The Record Reconstruction Table contains information that allows records of the given file to be reconstructed from the Field Values Table, in a way to be explained in Section 4.4. $NL$In subsequent chapters I’ll consider various possible refinements of those core concepts. Note: Those refinements might be regarded in some respects as “optional extras” or “frills,” but some of them are very important—so much so, that they’ll almost certainly be included in any concrete realization of the TR model, as we’ll see.$NL$Let r be some given record within some given file at the file level. Then the crucial insight underlying the TR model can be characterized as follows: $NL$The stored form of r involves two logically distinct pieces, a set of field values and a set of “linkage” information that ties those field values together, and there’s a wide range of possibilities for physically storing each piece. $NL$In direct-image systems, the two pieces (the field values and the linkage information) are kept together, of course in other words, the linkage information in such systems is represented by physical contiguity. In TR, by contrast, the two pieces are kept separate to be specific, the field values are kept in the Field Values Table, and the linkage information is kept in the Record Reconstruction Table. That separation makes TR strikingly different from virtually all previous approaches to implementing the relational model (see Chapters 1 and 2), and is the fundamental source of the numerous benefits that TR technology is capable of providing. In particular, it means that TR data representations are categorically not a direct image of what the user sees at the relational level. $NL$Note: One immediate advantage of the separation is that the Field Values Table and the Record Reconstruction Table can both be physically stored in a way that is highly efficient in terms of storage space and access time requirements. However, we’ll see many additional advantages as well, both in this chapter and in subsequent ones.
6;The Media Browser appears in many of Apple’s application software. It is a system wide utility that is accessed from within$NL$programs such as Keynote. It has a distinct icon displaying a frame of film, a picture frame and two musical notes. This$NL$icon appears in different places in different applications.$NL$The Media Browser can be launched from the Toolbar or View > Media Browser. It contains three tabs, Audio, Photos$NL$and Movies.$NL$Audio – This contains tracks from the current user’s iTune account. Garage band projects, these might include voice–overs$NL$or podcasts and audio from other applications such as Aperture or Final Cut.$NL$Photos – This contains images stored in iPhoto or Aperture is it is installed$NL$Movies – This contains video files stored in iMovie, the current user’s Movies Folder, or in iTunes. If Final Cut or Aperture$NL$are installed video files from those applications will also appear here.$NL$The bottom pane of the Media Browser contains thumbnails of the respective media files. Double–clicking the thumbnail$NL$previews the file. In the case of Photos it enlarges the thumbnail to fill the Media Browser pane.$NL$As with all iWork applications, Keynote stores graphics and other media within the file itself, with the exception of Fonts.$NL$This makes for easy transfer of files from workstation to laptop et cetera. A consequence of this is that iWork documents$NL$may have large files sizes though there is an easy remedy in the Reduce File Size command. This will be illustrated in$NL$Section 16 — Sharing Your Work.$NL$To insert a media file or files select them in the Media Browser and drag them onto the Keynote Slide Canvas.$NL$When images are placed into Keynote they may look too dark or lack contrast. Like all iWork applications there is an$NL$Adjust Image window that can be used to adjust the brightness, contrast, saturation and other image parameters directly$NL$within Keynote. There are several image parameters that can be used to finesse imported photographs though often just$NL$clicking the Enhance button will improve a picture.$NL$Image Adjust is an icon in the Toolbar, or can be launched from View > Adjust.$NL$When you have taken the time to format a picture frame or text box there is no need to repeat all the steps taken when$NL$you wish to apply that style to subsequent objects. Styles can be copied and pasted. Start this process by creating two text$NL$boxes. Use the Graphics Inspector to stylize one box. Ensure that Text Box is selected then go to Format > Copy Style.$NL$Select the second text box and go to Format > Paste Style. The second Text Box will take on the general appearance of$NL$the stylized one.$NL$There are several methods for preparing presentations. With presentations planning and rehearsal are essential. As$NL$previously stated this guide does not aim to explain the art of presentation. Information on presentation creation and$NL$delivery can be found on the web with several sites offering Keynote tips and techniques. Suffice to say planning is required$NL$before opening Keynote.$NL$Keynote can help structure planning processes, once you have a concept in mind. Outline mode is a particularly useful$NL$way to list bullet points as hierarchies and sequences when creating a presentation from scratch.$NL$There are five stages in the life of a presentation:$NL$1. Planning – What you need to do before opening Keynote and using Keynote in Outline mode.$NL$2. Construction – Using Outline, Navigator and Light Table modes to build and sequence the presentation.$NL$3. Rehearsal — For presentations that are to be delivered to an audience rehearsal is very important. Keynote has$NL$a rehearsal mode that can be used to check timings.$NL$4. Delivery — After rehearsals comes the main event often in front of an audience. Being able to stop and jump to$NL$different slides is a useful skill. Keynote presentations don’t have to be delivered in person. Using Kiosk mode$NL$allows users to navigate through presentation material on their own.$NL$5. Legacy – Making a presentation may be sufficient, though legacy items can be created, such as handouts or$NL$narrated versions of the presentation in video form that can be accessed on-demand, by whoever.$NL$Keynote ships with forty-four Apple designed themes. Themes are collections of Master Slides, for example, titles slides,$NL$bullets, bullets with photos, and so on. Themes have a coordinated design for fonts, colours and style across their respective$NL$Master Slides.$NL$An Apple Designed Theme may be a perfect starting point for a presentation, though users can generate and save their$NL$own themes, or purchase extra themes online. Defining Master Slides and creating custom themes will be explored later.$NL$Some Keynote Themes only have two resolutions, 800 x 600 pixels and 1024 x 768 pixels. Newer Themes come in three$NL$additional sizes 1280 x 720, 1920 x 1080†, and 1680 x 1050. Although presentations can be set to playback so that they$NL$fill the screen, choosing an option with sufficient resolution at an appropriate aspect ratio is very important.$NL$The Theme Chooser contains all the themes currently available including any custom ones you might make. To see$NL$previews of all the Master Slides in a Theme, skim the cursor over the Theme’s thumbnail. Before selecting a slide select$NL$the correct slide resolution from the Slide SIze Pop–up menu.$NL$Master Slides are preset layouts for title slides, bullet point slides, bullets with a picture and so on. Themes are a collection$NL$of Master Slides. The Master icon in the toolbar can be used to change one or more slides to a different master layout.$NL$The View icon in the toolbar can be used to Show Master Slides. Click dragging Master Slide thumbnails onto existing$NL$slides thumbnails changes their master layout.$NL$Although many users begin making their Keynote, or Powerpoint, presentations in slide view, Outline view is a great way$NL$to structure ideas. iWork’s Pages has several Outline templates to help with this task.$NL$Outline view is one of several options that can be found under the View menu. If the default Outline view is not suitable it$NL$can be adjusted. To make the Slide pane areas bigger move the cursor over the resize handle and click drag to adjust its size.
6;This guide explains the processes used to make Keynote documents. Keynote is Apple Inc.’s equivalent to Microsoft’s$NL$Powerpoint. Keynote’s strength is its ease of use and its ability to handle a variety of media types, including HD Video.$NL$This guide to Keynote is one of three books I have written for Bookboon on iWork. My books on Pages and Numbers$NL$complement this one, with some areas of repetition, each guide is designed to stand alone.$NL$A great way to learn is to experiment and play. Use this guide to focus your learning on specific areas of Keynote before$NL$taking a broad view of the myriad of possibilities for this software.$NL$This guide describes ways to assemble and edit content. It does not seek to give advice on presentation methodology, too$NL$often business presentations suffer from densely packed slides, with too much text and statistical information squeezed$NL$into them information that is best left for a report. Rather than cram the contents of a report onto a handful of slides,$NL$judicious planning will make for compelling presentations. Keynote is not a place to copy and paste all the text of a report.$NL$It is far better to highlight and illustrate important points using Keynote’s excellent graphic and animation capabilities. That$NL$said, animation and graphic elements should be used sparingly to aid the communication, and not as a distraction to it.$NL$Keynote can be used to create dynamic and engaging presentations. Text, images and charts can be arranged with ease.$NL$Keynote makes it easy to add audio and video, add transitions between slides, animate data, and then share presentations$NL$in a variety of ways.$NL$The guide describes software functions and outlines generic examples of the software in use. Further information can be$NL$found on Apple’s web pages, or via Apple’s Certified Training Scheme.$NL$Regarding keyboard shortcuts. The keyboard shortcuts mentioned in this book will work on International English$NL$QWERTY keyboards. For US keyboards the only difference is that Alt key (􀀂 ) is called Option (􀀂 ). For AZERTY and$NL$other language keywords please try the shortcuts, they will probably work.$NL$For seasoned Mac users please note that the Apple key is now referred to as the Command key. It is labelled cmd , not .$NL$There are three ways to launch Keynote.$NL$• Go to your Applications folder. In Finder choose Go > Applications. Open the iWork ’09 folder and double–$NL$click the Keynote icon. (Unless you have done a customized iWork installation the iWork folder will be in the$NL$Application folder found in the root of your primary hard drive.)$NL$• In the Dock, click the Keynote icon. (In Apple’s latest operating system named Lion, a Keynote icon will appear$NL$in Launchpad.)$NL$• Double–click any Keynote document.$NL$Every time you launch Keynote or try to create a new document, Keynote’s Template Chooser appears. The Template$NL$Chooser contains Apple designed templates and any Templates created by you. Apple’s Templates can be customized to$NL$suit your tastes or to comply with a business’s graphic identity. There will be more on Themes later. The White and Black$NL$Themes are good options to experiment with.$NL$Keynote has features that are shared with, or are similar to, features found in other Apple applications such as Numbers and$NL$Pages. This section lists these features. Understanding features including Inspectors and the Media Browser are essential$NL$when learning about any Apple software.$NL$To explore the features described in Section 1. launch Keynote and open any Template. When launching Keynote the$NL$following message may appear.$NL$By default Keynote’s window contains a customizable Tool Bar, a Format Bar, a Slides Pane, and the Slide Canvas. Other$NL$panes may be opened, including Master Slides Pane, and Presenter Notes Pane.$NL$The Tool Bar contains several icons. These control common functions and will be described later. Note that some Tool$NL$Bar Icons are greyed-out meaning they cannot be used. They become active once an Object is selected. Also note that the$NL$Tool Bar can be customized to display buttons for commands based on user preference. From left to right:$NL$• New – This adds a new slide. The keyboard shortcut is Command – Shift – N.$NL$• Play – This starts the presentation in Slideshow, at the selected slide, the keyboard shortcut is Command–Alt–P.$NL$To start a Slideshow from the first slide hold down the Alt key and click the Play button in the Tool Bar.$NL$• View – changes the View Mode. Options include Navigator, Outline, Slide Only and Light Table. Of these$NL$Navigator and Light Table are the most useful when constructing documents. Outline Mode will be discussed$NL$further in Section 5.4. View also controls the display of Rulers, Format Bat, Presenter Notes and Master Slides.$NL$When developing a presentation to support a speech or lecture using Presenter Notes can prove a great aid to memory.$NL$Keynote can be configured to display on two displays simultaneously, one showing slides to an audience and a comprehensive$NL$display of Presenter Notes, Next Slide and Time Elapsed or Remaining on the other.$NL$Comments can be temporarily hidden from the View Menu. Comments are like virtual sticky notes and can be found in$NL$all the iWork applications.$NL$• Guides – By default Keynote displays alignment guides to help layout. These appear as yellow lines that indicate$NL$whether an object is aligned to the top, bottom, centre or side of another object, or where it is in relation to$NL$the canvas. Choosing all four options from Show Guides at Object Center to Show Relative Sizes will give$NL$maximum layout feedback help.$NL$• Themes – On launching Keynote a Theme Chooser appears. Users can create their own Themes. Having a$NL$Theme button in the Tool Bar allows Themes to be swapped in an open document. This function is useful for$NL$companies wishing to update presentations to meet their latest visual identity guidelines. A presentation using$NL$their properly applied 2008 theme can be updated to their 2012 theme in a single click.$NL$• Masters – Themes contain several Master Slides Title & Subtitle, Title & Bullets, Title & Bullets – 2 Column…$NL$et cetera. Use this menu to change slides to different Masters.$NL$• Text Box – This adds a simple text box to the slide canvas. It is better to use a slide Master with a Text Box and$NL$modify it. This makes the Theme change function work speedily. It isn’t wrong, nor does it expose weaknesses$NL$in the Keynote software, to add Text Boxes. Additional Text Boxes cannot be automatically changed when$NL$changing Themes.
6;When the View is set to Navigator, the pane to the left of the Slide Canvas is labelled Slides. Change the View to Outline$NL$and the pane is labelled Outline. The other view modes include Slide Only and Light Table. The Navigator View is probably$NL$the best option for creating and editing presentations, as thumbnails of the slide appear in the Slides Pane. This allows$NL$thumbnails to be click–dragged into new positions and slides to be duplicated and deleted with ease.$NL$Click-dragging a slide in Navigator View will reposition it in the running order. Holding down the Alt key whilst doing$NL$this will copy the slide to a new location. As with nearly all keyboard short–cuts the modifier key, in this case the Alt$NL$Key, should only be released after the mouse button is released. Other operations can be accessed using right–click on$NL$the Slides Pane.$NL$Other operations include the ability to skip slides. This is useful when shorter versions of a presentation are required$NL$perhaps a thirty minute slot has been cut to twenty. The Skip function is a safe way to trim a presentation without deleting$NL$any data. Skipped slides won’t display as part of a Slide Show, but can be easily un–skipped too the context menu command$NL$for this is Don’t Skip Slide.$NL$In teaching scenarios a Keynote can be prepared with extra sections that might be useful in class, but can be set to Skip$NL$by default, and only revealed if the teacher thinks it prudent to explore such areas. Skipping slides avoids the need to have$NL$several similar Keynote documents on the same subject. So rather than having to update several Keynotes as data changes,$NL$just the one document needs updating and the Skip function helps tailor the presentation for a variety of audiences and$NL$durations.$NL$Only revealed when the View is set to Show Master Slides, the Master Slide Pane appears above the Slides or Outline$NL$Pane to the left of the Slide Canvas. From here the current slide’s format can be changed from one Master to another.$NL$Simply click the slide or slides to be changed and click on the desired Master. This operation can also be achieved using$NL$the Master button in the Tool Bar.$NL$The Presenter Notes Pane appears under the Slide Canvas when set from the View menu in the Tool Bar.$NL$Presenter notes can be copied and pasted from other applications such as Pages or typed directly into the Presenter Notes$NL$Pane. They can be a great aid to memory when making presentations. Presenter Notes can also be included in handouts$NL$printed from Keynote.$NL$The Inspector is a key feature in Apple’s iLife and iWork software. Users new to Apple Mac software need to embrace the$NL$use of the Inspector. Inspectors control nearly all the parameters in iWork software. Numbers, Pages and Keynote have$NL$some unique Inspectors and some in common. For instance all the applications have a Document Inspector containing$NL$slightly different parameters in each.$NL$The Inspector is launched by either clicking the Inspector icon in the Toolbar or View > Show Inspector.$NL$Keynote has ten inspector tabs. They are:$NL$• Document Inspector –$NL$The Document Inspector should be used when first setting up a document. Divided into three tabs. The first tab contains$NL$slideshow preferences with options for Looping the slideshow. From here the Presentation mode can be changed, along$NL$with the Slide Size.$NL$The bottom section is Require Password To Open. For additional security a password can be entered to lock a Keynote$NL$document. The password would then be required every-time the document is opened.$NL$The next tab is Audio. There are several ways to add audio to a presentation, and using this tab this may not be the most$NL$flexible option.$NL$The last tab is Spotlight. Completing the fields for Author, Title, Keywords, and Comments is recommended practice. It$NL$makes documents easier to search for, especially when using the Mac Spotlight search engine. All of these fields are for$NL$metadata. Author and Title are self-explanatory. Adding Keywords helps classify a document. For example a Keynote$NL$document designed for a seminar on Apple’s Aperture could have the keywords, ‘Apple’, ‘Aperture’, ‘photography’, ‘digital$NL$imaging’ and ’45 minutes’. The list could go on. There are five keywords here. Keywords are denoted by commas, ‘Apple,$NL$Aperture, photography’ ‘digital imaging’ and ‘45 minutes’ are all single keywords.$NL$Searches can be case-sensitive, but using ‘apple’ and ‘Apple’ as keywords is not strictly necessary. When choosing keywords$NL$it is helpful to invoke the spirit of the librarian. A less rigorous form of applying metadata is using the Comments field. Here$NL$paragraphs of descriptive text can add be added. In the example of the Apple Aperture Keynote document, the comments$NL$field could read, ‘Seminar Presentation for Apple’s Aperture, aimed at the serious amateur and professional audience…’$NL$• Slide Inspector –$NL$The Slide Inspector has two tabs, Transition and Appearance. Transitions can be applied to one or more slides at a time.$NL$There are currently forty-eight transition effects and something known as Magic Move, located in the Effect menu of the$NL$Transition tab. Experiment with the transitions to learn more, try changing the transition duration as this may enhance the$NL$effect. Some transitions have a direction, moving from left to right, or top to bottom. When this is the case the Direction$NL$menu becomes active. The Start Transition menu is set to On Click by default, meaning that to move the the next slide a$NL$mouse click or an alternative to click has to be made†. Transitions can be triggered in other ways, as listed in the Start$NL$Transition menu.$NL$† Alternatives to a mouse click, include clicking a trackpad, tapping the Spacebar, using a keyboard’s forward arrow key,$NL$using a remote control device
6;Spreadsheet tables can be created, complete with calculations, directly in Keynote. Clicking the Table icon in the Toolbar$NL$places a basic three column by three row table onto a slide. The Table Inspector can be used to modify that Table adding$NL$extra rows and columns as needed. The Table Inspector be used to format the Table. However in most circumstances$NL$spreadsheet data is best copied and pasted into slides from iWork’s Numbers.$NL$As Numbers is a dedicated spreadsheet package it offers far more data editing and function control than Keynote. Having$NL$Table creation control in Keynote is great though the primary reason for having this capability is to allow table and chart$NL$data to be modified within Keynote. For instance, if sales figures change immediately before a business presentation, the$NL$table and chart data can be changed in Keynote without the need to locate the original spreadsheet data.$NL$Clicking on any Table Cell that contains a formula invokes the Formula Editor. Once active the Formula Editor can be$NL$used to modify equations. If Numbers is not available, create a simple table, select a cell and press the equals (=) key.$NL$In common with the other iWork applications Keynote runs automatic spell checking. To switch this off go to Edit >$NL$Spelling > Check Spelling As You Type and mark sure it is un–ticked. When on potentially misspelt words are automatically$NL$underlined with a red dotted line. Using the context menu spelling suggestions are offered. The context menu also allows$NL$words to be learnt. To invoke the Context Menu use Right Click or Control Click. This context menu also launches the$NL$Mac’s dictionary and thesaurus, and even links to Wikipedia, or the Google search engine.$NL$To run spell check across a document go to Edit > Spelling >Spelling… The spell check dialogue box appears.$NL$Automatic Corrections can be controlled from Keynotes Preferences. For example typing ‘teh’ will automatically change$NL$to ‘the’. Also, scientific terms or business names can be abbreviated. Here typing ‘mwp’ will automatically change to$NL$Mark Wood | Photography. Genuine fractions can be achieved but only with certain font types. Once enabled Automatic$NL$Corrections, Symbol and Text Substitution will replace 1/3 with ⅓, but only if the chosen font contains glyphs for fractions.$NL$If the chosen font does not contain the required glyph an alternative font is used.$NL$Photographs, Movies and Sound can be added to a Keynote document in one of four different ways.$NL$1. The Media Browser$NL$2. The Insert menu and Choose…$NL$3. Dragging files from Finder$NL$4. Copying and Pasting from other applications$NL$The hassle free method is to use Media Browser, as it connects to other Apple software including iTunes. The available$NL$media will be in a Keynote compatible format. To include files from the Media Browser locate the correct tab for the$NL$media type, either Photos, Movies, Audio, then find the desired file and drag it on to a Keynote slide.$NL$Using the Insert command from the application menu bypasses the need to have files included in the Media Browser, but$NL$be aware that not all picture, movie and audio formats are supported by iWork and Keynote. If a file appears greyed out$NL$in the Finder Import window it means that file format is not supported.$NL$Supported file formats are:$NL$For Pictures – all QuickTime-supported formats, including TIFF, GIF, JPEG, PDF, PSD, EPS, PICT$NL$For Movies and Audio - any QuickTime or iTunes file type, including MOV, MP3, MPEG-4, AIFF and AAC$NL$Supported files can be simply dragged from a Finder window onto a Slide. If the contents of an entire folder are required,$NL$drag the folder from its Finder window onto the Media Browser. For a folder of mixed media, being Pictures, Movies and$NL$Audio this operation will have to be executed three times. Once for each tab of the Media Browser.$NL$$NL$If images placed onto slides look too dark or lack contrast Keynote has an Adjust Image control. This is a window that$NL$can be used to adjust the brightness, contrast, saturation and other image parameters directly within Keynote. There$NL$are several image parameters that can be used to finesse imported photographs, though often just clicking the Enhance$NL$button will improve a picture.$NL$Adjust Image is an icon in the Toolbar, or it can be launched from View > Adjust.$NL$By default Keynote’s preferences are set to include audio and movies in the document file. This option, found in the General$NL$tab of Keynote’s Preferences, should be left ticked its default. This ensures media is saved within Keynote documents.$NL$There is no mechanism for excluding picture data from automatically being saved in Keynote.$NL$Because Keynote embeds media files, presentations can become rather bloated, filling hard drive space. On modern$NL$workstations this is not a big problem but for the fast transmission of a Keynote via a network, file size can jeopardize$NL$smooth playback.$NL$All iWork applications have a Reduce File Size command found in the File menu. Use this to resample media files contained$NL$in Keynote. Caution: do not overwrite, that is save over, the original Keynote file. Using Reduce File Size may reduce the$NL$fidelity of media during resampling. This may not be an issue, but keeping the higher quality Keynote is a sensible precaution.$NL$Global Transitions can be added to slides. They help to indicate a change is taking place. The Dissolve Effect set to two$NL$or three seconds makes a pleasing transition from one picture slide to another. With so many transitions it is all too easy$NL$to overload a presentation with effects, and so obscure the intended message.$NL$Often transitions are triggered by a click, though they can also be set to change after a time delay. Mixing click-triggered$NL$transitions with time-delayed transitions can be confusing to a presenter. It is probably best to use click-triggering and$NL$no delays because often when a computer appears unresponsive users start clicking freely and frustratedly.
6;There are six table presets each providing an insight into the variety of Cell Formats. Controls for Cells can be explored$NL$by right–clicking or Control–Clicking a Cell or Cells.$NL$The six table types are:$NL$• Headers – This includes one header column and one header row. Headers are used to label columns and rows.$NL$Headers are automatically formatted so that they stand out from other cells in a Table. They are always the$NL$topmost row or the first column on the left of a Table. Numbers supports up to five header rows and columns.$NL$Multiple headers are useful when assigning names to two or more header columns or rows.$NL$• Basic – is similar to the Headers preset except that it does not contain a row header.$NL$• Sums – like Basic, this has columns headers, no row headers, but includes a Footer Row. The Footer Row cells$NL$contain a SUM formula which totals any number values entered into the columns.$NL$• Plain – is a simple grid of Cells with no headers or formula added.$NL$• Checklist – is a useful example of Cell Formatting. Column A contains Checkboxes. So if making, for instance,$NL$a Do List once a task has been completed it can be simply ticked to indicate its completion. Other types of Cell$NL$Format are Stepper, Slider and Pop-up Menu. Cells can be formatted for Numbers, Currency, Percentage, Date$NL$& Time, Duration, Fraction, Numeral System, Scientific and Text.$NL$• Sums Checklist – is identical to Checklist save that the first column contains checkboxes.$NL$Cells are formatted as Automatic when they are created. This means that data can be entered without the user having to$NL$consider which format to use. Numbers allows users to change cell formatting depending on their project requirements.$NL$The types of Cell Format are Numbers, Currency, Percentage, Date & Time, Duration, Fraction, Numeral System, Scientific,$NL$and Text. There are also, Stepper, Slider and Pop-up Menu formats.$NL$To manually format a cell, column or row, select it, then in the Cells Inspector click on the drop down menu and change$NL$the formatting accordingly. In addition users can make custom formats. This option is also found in the drop down menu$NL$of the Cells Inspector.$NL$If a date is entered into a Cell, for example 04/04/2012, Numbers will automatically read this as a date and will display$NL$it as such 4th April 2012.$NL$If this is not the desired date format, select that cell, column or row and in the Cells Inspector use the Date menu to$NL$change the formatting. The options found in the Cells Inspector can also be found on the Format Bar.$NL$When entering data into Cells, to move the insertion point to the next Cell in the row press TAB. To move the insertion$NL$point to the cell below press Return. Use the Arrow keys to move the insertion point freely.$NL$A series is a range of linked values. For example, months or weekdays where February follows January, Tuesday follows$NL$Monday. Numbers understands such series. To prove this point try typing January into a column. Select that cell and$NL$click-drag on the bottom right hand corner of the cell and, moving the cursor down the column, the column automatically$NL$sequences the rows with all the months of the year. The same process can be applied to weekdays.$NL$If a date is entered into the top body cell of a Column, for example 01/01/12, a date sequence can be created. By clickdragging$NL$on the bottom right hand corner of the cell and moving the cursor down the column, the column automatically$NL$sequences the rows as following days 2nd, 3rd, 4th January and so on. Brilliant, but this can be frustrating if, for instance, a$NL$series based on weeks is required. The way to achieve this is to manually enter the date information for the first two months$NL$of your series, say 04/04/2012 and 04/05/12. Then select the row for April, hold down the Shift key and select May, both$NL$rows are now selected. Click–drag the bottom right hand corner of the May cell and the date series now runs in months.$NL$In the illustration Column A was created using consecutive days, 04/04/12 was typed into Cell A1 then dragged to fill$NL$Column A. In Column B 04/04/12 was typed into Cell B1, then 11/04/12 into Cell B2 before both these Cells were selected$NL$and dragged to fill Column B. Column C is a series based on the first day of the month. Column D illustrates the drag$NL$operation, and is a series based on years.$NL$Use the Cell Inspector to change how the dates are displayed.$NL$Other series might include a dinner menu. Typing Aperitif, Starters, Main, Deserts, Coffee into a successive Rows of a$NL$Column then performing the Shift–Clicking and Dragging operation described above will repeat those five elements$NL$down the column ad nauseam.$NL$And there’s more. Hovering the cursor over the reference tab for the date column reveals a triangle, clicking on this$NL$invokes a menu. That menu has an option ‘Categorize by This Column’. If this is selected the months are automatically$NL$divided into years. If the sequence was stepping-up in days then ‘Categorize by This Column’ would divide the column$NL$into months.$NL$Numbers is full of features like this. There is not space to explore all the permutations here. The key to understanding lies$NL$in understanding how Cells are automatically formatted and how this formatting can be ‘trained’ to behave in new ways.$NL$Using the menus that appear on the reference tabs, users can sort columns and rows. Typically data is sorted by Column,$NL$for example by date ascending. Sometimes more sophisticated sort options are required, these can be accessed via the$NL$reference tab menus and are labelled, ‘Show More Options’. In the example Column B, Fruit Type is being sorted to display$NL$in alphabetical order.$NL$The following example shows a fictitious Table for a green-grocer’s fruit order. This and the previous illustration contain$NL$the same data, but the second Table has been categorized by Column.
7;The entry point into all our programs is called main() and this is a function, or a piece of code that$NL$does something, usually returning some value. We structure programs into functions to stop them$NL$become long unreadable blocks of code than cannot be seen in one screen or page and also to ensure$NL$that we do not have repeated identical chunks of code all over the place. We can call library$NL$functions like printf or strtok which are part of the C language and we can call our own or other$NL$peoples functions and libraries of functions. We have to ensure that the appropriate header file exists$NL$and can be read by the preprocessor and that the source code or compiled library exists too and is$NL$accessible.$NL$As we learned before, the scope of data is restricted to the function in which is was declared, so we$NL$use pointers to data and blocks of data to pass to functions that we wish to do some work on our data.$NL$We have seen already that strings are handled as pointers to arrays of single characters terminated with$NL$a NULL character.$NL$In this example we can repeatedly call the function “doit” that takes two integer arguments and reurns$NL$the result of some mathematical calculation.$NL$(by now you should be maintaining a Makefile as you progress, adding targets to compile examples as$NL$you go.)$NL$The result in a browser looks like this called with “func1?5:5”.$NL$In this case the arguments to our function are sent as copies and are not modified in the function but$NL$used.$NL$If we want to actual modify a variable we would have to send its pointer to a function.$NL$We send the address of the variable 'result' with &result, and in the function doit we de-reference the$NL$pointer with *result to get at the float and change its value, outside its scope inside main . This gives$NL$identical output to chapter3_1.c.$NL$C contains a number of built-in functions for doing commonly used tasks. So far we have used atoi,$NL$printf, sizeof, strtok, and sqrt. To get full details of any built-in library function all we have to do is$NL$type for example:$NL$and we will see all this:$NL$Which pretty-well tells you everything you need to know about this function and how to use it and$NL$variants of it. Most importantly it tells you which header file to include.$NL$There is no point in learning about library functions until you find you need to do something which$NL$then leads you to look for a function or a library of functions that has been written for this purpose.$NL$You will need to understand the function signature – or what the argument list means and how to use it$NL$and what will be returned by the function or done to variables passed as pointers to functions.$NL$Sometimes we wish to manage a set of variable as a group, perhaps taking all the values from a$NL$database record and passing the whole record around our program to process it. To do this we can$NL$group data into structures.$NL$This program uses a struct to define a set of properties for something called a player. The main$NL$function contains a declaration and instantiation of an array of 5 players. We pass a pointer to each$NL$array member in turn to a function to rank each one. This uses a switch statement to examine the first$NL$letter of each player name to make an arbitrary ranking. Then we pass a pointer to each array member$NL$in turn to a function that prints out the details.$NL$The results are shown here, as usual in a browser:$NL$This is a very powerful technique that is quite advanced but you will need to be aware of it. The idea$NL$of structures leads directly to the idea of classes and objects.$NL$We can see that using a struct greatly simplifies the business task of passing the data elements around$NL$the program to have different work done. If we make a change to the definition of the struct it will$NL$still work and we simply have to add code to handle new properties rather than having to change the$NL$argument lists or signatures of the functions doing the work.$NL$The definition of the structure does not actually create any data, but just sets out the formal shape of$NL$what we can instantiate. In the main function we can express this instantiation in the form shown$NL$creating a list of sequences of data elements that conform to the definition we have made.$NL$You can probably see that a struct with additional functions or methods is essentially what a class is in$NL$Java, and this is also the case in C++. Object Oriented languages start here and in fact many early$NL$systems described as “object oriented” were in fact just built using C language structs.$NL$If you take a look for example, at the Apache server development header files you will see a lot of$NL$structs for example in this fragment of httpd.h :$NL$Dont worry about what this all means – just notice that this is a very common and very powerful$NL$technique, and the design of data structures, just like the design of database tables to which it is closely$NL$related are the core, key, vital task for you to understand as a programmer.$NL$You make the philosophical decisions that the world is like this and can be modelled in this way. A$NL$heavy responsibility - in philosophy this work is called ontology (what exists?) and epistemology$NL$(how we can know about it?). I bet you never thought that this was what you were doing!$NL$We have used some simple data types to represent some information and transmit input to a program$NL$and to organise and display some visual output.$NL$We have used HTML embedded in output strings to make output visible in a web browser.$NL$We have learned about creating libraries of functions for reuse.$NL$We have learning about data structures and the use of pointers to pass them around a program.
7;Using the File Manager (in KDE, Konqueror or in Gnome, Nautilus) create a new directory$NL$somewhere in your home directory called something appropriate for all the examples in this book,$NL$perhaps “Programming_In_Linux” without any spaces in the name.$NL$Open an editor (in KDE, kate, or in Gnome, gedit) and type in (or copy from the supplied source code$NL$zip bundle) the following:$NL$Save the text as chapter1_1.c in the new folder you created in your home directory.$NL$Open a terminal window and type: gcc -o hello chapter1_1.c$NL$to compile the program into a form that can be executed.$NL$Now type “ls -l” to list the details of all the files in this directory. You should see that chapter1_2.c is$NL$there and a file called “hello” which is the compiled C program you have just written.$NL$Now type: ./hello$NL$to execute, or run the program and it should return the text:$NL$"Hello you are learning C!!".$NL$If this worked, congratulations, you are now a programmer!$NL$The part inside /*** ***/ is a comment and is not compiled but just for information and$NL$reference.$NL$The “#include...” part tells the compiler which system libraries are needed and which header files$NL$are being referenced by this program. In our case “printf” is used and this is defined in the$NL$stdio.h header.$NL$The “int main(int argc, char *argv[])” part is the start of the actual program. This is an entrypoint$NL$and most C programs have a main function.$NL$The “int argc” is an argument to the function “main” which is an integer count of the number of$NL$character string arguments passed in “char *argv[]” (a list of pointers to character strings) that$NL$might be passed at the command line when we run it.$NL$A pointer to some thing is a name given to a memory address for this kind of data type. We can$NL$have a pointer to an integer: int *iptr, or a floating point number: float *fPtr. Any list of things is$NL$described by [], and if we know exactly how big this list is we might declare it as [200]. In this$NL$case we know that the second argument is a list of pointers to character strings.$NL$Everything else in the curly brackets is the main function and in this case the entire program$NL$expressed as lines.$NL$Each line or statement end with a semi-colon “”.$NL$We have function calls like “printf(...)” which is a call to the standard input / output library$NL$defined in the header file stdio.h.$NL$At the end of the program “return 0” ends the program by returning a zero to the system.$NL$Return values are often used to indicate the success or status should the program not run$NL$correctly.$NL$Taking this example a stage further, examine the start of the program at the declaration of the entry$NL$point function: int main(int argc, char *argv[])$NL$In plain English this means:$NL$The function called “main”, which returns an integer, takes two arguments, an integer called “argc”$NL$which is a count of the number of command arguments then *argv[] which is a list or array of pointers$NL$to strings which are the actual arguments typed in when you run the program from the command line.$NL$Let's rewrite the program to see what all this means before we start to panic.$NL$Save the text as chapter1_2.c in the same folder.$NL$Open a terminal window and type:$NL$gcc -o hello2 chapter1_2.c to compile the program into a form that can be executed.$NL$Now type ls -l to list the details of all the files in this directory. You should see that chapter1_2.c is$NL$there and a file called hello2 which is the compiled C program you have just written.$NL$Now type ./hello2 to execute, or run the program and it should return the text:$NL$We can see that the name of the program itself is counted as a command line argument and that the$NL$counting of things in the list or array of arguments starts at zero not at one.$NL$Now type ./hello2 my name is David to execute the program and it should return the text:$NL$So, what is happening here? It seems we are reading back each of the character strings (words) that$NL$were typed in to run the program.$NL$Lets get real and run this in a web page. Make the extra change adding the first output printf statement$NL$“Content-type:text/plain\n\n” which tells our server what kind of MIME type is going to be$NL$transmitted.$NL$Compile using gcc -o hello3 chapter1_3.c and copy the compiled file hello3 to your$NL$public_html/cgi-bin directory (or on your own machine as superuser copy the program to$NL$/srv/www/cgi-bin (OpenSuse) or /usr/lib/cgi-bin (Ubuntu)).$NL$Open a web browser and type in the URL http://localhost/cgi-bin/hello3?david+haskins and you$NL$should see that web content can be generated by a C program.$NL$A seldom documented feature of the function signature for “main” is that it can take three arguments$NL$and the last one we will now look at is char *env[ ] which is also a list of pointers to strings, but in this$NL$case these are the system environment variables available to the program at the time it is run$NL$Compile with gcc -o hello4 chapter1_4.c and as superuser copy the program to /srv/www/cgi-bin$NL$(OpenSuse) or /usr/lib/cgi-bin (Ubuntu). You can run this from the terminal where you compiled it$NL$with ./hello4 and you will see a long list of environment variables. In the browser when you enter$NL$http://localhost/cgi-bin/hello4 you will a different set altogether.$NL$We will soon find out that QUERY_STRING is an important environment variable for us in$NL$communicating with our program and in this case we see it has a value of “david+haskins” or$NL$everything after the “?” in the URL we typed. It is a valid way to send information to a common$NL$gateway interface (CGI) program like hello4 but we should restrict this to just one string. In our case$NL$we have used a “+” to join up two strings. If we typed: “david haskins” the browser would translate$NL$this so we would see:$NL$QUERY_STRING=david%20haskins$NL$We will learn later how complex sets of input values can be transmitted to our programs.
7;When we write programs we have to make decisions or assertions about the nature of the world as we$NL$declare and describe variables to represent the kinds of things we want to include in our information$NL$processing.$NL$This process is deeply philosophical we make ontological assertions that this or that thing exists and$NL$we make epistemological assertions when we select particular data types or collections of data types$NL$to use to describe the attributes of these things. Heavy stuff with a great responsibility and not to be$NL$lightly undertaken.$NL$As a practical example we might declare something that looks like the beginnings of a database record$NL$for geography.$NL$Here we are doing the following:$NL$- asserting that all the character strings we will ever encounter in this application will be 255$NL$limited to characters so we define this with a preprocessor statement – these start with #.$NL$- assert that towns are associated with counties, and counties are associated with countries some$NL$hierarchical manner.$NL$- assert that the population is counted in whole numbers – no half-people.$NL$- assert the location is to be recorded in a particular variant (WGS84) of the convention of$NL$describing spots on the surface of the world in latitude and longitude that uses a decimal$NL$fraction for degrees, minutes, and seconds.$NL$Each of these statements allocates memory within the scope of the function in which it is declared.$NL$Each data declaration will occupy an amount of memory in bytes and give that bit of memory a$NL$label which is the variable name. Each data type has a specified size and the sizeof() library function$NL$will return this as an integer. In this case 3 x 256 characters, one integer, and two floats. The exact$NL$size is machine dependent but probably it is 780 bytes.$NL$Outside the function in which the data has been declared this data is inaccessible – this is the scope of$NL$declaration. If we had declared outside the main() function it would be global in scope and other$NL$functions could access it. C lets you do this kind of dangerous stuff if you want to, so be careful.$NL$Generally we keep a close eye on the scope of data, and pass either read-only copies, or labelled$NL$memory addresses to our data to parts of the programs that might need to do work on it and even$NL$change it. These labelled memory addresses are called pointers.$NL$We are using for output the printf family of library functions (sprintf for creating strings, fprintf for$NL$writing to files etc) which all use a common format string argument to specify how the data is to be$NL$represented.$NL$- %c character$NL$- %s string$NL$- %d integer$NL$- %f floating point number etc.$NL$The remaining series of variables in the arguments are placed in sequence into the format string as$NL$specified.$NL$In C it is a good idea to intialise any data you declare as the contents of the memory allocated for$NL$them is not cleared but may contain any old rubbish.$NL$Compile with: gcc -o data1 chapter2_1.c -lc$NL$Output of the program when called with : ./data1$NL$Some programming languages like Java and C++ have a string data type that hides some of the$NL$complexity underneath what might seem a simple thing.$NL$An essential attribute of a character string is that it is a series of individual character elements of$NL$indeterminate length.$NL$Most of the individual characters we can type into a keyboard are represented by simple numerical$NL$ASCII codes and the C data type char is used to store character data.$NL$Strings are stored as arrays of characters ending with a NULL so an array must be large enough to$NL$hold the sequence of characters plus one. Remember array members are always counted from zero.$NL$In this example we can see 5 individual characters declared and initialised with values, and an empty$NL$character array set to “”.$NL$Take care to notice the difference between single quote marks ' used around characters and double$NL$quote marks “ used around character strings.$NL$Compile with: gcc -o data2 chapter2_2.c -lc$NL$Output of the program when called with : ./data2$NL$Anything at all – name given to a variable and its meaning or its use is entirely in the mind of the$NL$beholder. Try this$NL$Download free ebooks at bookboon.com$NL$C Programming in Linux$NL$29$NL$Data and Memory$NL$Compile with: gcc -o data3 chapter2_3.c -lc$NL$As superuser copy the program to your public_html/cgi-bin directory (or /srv/www/cgi-bin$NL$(OpenSuse) or /usr/lib/cgi-bin (Ubuntu)).$NL$In the browser enter: http://localhost/cgi-bin/data3?red$NL$what you should see is this:$NL$Or if send a parameter of anything at all you will get surprising results:$NL$What we are doing here is using the string parameter argv[1] as a background colour code inside an$NL$HTML body tag. We change the Content-type specification to text/html and miraculously now our$NL$program is generating HTML content. A language being expressed inside another language. Web$NL$browsers understand a limited set of colour terms and colours can be also defined hexadecimal codes$NL$such as #FFFFFF (white) #FF0000 (red) #00FF00 (green) #0000FF (blue).$NL$This fun exercise is not just a lightweight trick, the idea that one program can generate another in$NL$another language is very powerful and behind the whole power of the internet. When we generate$NL$HTML (or XML or anything else) from a common gateway interface program like this we are$NL$creating dynamic content that can be linked to live, changing data rather than static pre-edited web$NL$pages. In practice most web sites have a mix of dynamic and static content, but here we see just how$NL$this is done at a very simple level.$NL$Throughout this book we will use the browser as the preferred interface to our programs hence we will$NL$be generating HTML and binary image stream web content purely as a means to make immediate the$NL$power of our programs. Writing code that you peer at in a terminal screen is not too impressive, and$NL$writing window-type applications is not nearly so straightforward.$NL$In practice most of the software you may be asked to write will be running on the web so we might as$NL$well start with this idea straight away. Most web applications involve multiple languages too such as$NL$CSS, (X)HTML, XML, JavaScript, PHP, JAVA, JSP, ASP, .NET, SQL. If this sounds frightening, don't$NL$panic. A knowledge of C will show you that many of these languages, which all perform different$NL$functions, have a basis of C in their syntax.
7;This creates a folder of the name you give it (labelmaker) and a Makefile, a modules.mk file which$NL$can be used by the Make utility, and a file called mod_labelmaker.c.$NL$The C file generated is kind of like a Hello World for Apache. It may look like a complex thing but it$NL$does supply a long explanatory comment header which is worth reading. The idea is that when$NL$Apache starts any modules in a specified location which are configured as needing to be loaded in the$NL$server configuration files, will be loaded. The *_register_hooks function lists the names and$NL$signatures of functions that can be called at specific stages in the Apache server process. In this case if$NL$the name http://localhost/labelmaker is called this module will be asked to handle whatever happens in$NL$the *_handler function.$NL$$NL$The configuration of the server can be a bit fiddly but in OpenSuse we have to add this to the file$NL$$NL$and in /etc/config.sys/apache2 we add the name of our module labelmaker to long comma-separated$NL$list in the line starting$NL$APACHE_MODULES=”.....,labelmaker”$NL$Now go to the folder labelmaker and type:$NL$$NL$Now we can plug in the work we did for the graphics library in Chapter 6 as a replacement handler$NL$function (in the code Chapter7_1.c there are BOTH handlers, one commented out). Note the$NL$(highlighted) call to a modified decode_value function that uses the r->args pointer to get the$NL$QUERY_STRING rather than getenv() . Also Apache handles the output a bit differently too – get get$NL$a pointer to the array of bytes in the image by calling gdImageGifPtr then the ap_rwrite function$NL$outputs the data. We have to free the pointer with gdFree after the output call.$NL$$NL$Whilst tricky to write and debug, this is probably the most rewarding and esoteric area where you can$NL$do real, commerically useful and safely deployable web content generation. It is easy to see how this$NL$example could be extended with parameters for colours and fonts to make a useful web content tool.$NL$There is very little clear simple material about apache modukles but start with the on-line$NL$documentation at http://httpd.apache.org/docs/2.2/developer/$NL$One recent book worth looking at is “The Apache Modules Book” Nick Kew, Prentice Hall.$NL$$NL$The ability to write short programs in C to automate tedious tasks or to do things that would otherwise$NL$take hours of fiddling about with cumbersome tools such as doing mail-merge, is one on the things$NL$you will be most pleased you have learned how to do. This project is such a time-saver. Ghost is a$NL$lightweight PHP generator for you to customise.$NL$If you find yourself having to build PHP web sites all the time, a quick way to generate all the$NL$parameter-passing, decoding, forms building and database management code in one step would be$NL$useful. Tools like Ruby on Rails offer such functionality but are infinitely more complex to set up and$NL$run and you end up with needing to learn yet another language to go any further.$NL$Probably the best way to start with this tool is to compile and run it. Unzip the ghost.zip source into$NL$your public_html folder which creates a folder called ghost. The Makefile contains a target g1 that$NL$compiles and links ghost. So go to public_html/ghost and type: make g1 .$NL$To run the site generator type:$NL$- ./ghost testwebsite data1 data2 data1 data3 data4 data6 data6$NL$- This will create:$NL$- a folder public_html/testwebsite$NL$- a mysql database table called testwebsite with text fields data1 data2 data1 data3 data4 data6$NL$data6$NL$- atestwebsite.css file$NL$- empty header.html and footer.html pages$NL$- index.php that demonstrates a form handling entry, edit & update, and delete to the database$NL$table for the data items specified.$NL$In a browser what you see is this at http://localhost/~yourname/testwebsite$NL$The idea behind this is that all the mechanical bits to create and manage the form content are done and$NL$can be customised. This screen shot shows the result of submitting one record. Thde top row is for$NL$entering new data, the lower row(s) allow editing or deleting of records. It is a framework that allows$NL$you to take and use parts in your own website design.$NL$Let us examine this code in sections.$NL$The first section declares the required data and creates the folder and CSS file.$NL$Next the header.html and footer.html files are generated. These files is loaded by the PHP file and$NL$could be used as a generic common header and footers. The CSS file is referenced from the$NL$header.html file.$NL$Next we create the data base.$NL$The complicated part starts now, of generating a php script. The best way to understand this is to$NL$examine the actual output of the program when we view the source of the page in the browser.$NL$The top row is a form with a text box for each column defined in the table generated by running the$NL$ghost program.$NL$For each row in the table we now generate a form allowing editing of the data and an anchor link to do$NL$a delete operation.$NL$Close examination of the file index.php will allow you to see where all this happens, and to work$NL$backward to find where in the ghost.c source code this PHP code is generated. A good idea is to use a$NL$highlighter pen on a printout as we are embedding a language (HTML) inside another language (PHP)$NL$which is in turn inside another language so very very careful use is made of the escape characters '\ 'to$NL$express quotation marks both single and double where necessary to make it all work. This may seem$NL$complex – but the speedy prototyping that ghost permits makes it worthwhile to spend time customising$NL$the C code so the PHP that you want and the database you want come out the way you want it.$NL$Here is the part of the PHP file index.php which generates the edit or delete rows. The static HTML is$NL$highlighted and the other parts are inserted by MySQL PHP function calls.$NL$As you can see a great deal of tedious and repetitive works has been automated. You can move on by$NL$modifying the PHP code or go deeper to customise the C program which generates all of it.$NL$I personally use ghost frequently to save time on site-building and this is why I wrote it. I got bored$NL$making mistakes writing virtually identical code to decode HTML forms and populate or update$NL$databases.
8;In Chapter 1 I defined a database to be “… an organised, machine-readable collection of symbols, to be$NL$interpreted as a true account of some enterprise.” I also gave this example (extracted from Figure 1.1):$NL$I suggested that those green symbols, organised as they are with respect to the blue ones, might be$NL$understood to mean:$NL$“Student S1, named Anne, is enrolled on course C1.”$NL$In this chapter I explain exactly how such an interpretation can be justified. In fact, I describe the general$NL$method under which data organized in the form of relations is to be interpretedto yield information, as$NL$some people say. This method of interpretation is firmly based in the science of logic. Relational database$NL$theory is based very directly on logic. Predicates and propositions are the fundamental concepts that logic$NL$deals with.$NL$Fortunately, we need to understand only the few basic principles on which logic is founded. You may well$NL$already have a good grasp of the principles in question, but even if you do, please do not skip this chapter.$NL$For one thing, the textbooks on logic do not all use exactly the same terminology and I have chosen the$NL$terms and definitions that seem most suitable for the purpose at hand. For another, I do of course$NL$concentrate on the points that are particularly relevant to relational theory you need to know which points$NL$those are and to understand exactly why they are so relevant.$NL$Predicates, one might say, are what logic is all about. And yet the textbooks do not speak with one voice$NL$when it comes to pinning down exactly what the term refers to! I choose the definition that appears to me$NL$to fit best, so to speak, with relational database theory. We start by looking again at that possible$NL$interpretation of the symbols S1, Anne, and C1, placed the way they are in Figure 1.1:$NL$“Student S1, named Anne, is enrolled on course C1.”$NL$This is a sentence. Sentences are what human beings typically use to communicate with each other, using$NL$language. We express our interpretations of the data using sentences in human language and we use$NL$relations to organize the data to be interpreted. Logic bridges the gap between relations and sentences.$NL$Our example sentence can be recast into two simpler sentences, “Student S1 is named Anne” and “Student$NL$S1 is enrolled on course C1”. Let’s focus on the second:$NL$The symbols S1 and C1 appear both in this sentence and in the data whose meaning it expresses. Because$NL$they each designate, or refer to, a particular thingS1 a particular student, C1 a particular coursethey$NL$are called designators. The word Anne is another designator, referring to a particular forename. “An$NL$Introduction to Relational Database Theory” is also a designator, referring to a particular book, and so is,$NL$for example, -7, referring to a particular number.$NL$Now, suppose we replace S1 and C1 in Example 3.1 by another pair of symbols, taken from the same$NL$columns of Figure 1.1 but a different row. Then we might obtain$NL$A pattern is clearly emerging. For every row in Figure 1.1, considering just the columns headed StudentId$NL$and CourseId, we can obtain a sentence in the form of Examples 3.1 and 3.2. The words “Student … is$NL$enrolled on course …” appear in that order in each case and in each case the gaps indicated$NL$by …sometimes called placeholdersare replaced by appropriate designators. If we now replace each$NL$placeholder by the name given in the heading of the column from which the appropriate designator is to be$NL$drawn, we obtain this:$NL$Example 3.3 succinctly expresses the way in which the named columns in each row of Figure 1.1 are$NL$probably to be interpreted. And we now know that those names, StudentId and CourseId, in the column$NL$headings are the names of two of the attributes of the relation that Figure 1.1 depicts in tabular form.$NL$Now, the sentences in Examples 3.1 and 3.2 are in fact statements. They state something of which it can$NL$be said, “That is true”, or “That is not true”, or “I believe that”, or “I don’t believe that”.$NL$Not all sentences are statements. A good informal test, in English, to determine whether a sentence is a$NL$statement is to place “Is it true that” in front of it. If the result is a grammatical English question, then the$NL$original sentence is indeed a statement otherwise it is not. Here are some sentences that are not statements:$NL$•“Let’s all get drunk.”$NL$•“Will you marry me?”$NL$•“Please pass me the salt.”$NL$•“If music be the food of love, play on.”$NL$They each fail the test. In fact one of them is a question itself and the other three are imperatives, but we$NL$have no need of such sentences in our interpretation of relations because we seek only information, in the$NL$form of statementsstatements that we are prepared to believe are statements of fact in other words,$NL$statements we believe to be true. We do not expect a database to be interpreted as asking questions or$NL$giving orders. We expect it to be stating facts (or at least what are believed to be facts). As an aside, I must own up to the fact that some sentences that would be accepted as statements in English$NL$don’t really pass the test as they stand. Here are two cases in point, from Shakespeare:$NL$•“O for a muse of fire that would ascend the highest heaven of invention.”$NL$•“To be or not to bethat is the question.”$NL$The first appears to lack a verb, but we know that “O for a …” is a poetical way of expressing a wish for$NL$something on the part of the speaker, so we can paraphrase it fairly accurately by replacing “O” by “I$NL$wish”, and the sentence thus revised passes the test. In the second case we have only to delete the word$NL$“that”, whose presence serves only for emphasis (and scansion, of course!), and alter the punctuation$NL$slightly: “It is true that ‘to be or not to be?’ is the question.”$NL$Now, a statement is a sentence that is declarative in form: it declares something that is supposed to be true.$NL$Example 3.3, “Student StudentId is enrolled on course CourseId”, is not a statementit does not pass the$NL$test. It does, however, have the grammatical form of a statement. We can say that, like a statement, it is$NL$declarative in form.
8;This chapter gives a very broad overview of$NL$•what a database is$NL$•what a relational database is, in particular$NL$•what a database management system (DBMS) is$NL$•what a DBMS does$NL$•how a relational DBMS does what a DBMS does$NL$We start to familiarise ourselves with terminology and notation used in the remainder of the book, and we$NL$get a brief introduction to each topic that is covered in more detail in later sections.$NL$$NL$You will find many definitions of this term if you look around the literature and the Web. At one time (in$NL$2008), Wikipedia [1] offered this: “A structured collection of records or data.” I prefer to elaborate a little:$NL$$NL$The organized, machine-readable collection of symbols is what you “see” if you “look at” a database at a$NL$particular point in time. It is to be interpreted as a true account of the enterprise at that point in time. Of$NL$course it might happen to be incorrect, incomplete or inaccurate, so perhaps it is better to say that the$NL$account is believed to be true.$NL$The alternative view of a database as a collection of variables reflects the fact that the account of the$NL$enterprise has to change from time to time, depending on the frequency of change in the details we choose$NL$to include in that account.$NL$The suitability of a particular kind of database (such as relational, or object-oriented) might depend to$NL$some extent on the requirements of its user(s). When E.F. Codd developed his theory of relational$NL$databases (first published in 1969), he sought an approach that would satisfy the widest possible ranges of$NL$users and uses. Thus, when designing a relational database we do so without trying to anticipate specific$NL$uses to which it might be put, without building in biases that would favour particular applications. That is$NL$perhaps the distinguishing feature of the relational approach, and you should bear it in mind as we explore$NL$some of its ramifications.$NL$$NL$For example, the table in Figure 1.1 shows an organized collection of symbols.$NL$$NL$Can you guess what this tabular arrangement of symbols might be trying to tell us? What might it mean,$NL$for symbols to appear in the same row? In the same column? In what way might the meaning of the$NL$symbols in the very first row (shown in blue) differ from the meaning of those below them?$NL$Do you intuitively guess that the symbols below the first row in the first column are all student$NL$identifiers, those in the second column names of students, and those in the third course identifiers? Do$NL$you guess that student S1’s name is Anne? And that Anne is enrolled on courses C1 and C2? And that$NL$Cindy is enrolled on neither of those two courses? If so, what features of the organization of the symbols$NL$led you to those guesses?$NL$Remember those features. In an informal way they form the foundation of relational theory. Each of them$NL$has a formal counterpart in relational theory, and those formal counterparts are the only constituents of the$NL$organized structure that is a relational database.$NL$$NL$Perhaps those green symbols, organized as they are with respect to the blue ones, are to be$NL$understood to mean:$NL$“Student S1, named Anne, is enrolled on course C1.”$NL$An important thing to note here is that only certain symbols from the sentence in quotes appear in the$NL$table—S1, Anne, and C1. None of the other words appear in the table. The symbols in the top row of$NL$the table (presumably column headings, though we haven’t actually been told that) might help us to$NL$guess “student”, “named”, and “course”, but nothing in the table hints at “enrolled”. And even if those$NL$assumed column headings had been A, B and C, or X, Y and Z, the given interpretation might still be$NL$the intended one.$NL$Now, we can take the sentence “Student S1, named Anne, is enrolled on course C1” and replace$NL$each of S1, Anne, and C1 by the corresponding symbols taken from some other row in the table, such$NL$as S2, Boris, and C1. In so doing, we are applying exactly the same mode of interpretation to each row.$NL$If that is indeed how the table is meant to be interpreted, then we can conclude that the following$NL$sentences are all true:$NL$Student S1, named Anne, is enrolled on course C1.$NL$Student S1, named Anne, is enrolled on course C2.$NL$Student S2, named Boris, is enrolled on course C1.$NL$Student S3, named Cindy, is enrolled on course C3.$NL$$NL$In Chapter 3, “Predicates and Propositions”, we shall see exactly how such interpretations can be$NL$systematically formalized. In Chapter 4, “Relational AlgebraThe Foundation”, and Chapter 5, “Building$NL$on The Foundation”, we shall see how they help us to formulate correct queries to derive useful$NL$information from a relational database.$NL$$NL$We have added the name, ENROLMENT, above the table, and we have added an extra row.$NL$ENROLMENT is a variable. Perhaps the table we saw earlier was once its value. If so, it (the variable) has$NL$been updated since thenthe row for S4 has been added. Our interpretation of Figure 1.1 now has to be$NL$revised to include the sentence represented by that additional row:$NL$Student S1, named Anne, is enrolled on course C1.$NL$Student S1, named Anne, is enrolled on course C2.$NL$Student S2, named Boris, is enrolled on course C1.$NL$Student S3, named Cindy, is enrolled on course C3.$NL$Student S4, named Devinder, is enrolled on course C1.$NL$Notice that in English we can join all these sentences together to form a single sentence, using$NL$conjunctions like “and”, “or”, “because” and so on. If we join them using “and” in particular, we get a$NL$single sentence that is logically equivalent to the given set of sentences in the sense that it is true if each$NL$one of them is true (and false if any one of them is false). A database, then, can be thought of as a$NL$representation of an account of the enterprise expressed as a single sentence! (But it’s more usual to think$NL$in terms of a collection of individual sentences.)$NL$We might also be able to conclude that the following sentences (for example) are false:$NL$Student S2, named Boris, is enrolled on course C2.$NL$Student S2, named Beth, is enrolled on course C1.
8;In this chapter we look at the four fundamental concepts on which most computer languages are based.$NL$We acquire some useful terminology to help us talk about these concepts in a precise way, and we begin$NL$to see how the concepts apply to relational database languages in particular. It is quite possible that you$NL$are already very familiar with these conceptsindeed, if you have done any computer programming they$NL$cannot be totally new to youbut I urge you to study the chapter carefully anyway, as not everybody$NL$uses exactly the same terminology (and not everybody is as careful about their use of terminology as we$NL$need to be in the present context). And in any case I also define some special terms, introduced by C.J.$NL$Date and myself in the 1990s, which have perhaps not yet achieved wide usagefor example, selector$NL$and possrep.$NL$I wrote “most computer languages” because some languages dispense with variables. Database languages$NL$typically do not dispense with variables because it seems to be the very nature of what we call a database$NL$that it varies over time in keeping with changes in the enterprise. Money changes hands, employees come$NL$and go, get salary rises, change jobs, and so on. A language that supports variables is said to be an$NL$imperative language (and one that does not is a functional language). The term “imperative” appeals to the$NL$notion of commands that such a language needs for purposes such as updating variables. A command is an$NL$instruction, written in some computer language, to tell the system to do something. The terms statement$NL$(very commonly) and imperative (rarely) are used instead of command. In this book I use statement quite$NL$frequently, bowing to common usage, but I really prefer command because it is more appropriate also, in$NL$normal discourse statement refers to a sentence of the very important kind described in Chapter 3 and does$NL$not instruct anybody to do anything.$NL$$NL$Figure 2.1 shows a simple commandthe assignment, Y := X + 1dissected into its component parts.$NL$The annotations show the terms we use for those components.$NL$$NL$It is important to distinguish carefully between the concepts and the language constructs that$NL$represent (denote) those concepts. It is the distinction between what is written and what it meanssyntax$NL$and semantics.$NL$$NL$Each annotated component in Figure 1 is an example of a certain language construct. The annotation$NL$shows the term used for the language construct and also the term for the concept it denotes. Honouring$NL$this distinction at all times can lead to laborious prose. Furthermore, we don’t always have distinct terms$NL$for the language construct and the corresponding concept. For example, there is no single-word term for$NL$an expression denoting an argument. We can write “argument expression” when we need to be absolutely$NL$clear and there is any danger of ambiguity, but normally we would just say, for example, that X+1 is an$NL$argument to that invocation of the operator “:=” shown in Figure 2.1. (The real argument is the result of$NL$evaluating X+1.)$NL$The update operator “:=” is known as assignment. The command Y := X+1 is an invocation of$NL$assignment, often referred to as just an assignment. The effect of that assignment is to evaluate the$NL$expression X+1, yielding some numerical result r and then to assign r to the variable Y. Subsequent$NL$references to Y therefore yield r (until some command is given to assign something else to Y).$NL$Note the two operands of the assignment: Y is the target, X+1 the source. The terms target and source$NL$here are names for the parameters of the operator. In the example, the argument expression Y is$NL$substituted for the parameter target and the argument expression X+1 is substituted for the parameter$NL$source. We say that target is subject to update, meaning that any argument expression substituted for it$NL$must denote a variable. The other parameter, source, is not subject to update, so any argument expression$NL$substituted must denote a value, not a variable. Y denotes a variable and X+1 denotes a value. When the$NL$assignment is evaluated (or, as we sometimes say of commands, executed), the variable denoted by Y$NL$becomes the argument substituted for target, and the current value of X+1 becomes the argument$NL$substituted for source.$NL$Whereas the Y in Y := X + 1 denotes a variable, as I have explained, the X in Y := X + 1 does not,$NL$as I am about to explain. So now let’s analyse the expression X+1. It is an invocation of the read-only$NL$operator +, which has two parameters, perhaps named a and b. Neither a nor b is subject to update. A$NL$read-only operator is one that has no parameter that is subject to update. Evaluation of an invocation of a$NL$read-only operator yields a value and updates nothing. The arguments to the invocation, in this example$NL$denoted by the expressions X and 1, are the values denoted by those two expressions. 1 is a literal,$NL$denoting the numerical value that it always denotes X is a variable reference, denoting the value currently$NL$assigned to X.$NL$A literal is an expression that denotes a value and does not contain any variable references. But we do not$NL$use that term for all such expressions: for example, the expression 1+2, denoting the number 3, is not a$NL$literal. I defer a precise definition of literal to later in the present chapter.$NL$$NL$The following very important distinctions emerge from the previous section and should be firmly$NL$taken on board:$NL$•Syntax versus semantics$NL$•Value versus variable$NL$•Variable versus variable reference$NL$•Update operator versus read-only operator$NL$•Operator versus invocation$NL$•Parameter versus argument$NL$•Parameter subject to update versus parameter not subject to update$NL$Each of these distinctions is illustrated in Figure 2.1, as follows:$NL$•Value versus variable: Y denotes a variable, X denotes the value currently assigned to the$NL$variable X. 1 denotes a value. Although X and Y are both symbols referencing variables, what they$NL$denote depends in the context in which those references appear. Y appears as an update target and$NL$thus denotes the variable of that name, whereas X appears where an expression denoting a value is$NL$expected and that position denotes the current value of the referenced variable. Note that variables,$NL$by definition, are subject to change (in value) from time to time. A value, by contrast, exists$NL$independently of time and space and is not subject to change.
9;Many kinds of things in the world fall into related groups of ‘families’. ‘Inheritance’ is the idea ‘passing down’ characteristics from parent to child, and plays an important part in Object Oriented design and programming.$NL$While you are probably already familiar with constructors, and access control (public/private), there are particular issues in relating these to inheritance we need to consider.$NL$Additionally we need to consider the use of Abstract classes and method overriding as these are important concepts in the context of inheritance.$NL$Finally we will look at the ‘Object’ class which has a special role in relation to all other classes in C#.$NL$Classes are a generalized form from which objects with differing details can be created. Objects are thus ‘instances’ of their class. For example Student 051234567 is an instance of class Student. More concisely, 051234567 is a Student. Constructors are special methods that create an object from the class definition.$NL$Classes themselves can often be organised by a similar kind of relationship.$NL$One hierarchy, that we all have some familiarity with, is that which describes the animal kingdom :-$NL$• Kingdom (e.g. animals)$NL$• Phylum (e.g. vertebrates)$NL$• Class (e.g. mammal)$NL$• Order (e.g. carnivore)$NL$• Family (e.g. cat)$NL$• Genus (e.g. felix)$NL$• Species (e.g. felix leo)$NL$We can represent this hierarchy graphically ….$NL$Of course to draw the complete diagram would take more time and space than we have available.$NL$Here we can see one specific animal shown here :-‘Fred’. Fred is not a class of animal but an actual animal.$NL$Fred is a felix leo is a felix is a cat is a carnivore$NL$Carnivores eat meat so Fred has the characteristic ‘eats meat’.$NL$Fred is a felix leo is a felix is a cat is a carnivore is a mammal is a vertebrate$NL$Vertebrates have a backbone so Fred has the characteristic ‘has a backbone’.$NL$The ‘is a’ relationship links an individual to a hierarchy of characteristics. This sort of relationship applies to many real world entities, e.g. BonusSuperSaver is a SavingsAccount is a BankAccount.$NL$We specify the general characteristics high up in the hierarchy and more specific characteristics lower down. An important principle in OO – we call this generalization and specialization.$NL$All the characteristics from classes above in a class/object in the hierarchy are automatically featured in it – we call this inheritance.$NL$Consider books and magazines - both are specific types of publication.$NL$We can show classes to represent these on a UML class diagram. In doing so we can see some of the instance variables and methods these classes may have.$NL$Attributes ‘title’, ‘author’ and ‘price’ are obvious. Less obvious is ‘copies’ this is how many are currently in stock. $NL$For books, OrderCopies() takes a parameter specifying how many extra copies are added to stock.$NL$For magazines, orderQty is the number of copies received of each new issue and currIssue is the date/period of the current issue (e.g. “January 2011”, “Fri 6 Jan”, “Spring 2011” etc.) When a new issue is received the old issues are discarded and orderQty copies are placed in stock. Therefore RecNewIssue() sets currIssue to the date of new issue and restores copies to orderQty. AdjustQty() modifies orderQty to alter how many copies of subsequent issues will be stocked.$NL$We can separate out (‘factor out’) these common members of the classes into a superclass called Publication. In C# a superclass is often called a base class.$NL$The differences will need to be specified as additional members for the ‘subclasses’ Book and Magazine.$NL$In this is a UML Class Diagram the hollow-centred arrow denotes inheritance.$NL$Note the subclass has the generalized superclass (or base class) characteristics + additional specialized characteristics. Thus the Book class has four instance variables (title, price, copies and author) it also has two methods (SellCopy() and OrderCopies()).$NL$The inherited characteristics are not listed in subclasses. The arrow shows they are acquired from the superclass.$NL$No special features are required to create a superclass. Thus any class can be a superclass unless specifically prevented.$NL$A subclass specifies it is inheriting features from a superclass using the : symbol. For example….$NL$Constructors are methods that create objects from a class. Each class (whether sub or super) should encapsulate its own initialization in a constructor, usually relating to setting the initial state of its instance variables. Constructors are methods given the same name as the class.$NL$A constructor for a superclass, or base class, should deal with general initialization.$NL$Each subclass can have its own constructor for specialised initialization but it must often invoke the behaviour of the base constructor. It does this using the keyword base.$NL$Usually some of the parameters passed to MySubClass will be initializer values for superclass instance variables, and these will simply be passed on to the superclass constructor as parameters. In other words super-parameters will be some (or all) of sub-parameters. $NL$Shown below are two constructors, one for the Publication class and one for Book. The book constructor requires four parameters three of which are immediately passed on to the base constructor to initialize its instance variables.$NL$Thus in creating a book object we first create a publication object. The constructor for Book does this by calling the constructor for Publication.$NL$Rules exist that govern the invocation of a superconstructor.$NL$If the superclass has a parameterless (or default) constructor this will be called automatically if no explicit call to base is made in the subclass constructor though an explicit call is still better style for reasons of clarity.$NL$However if the superclass has no parameterless constructor but does have a parameterized one, this must be called explicitly using : base.$NL$To illustrate this….$NL$On the left above:- it is legal, though bad practice, to have a subclass with no constructor because superclass has a parameterless constructor.$NL$In the centre:- if subclass constructor doesn’t call the base constructor then the parameterless superclass constructor will be called.$NL$On the right:- because superclass has no paramterless constructor, subclass must have a constructor, it must call the super constructor using the keyword base and it must pass on the required paramter. This is simply because a (super) class with only a parameterized constructor can only be initialized by providing the required parameter(s).
9;Historically in computer programs method names were required to be unique. Thus the compiler could identify which method was being invoked just by looking at its name.$NL$However several methods were often required to perform very similar functionality for example a method could add two integer numbers together and another method may be required to add two floating point numbers. If you have to give these two methods unique names which one would you call ‘Add()’? $NL$In order to give each method a unique name the names would need to be longer and more specific. We could therefore call one method AddInt() and the other AddFloat() but this could lead to a proliferation of names each one describing different methods that are essentially performing the same operation i.e. adding two numbers.$NL$To overcome this problem in C# you are not required to give each method a unique name – thus both of the methods above could be called Add(). However if method names are not unique the C# must have some other way of deciding which method to invoke at run time. i.e. when a call is made to Add(number1, number2) the machine must decide which of the two methods to use. It does this by looking at the parameter list.$NL$While the two methods may have the same name they can still be distinguished by looking at the parameter list. :-$NL$Add(int number1, int number2)$NL$Add(float number1, float number2)$NL$This is resolved at run time by looking at the method call and the actual parameters being passed. If two integers are being passed then the first method is invoked. However if two floating point numbers are passed then the second method is used.$NL$Overloading refers to the fact that several methods may share the same name. As method names are no longer uniquely identify the method then the name is ‘overloaded’.$NL$Having several methods that essentially perform the same operation, but which take different parameter lists, can lead to enhanced flexibility and robustness in a system. $NL$Imagine a University student management system. A method would probably be required to enrol, or register, a new student. Such a method could have the following signature …$NL$EnrollStudent(String name, String address, String coursecode) $NL$However if a student had just arrived in the city and had not yet sorted out where they were living would the University want to refuse to enrol the student? They could do so but would it not be better to allow such a student to enrol (and set the address to ‘unkown’)?$NL$To allow this the method EnrollStudent() could be overloaded and an alternative method provided as…$NL$EnrollStudent(String name, String coursecode) $NL$At run time the method invoked will depend upon the parameter list provided. Thus given a call to $NL$EnrollStudent(“Fred”, “123 Abbey Gardens”, “G700”)$NL$the first method would be used.$NL$Overloading methods don’t just provide more flexibility for the user they also provide more flexibility for programmers who may have the job of extending the system in the future and thus overloading methods can make the system more future proof and robust to changing requirements.$NL$Constructors can be overloaded as well as ordinary methods.$NL$We can make our programs more adaptable by overloading constructors and other methods. Even if we don’t initially use all of the different constructors, or methods, by providing them we are making our programs more flexible and adaptable to meet changing requirements.$NL$Method overloading is the name given to the concept that several methods may exist that essentially perform the same operation and thus have the same name.$NL$The CLR engine distinguishes these by looking at the parameter list. If two or more methods have the same name then their parameter list must be different.$NL$At run time each method call, which may be ambiguous, is resolved by the CLR engine by looking at the parameters passed and matching the data types with the method signatures defined in the class. $NL$By overloading constructors and ordinary methods we are providing extra flexibility to the programmers who may use our classes in the future. Even if these are not all used initially, providing these can help make the program more flexible to meet changing user requirements.$NL$The development of any computer program starts by identifying a need :- $NL$• An engineer who specialises in designing bridges may need some software to create three dimensional models of the designs so people can visualise the finished bridge long before it is actually built.$NL$• A manager may need a piece of software to keep track of personnel, what projects they are assigned to, what skills they have and what skills need to be developed etc.$NL$But how do we get from a ‘need’ for some software to an object oriented software design that will meet this need?$NL$Some software engineers specialise in the task of Requirement Analysis which is the task of clarifying exactly what is required of the software. Often this is done by iteratively performing the following tasks :- $NL$1) interviewing clients and potential users of the system to find out what they say about the system needed$NL$2) documenting the results of these conversations,$NL$3) identifying the essential features of the required system$NL$4) producing preliminary designs (and possibly prototypes of the system)$NL$5) evaluating these initial plans with the client and potential users$NL$6) repeating the steps above until a finished design has evolved.$NL$Performing requirements analysis is a specialised skill that is outside the scope of this text but here we will focus on steps three and four above ie. given a description of a system how do we convert this into a potential OO design.$NL$While we can hope to develop preliminary design skills experience is a significant factor in this task. Producing simple and elegant designs is important if we want the software to work well and be easy to develop however identifying good designs from weaker designs is not simple and experience is a key factor.$NL$A novice chess player may know all the rules but it takes experience to learn how to choose good moves from bad moves and experience is essential to becoming a skilled player. Similarly experience is essential to becoming skilled at performing user requirements analysis and in producing good designs.
9;Computing is a constantly changing our world and our environment. In the 1960s large machines called mainframes were created to manage large volumes of data (numbers) efficiently. Bank account and payroll programs changed the way organisations worked and made parts of these organisations much more efficient. In the 1980s personal computers became common and changed the way many individuals worked. People started to own their own computers and many used word processors and spreadsheets applications (to write letters and to manage home accounts). In the 1990s email became common and the world wide web was born. These technologies revolutionised communications allowing individuals to publish information that could easily be accessed on a global scale. The ramifications of these new technologies are still not fully understood as society is adapting to opportunities of internet commerce, new social networking technologies (twitter, facebook, myspace, online gaming etc) and the challenges of internet related crime.$NL$Just as new computing technologies are changing our world so too are new techniques and ideas changing the way we develop computer systems. In the 1950s the use machine code (unsophisticated, complex and machine specific) languages were common. $NL$In the 1960s high level languages, which made programming simpler, became common. However these led to the development of large complex programs that were difficult to manage and maintain. $NL$In the 1970s the structured programming paradigm became the accepted standard for large complex computer programs. The structured programming paradigm proposed methods to logically structure the programs developed into separate smaller, more manageable components. Furthermore methods for analysing data were proposed that allowed large databases to be created that were efficient, preventing needless duplication of data and protected us against the risks associated with data becoming out of sync. However significant problems still persisted in a) understanding the systems we need to create and b) changing existing software as users requirements changed. $NL$In the 1980s ‘modular’ languages, such as Modula-2 and ADA were developed that became the precursor to modern Object Oriented languages. $NL$In the 1990s the Object Oriented paradigm and component-based software development ideas were developed and Object Oriented languages became the norm from 2000 onwards. $NL$The object oriented paradigm is based on many of the ideas developed over the previous 30 years of abstraction, encapsulation, generalisation and polymorphism and led to the development of software components where the operation of the software and the data it operates on are modelled together. Proponents of the Object Oriented software development paradigm argue that this leads to the development of software components that can be re-used in different applications thus saving significant development time and cost savings but more importantly allow better software models to be produced that make systems more maintainable and easier to understand.$NL$It should perhaps be noted that software development ideas are still evolving and new agile methods of working are being proposed and tested. Where these will lead us in 2020 and beyond remains to be seen.$NL$The structured programming paradigm proposed that programs could be developed in sensible blocks that make the program more understandable and easier to maintain.$NL$Activity 1 $NL$Assume you undertake the following activities on a daily basis. Arrange this list into a sensible order then split this list into three blocks of related activities and give each block a heading to summarise the activities carried out in that block.$NL$Get out of bed$NL$Eat breakfast$NL$Park the car$NL$Get dressed$NL$Get the car out of the garage$NL$Drive to work$NL$Find out what your boss wants you to do today$NL$Feedback to the boss on today’s results.$NL$Do what the boss wants you to do $NL$Feedback 1$NL$You should have been able to organise these into groups of related activities and give each group a title that summarises those activities.$NL$Get up :-$NL$Get out of bed$NL$Get dressed$NL$Eat breakfast$NL$Go to Work :-$NL$Get the car out of the garage$NL$Drive to work$NL$Park the car$NL$Do your job :-$NL$Find out what your boss wants you to do today$NL$Do what the boss wants you to do$NL$Feedback to the boss on today’s results.$NL$By structuring our list of instructions and considering the overall structure of the day (Get up, go to work, do your job) we can change and improve one section of the instructions without changing the other parts. For example we could improve the instructions for going to work….$NL$Listen to the local traffic and weather report$NL$Decide whether to go by bus or by car$NL$If going by car, get the car and drive to work.$NL$Else walk to the bus station and catch the bus$NL$without worrying about any potential impact this may have on ‘getting up’ or ‘doing your job’. In the same way structuring computer programs can make each part more understandable and make large programs easier to maintain. $NL$Feedback 2$NL$With an address book we would want to be able to perform the following actions :- find out details of a friend i.e. their telephone number, add an address to the address book and, of course, delete an address.$NL$We can create a simple software component to store the data in the address book (i.e. list of names etc) and the operations, things we can do with the address book (i.e add address, find telephone number etc). $NL$By creating a simple software component to store and manage addresses of friends we can reuse this in another software system i.e. it could be used by a business manager to store and find details of customers. It could also become part of a library system to be used by a librarian to store and retrieve details of the users of the library. $NL$Thus in object oriented programming we can create re-usable software components (in this case an address book).$NL$While we can focus our attention on the actual program code we are writing, whatever development methodology is adopted, it is not the creation of the code that is generally the source of most problems. Most problems arise from :-$NL$• poor maintainability: the system is hard to understand and revise when, as is inevitable, requests for change arise.$NL$• Statistics show 70% of the cost of software is not incurred during its initial development phase but is incurred during subsequent years as the software is amended to meet the ever changing needs of the organisation for which it was developed. For this reason it is essential that software engineers do everything possible to ensure that software is easy to maintain during the years after its initial creation.
9;We have seen previously how methods are identified at run time by their signature i.e. the name of the method and the list of parameters the method takes.$NL$Thus we can have two methods with the same name. Shown below are two methods that find a highest value… one finds the highest value given two integer numbers, the other finds the highest value of two double numbers.$NL$Given the following code the CLR engine will invoke the first of these methods then the second as the correct method to implement is identified by its name and the type of its parameters.$NL$Given the following definition of a Student class…$NL$We could even define a version of this method to find the highest student (alphabetically).$NL$And invoke this using ..$NL$In doing this we have created three methods that essentially do exactly the same thing only using different parameter types. This leads us to the idea that it would be nice to create just one method that would work with objects of any type.$NL$The method below is a first attempt to do this :-$NL$This method takes any two ‘Objects’ as parameters. ‘Object’ is the base class of all other classes thus this method can take any two objects of any more specific type and treat these as of the base type ‘Object’. This is polymorphism. $NL$The method above then converts these two ‘Object’s to strings and compares these strings.$NL$Thus this one method can be invoked three times using the following code….$NL$In these cases respectively the CLR engine will treat int, double, and Student objects as the most general type of thing available ‘Object’. It will then convert this object to a string and compare the strings. $NL$This will work however in many situations creating methods which take parameters of type ‘Object’ is flawed or at least very limited. $NL$Inside these methods we do not know what type of object was actually passed as a parameter and hence in the example above we do not know what type of object is actually being returned. When two students object are passed the object returned is a student but we cannot invoke Student specific methods on this object unless we first cast the object returned to a Student.$NL$Assume that we want to invoke an ‘AwardMerit() method on the ‘Student’ returned via the Highest() method. We can do this is we first cast the returned ‘Object’ onto a ‘Student’ object. E.g.$NL$However the compiler cannot be certain that the returned object is a ‘Student’ and the compiler cannot detect the potentially critical error that would occur is we invoked Highest() on two integer numbers and then tried to cast the returning integer onto an object of type ‘Student’.$NL$This leads us to the idea that we would like to be able to create a method that will take parameters of ANY type and return values that are again of ANY type but where we will define these types when we invoke these methods. Such methods do exist and they are called Generic Methods.$NL$Generic methods are methods where the parameter types are not defined until the method is invoked. Parameter types are specified each time the method is invoked and the compiler can thus still check the code is valid.$NL$In other words a generic method uses a parameterized type – a data type that is determined by a parameter. $NL$In the code below a generic method Highest() has been defined as a method that takes two objects as parameters of unspecified type :-$NL$We can use this method and each time we invoke it we define the type of object being compared. If two students are compared the compiler will know that the object being returned is also type student and will therefore know it is legal to invoke student specific methods on this object. $NL$A list of generic data types contained between angle brackets that follow the method’s name is provided. If multiple generic types are used by a method, their names are separated by commas.$NL$In the example above, the identifer T can stand for any data type. So, when T is used within the brackets in the method’s parameter list to describe data, it might be an int, double, ‘Student’ or any other data type. The only requirement is that the method must work no matter what type of object is passed to it. All objects have a ToString() method because every data type inherits the ToString() method form the ‘Object’ class, or contains its own overriding version.$NL$In the case above only one generic data type is specified. This is used to define the type of both parameters and the return value.$NL$The generic data type identifier for a generic method can be any legal C# identifier, but by convention it is usually an upper case letter. “T” is used to stand for “type”.$NL$When a generic method is defined i.e. one that works with any data type the type is specified when the method is used.$NL$Thus in the code below, while Highest() is a generic method, the compiler can see that a ‘Student’ object is being passed as a parameter and thus the object being returned must also be of type ‘Student’.$NL$Given the object being returned is of type ‘Student’ it must be OK to store this in a variable of type Student and it must be OK to invoke and Student methods on the object returned.$NL$Generic methods can therefore that work with any type data but the compiler can still check for type errors (as the type is specified each time the method is invoked).$NL$Generic classes have been created, based on the same mechanisms as generic methods and these are particularly useful because there were used by the creators of the .NET libraries to create generic collections. $NL$While we won’t be making much use of generic methods and generic classes ourselves in this book we will be making significant use of generic collections, but before using these we need a basic understanding of collections themselves.
9;The Unified Modelling Language, UML, is sometimes described as though it was a methodology. It is not!$NL$A methodology is a system of processes in order to achieve a particular outcome e.g. an organised sequence of activities in order to gather user requirements. UML does not describe the procedures a programmer should follow – hence it is not a methodology. It is, on the other hand, a precise diagramming notation that will allow program designs to be represented and discussed. As it is graphical in nature it becomes easy to visualise, understand and discuss the information presented in the diagram. However, as the diagrams represent technical information they must be precise and clear – in order for them to work - therefore there is a precise notation that must be followed.$NL$As UML is not a methodology it is left to the user to follow whatever processes they deem appropriate in order to generate the designs described by the diagrams. UML does not constrain this – it merely allows those designs to be expressed in an easy to use, but precise, graphical notation. $NL$A process will be explained in chapter 6 that will help you to generate good UML designs. Developing good designs is a skill that takes practise to this end the process is repeated in the case study (chapter 11). For now we will just concentrate on the UML notation not these processes.$NL$Classes are the basic components of any object oriented software system and UML class diagrams provide an easy way to represent these. As well as showing individual classes, in detail, class diagrams show multiple classes and how they are related to each other. Thus a class diagram shows the architecture of a system.$NL$A class consists of :- $NL$• a unique name (conventionally starting with an uppercase letter)$NL$• a list of attributes (int, double, boolean, String etc)$NL$• a list of methods$NL$This is shown in a simple box structure…$NL$For attributes and methods visibility modifiers are shown (+ for public access, – for private access). Attributes are normally kept private and methods are normally made public.$NL$Accessor methods are created to provide access to private attributes when required. Thus a public method SetTitle() can be created to change the value of a private attribute ‘title’. $NL$Thus a class Book, with String attributes of title and author, and the following methods SetTitle(), GetTitle(), SetAuthor(), GetAuthor() and ToString() would be shown as ….$NL$Note: String shown above is not a primitive data type but is itself a class. Hence it starts with a capital letter.$NL$Some programmers use words beginning in capitals to denote class names and words beginning in lowercase to represent attributes or methods (thus ToString() would be shown as toString()). This is a common convention when designing and writing programs in Java (another common OO language). However it is not a convention followed by C# programmers – where method names usually start in Uppercase. Method names can be distinguished from class names by the use of (). This in the example above.$NL$‘Book’ is a class$NL$‘title’ is an attribute and$NL$‘SetTitle()’ is a method.$NL$UML diagrams are not language specific thus a software design, communicated via UML diagrams, can be implemented in a range of OO languages. $NL$Furthermore traditional accessor methods, getters and setters, are not required in C# programs as they are replaced by ‘properties’. Properties are in effect hidden accessor methods thus the getter and setter methods shown above, GetTitle(), SetTitle() etc are not required in a C# program. In C# an attribute would be defined called ‘title’ and a property would be defined as ‘Title’. This would allow us to set the ‘title’ directly by using the associated property ‘Title =…..’. $NL$The UML diagrams shown in this book will use the naming convention common among C# programmers … for the simple reason that we will be writing sample code in C# to demonstrate the OO principles discussed here. Though initially we will show conventional assessor methods these will be replaced with properties when coding.$NL$UML allows us to suppress any information we do not wish to highlight in our diagrams – this allows us to suppress irrelevant detail and bring to the readers attention just the information we wish to focus on. Therefore the following are all valid class diagrams…$NL$Firstly with the access modifiers not shown….$NL$Secondly with the access modifiers and the data types not shown…..$NL$And finally with the attributes and methods not shown…..$NL$i.e. there is a class called ‘BankAccount’ but the details of this are not being shown.$NL$Of course virtually all C# programs will be made up of many classes and classes will relate to each other – some classes will make use of other classes. These relationships are shown by arrows. Different type of arrow indicate different relationships (including inheritance and aggregation relationships). $NL$In addition to this class diagrams can make use of keywords, notes and comments.$NL$As we will see in examples that follow, a class diagram can show the following information :-$NL$• Classes$NL$--attributes$NL$--operations$NL$--visibility$NL$• Relationships$NL$--navigability$NL$--multiplicity$NL$--dependency$NL$--aggregation $NL$--composition$NL$• Generalization / specialization$NL$--inheritance$NL$--interfaces$NL$• Keywords$NL$• Notes and Comments$NL$As UML diagrams convey precise information there is a precise syntax that should be followed.$NL$Attributes should be shown as: visibility name : type multiplicity$NL$Where visibility is one of :-$NL$--‘+’ public$NL$--‘-’ private$NL$--‘#’ protected$NL$--‘~’ package$NL$and Multiplicity is one of :-$NL$--‘n’ exactly n$NL$--‘*’ zero or more$NL$--‘m..‘n’ between m and n$NL$The following are examples of attributes correctly specified using UML :-$NL$- custRef : int [1]$NL$a private attribute custRef is a single int value$NL$this would often be shown as - custRef : int However with no multiplicity shown we cannot safely assume a multiplicity of one was intended by the author.$NL$# itemCodes : String [1..*]$NL$a protected attribute itemCodes is one or more String values$NL$validCard : boolean$NL$an attribute validCard, of unspecified visibility, has unspecified multiplicity
9;Within hierarchical classification of animals$NL$Pinky is a pig (species sus scrofa)$NL$Pinky is (also, more generally) a mammal$NL$Pinky is (also, even more generally) an animal$NL$We can specify the type of thing an organism is at different levels of detail:$NL$higher level = less specific$NL$lower level = more specific$NL$If you were asked to give someone a pig you could give them Pinky or any other pig. $NL$If you were asked to give someone a mammal you could give them Pinky, any other pig or any other mammal (e.g. any lion, or any mouse, or any cat). $NL$If you were asked to give someone an animal you could give them Pinky, any other pig, any other mammal, or any other animal (bird, fish, insect etc). $NL$The idea here is that an object in a classification hierarchy has an ‘is a’ relationship with every class from which it is descended and each classification represents a type of animal.$NL$This is true in object oriented programs as well. Every time we define a class we create a new ‘type’. Types determine compatibility between variables, parameters etc. $NL$A subclass type is a subtype of the superclass type and we can substitute a subtype wherever a ‘supertype’ is expected. Following this we can substitute objects of a subtype whenever objects of a supertype are required (as in the example above).$NL$The class diagram below shows a hierarchical relationship of types of object – or classes.$NL$In other words we can ‘substitute’ an object of any subclass where an object of a superclass is required. This is NOT true in reverse!$NL$When designing class/type hierarchies, the type mechanism allows us to place a subclass object where a superclass is specified. However this has implications for the design of subclasses – we need to make sure they are genuinely substitutable for the superclass. If a subclass object is substitutable then clearly it must implement all of the methods of the superclass – this is easy to guarantee as all of the methods defined in the superclass are inherited by the subclass. Thus while a subclass may have additional methods it must at least have all of the methods defined in the superclass and should therefore be substitutable. However what happens if a method is overridden in the subclass?$NL$When overriding methods we must ensure that they are still substitutable for the method being replaced. Therefore when overriding methods, while it is perfectly acceptable to tailor the method to the needs of the subclass a method should not be overridden with functionality which performs an inherently different operation.$NL$For example, RecNewIssue() in DiscMag overrides RecNewIssue() from Magazine but does the same basic job (“fulfils the contract”) as the inherited version with respect to updating the number of copies and the current issue. While it extends that functionality in a way specifically relevant to DiscMags by displaying a reminder to check the cover discs, essentially these two methods perform the same operation.$NL$What do we know about a ‘Publication’?$NL$Answer: It’s an object which supports (at least) the operations:$NL$void SellCopy()$NL$String ToString()$NL$and it has properties that allow us to $NL$set the price,$NL$get the number of copies$NL$set the number of copies.$NL$Inheritance guarantees that objects of any subclass of Publications provides at least these. $NL$Note that a subclass can never remove an operation inherited from its superclass(es) – this would break the guarantee. Because subclasses extend the capabilities of their superclasses, the superclass functionality can be assumed. $NL$It is quite likely that we would choose to override the ToString() method (initially defined within ‘Object’) within Publication and override it again within Magazine so that the String returned provides a better description of Publications and Magazines. However we should not override the ToString() method in order to return the price – this would be changing the functionality of the method so that the method performs an inherently different function. Doing this would break the substitutability principle.$NL$Because an instance of a subclass is an instance of its superclass we can handle subclass objects as if they were superclass objects. Furthermore because a superclass guarantees certain operations in its subclasses we can invoke those operations without caring which subclass the actual object is an instance of.$NL$This characteristic is termed ‘polymorphism’, originally meaning ‘having multiple shapes’. $NL$Thus a Publication comes in various shapes … it could be a Book, Magazine or DiscMag. We can invoke the SellCopy() method on any of these Publications irrespective of their specific details.$NL$Polymorphism is a fancy name for a common idea. Someone who knows how to drive can get into and drive most cars because they have a set of shared key characteristics – steering wheel, gear stick, pedals for clutch, brake and accelerator etc – which the driver knows how to use. There will be lots of differences between any two cars, but you can think of them as subclasses of a superclass which defines these crucial shared ‘operations’.$NL$If ‘p’ ‘is a’ Publication, it might be a Book or a Magazine or a DiscMag.$NL$Whichever it is we know that it has a SellCopy() method.$NL$So we can invoke p.SellCopy() without worrying about what exactly ‘p’ is.$NL$This can make life a lot simpler when we are manipulating objects within an inheritance hierarchy. We can create new types of Publication e.g. a Newspaper and invoke p,SellCopy() on a Newspaper without have to create any functionality within the new class – all the functionality required is already defined in Publication.$NL$Polymorphism makes it very easy to extend the functionality of our programs as we will see now and we will see this again in the case study (in Chapter 11).$NL$Huge sums of money are spent annually creating new computer programs but over the years even more is spent changing and adapting those programs to meet the changing needs of an organisation. Thus as professional software engineers we have a duty to facilitate this and help to make those programs easier to maintain and adapt. Of course the application of good programming standards, commenting and layout etc, have a part to play here but also polymorphism can help as it allows programs to be made that are easily extended.
9;Before a computer can complete useful tasks for us (e.g. check the spelling in our documents) software$NL$needs to be written and implemented on the machine it will run on. Software implementation involves$NL$the writing of program source code and preparation for execution on a particular machine. Of course$NL$before the software is written it needs to be designed and at some point it needs to be tested. There are$NL$many iterative lifecycles to support the process of design, implementation and testing that involve$NL$multiple implementation phases. Of particular concern here are the three long established approaches$NL$to getting source code to execute on a particular machine…$NL$􀁸compilation into machine-language object code$NL$􀁸direct execution of source code by ‘interpreter’ program$NL$􀁸compilation into intermediate object code which is then interpreted by run-time system$NL$Implementing Java programs involves compiling the source code (Java) into intermediate object code$NL$which is then interpreted by a run-time system called the JRE. This approach has some advantages and$NL$disadvantages and it is worth comparing these three options in order to appreciate the implications for$NL$the Java developer.$NL$The compiler translates the source code into machine code for the relevant hardware/OS combination.$NL$Strictly speaking there are two stages: compilation of program units (usually files), followed by$NL$‘linking’ when the complete executable program is put together including the separate program units$NL$and relevant library code etc.$NL$The compiled program then runs as a ‘native’ application for that platform.$NL$This is the oldest model, used by early languages like Fortran and Cobol, and many modern ones like$NL$C++. It allows fast execution speeds but requires re-compilation of the program each time the code$NL$is changed.$NL$Here the source code is not translated into machine code. Instead an interpreter reads the source code$NL$and performs the actions it specifies.$NL$We can say that the interpreter is like a ‘virtual machine’ whose machine language is the source$NL$code language.$NL$No re-compilation is required after changing the code, but the interpretation process inflicts a$NL$significant impact on execution speed.$NL$Scripting languages tend to work in this way.$NL$This model is a hybrid between the previous two.$NL$Compilation takes place to convert the source code into a more efficient intermediate representation$NL$which can be executed by a ‘run-time system’ (again a sort of ‘virtual machine’) more quickly that$NL$direct interpretation of the source code. However, the use of an intermediate code which is then$NL$executed by run-time system software allows the compilation process to be independent of the$NL$OS/hardware platform, i.e. the same intermediate code should run on different platforms so long as an$NL$appropriate run-time system is available for each platform.$NL$This approach is long-established (e.g. in Pascal from the early 1970s) and is how Java works.$NL$To run Java programs we must first generate intermediate code (called bytecode) using a compiler$NL$available as part of the Java Development Kit (JDK) – see section 8.4 below.$NL$A version of the Java Runtime Environment (JRE), which incorporates a Java Virtual machine (VM),$NL$is required to execute the bytecode and the Java library packages. Thus a JRE must be present on any$NL$machine which is to run Java programs.$NL$The Java bytecode is standard and platform independent and as JRE’s have been created for most$NL$computing devices (including PC’s, laptops, mobile devices, mobile phones, internet devices etc) this$NL$makes Java programs highly portable.$NL$Whatever mode of execution is employed, programmers can work with a variety of tools to create$NL$source code. Options include the use of simple discrete tools (e.g. editor, compiler, interpreter)$NL$invoked manually as required or alternatively the use of an Integrated Development Environment (IDE)$NL$which incorporates these implementation tools behind a seamless interface. Still more sophisticated$NL$CASE (Computer Aided Software Engineering) tools which integrate the implementation process with$NL$other phases of the development cycle – such software could take UML class diagrams and generate$NL$classes and method stubs automatically saving some of the effort required to write the Java code.$NL$When writing java programs each class (or interface) in a Java program has its own name.java file$NL$containing the source code.$NL$These are processed by the compiler to produce name.class files containing the corresponding$NL$bytecode.$NL$To actually run as an application, one of the classes must contain a main() method with the signature$NL$shown above.$NL$To develop Java programs you must first install the Java Development Kit (JDK). This was developed$NL$by Sun and is available freely from the internet via http://java.sun.com/. Prior to version 5.0 (or 1.5)$NL$this was known as the Java Software Development Kit (SDK).$NL$A Java IDE’s, e.g. Eclipse or NetBeans, sits on top of’ the JDK to add the IDE features - these may$NL$include interface development tools, code debugging tools, testing tools and refactoring tools (more on$NL$these later). When using an IDE it is easy to forget that much of the functionality is in fact part of the$NL$JDK and when the IDE is asked to compile a program it is infact just passing on this request to the$NL$JDK sitting underneath.$NL$We can use the JDK directly from the command line to compile and run Java programs though mostly$NL$it is easier to use the additional facilities offered by an IDE.$NL$Somewhat confusingly the current version of Java is known both as 6.0, 1.6 and even 1.6.0. These$NL$supposedly have subtly different meanings – don’t worry about it!$NL$There are many tools in the JDK. A description of each of these is available from$NL$http://java.sun.com/javase/downloads/ and following the links for Documentation, APIs and JDK$NL$Programmer guides.$NL$The two most important basic tools are:$NL$javac – the java compiler$NL$java – the java program launcher (that runs the VM)$NL$To compile MyProg.java we type$NL$javac MyProg.java$NL$If successful this will create MyProg.class$NL$To run Myprog (assuming it has a main() method) we type$NL$java MyProg$NL$Another, extremely useful tool, is javadoc - this uses comments in Java source code to generate$NL$automatic documentation for programs.$NL$Moving to an ‘industrial strength’ IDE is an important stepping stone in your progress as a software$NL$developer, like riding a bicycle without stabilisers for the first time. With some practice you will soon$NL$find it offers lots of helpful and time-saving facilities that you will not want to work without again…$NL$Eclipse is a flexible and extensible IDE platform. It was first developed by IBM but is now open$NL$source and can be downloaded from the Eclipse Foundation at www.eclipse.org.
10;While we focus on white-collar financial crime in this book on computer crime, we must not forget that there are a number of other types of crime that are typical for cyber crime and Internet crime as well. Typical examples are hacking, child pornography and online child grooming. In this chapter, we present the case of child grooming as computer crime. Internet use has grown considerably in the last decade. Information technology now forms a core part of the formal education system in many countries, ensuring that each new generation of Internet users is more adept than the last. Research studies in the UK suggest that the majority of young people aged 9-19 accessed the Internet at least once a day. The Internet provides the opportunity to interact with friends on social networking sites such as Myspace and Bebo and enables young people to access information in a way that previous generations would not have thought possible. The medium also allows users to post detailed personal information, which may be accessed by any site visitor and provides a platform for peer communication hitherto unknown (Davidson and Martellozzo, 2008). There is, however, increasing evidence that the Internet is used by some adults to access children and young people in order to groom them for the purposes of sexual abuse. Myspace have recently expelled 29,000 suspected sex offenders and is being sued in the United States by parents who claim that their children were contacted by sex offenders on the site and consequently abused (BBC, 2007). The Internet also plays a role in facilitating the production and distribution of indecent illegal images of children, which may encourage and complement online grooming. $NL$$NL$Recent advances in computer technology have been aiding sexual sex offenders, stalkers, child pornographers, child traffickers, and others with the intent of exploiting children (Kierkegaard, 2008: 41): Internet bulletin boards, chat rooms, private websites, and peer-to-peer networks are being used daily by pedophiles to meet unsuspecting children. Compounding the problem is the lack of direct governance by an international body, which will curb the illegal content and activity. Most countries already have laws protecting children, but what is needed is a concerted law enforcement and international legislation to combat child sex abuse. Men who target young people online for sex are pedophiles (Kierkegaard, 2008 Wolak et al., 2008). According to Dunaigre (2001), the pedophile is an emblematic figure, made into a caricature and imbued with all the fears, anxieties and apprehensions rocking our society today. Pedophile acts are - according to the World Health Organization (WHO) - sexual behavior that an adult major (16 years or over), overwhelmingly of the male sex, acts out towards prepubescent children (13 years or under). According to the WHO, there must normally be a five-year age difference between the two, except in the case of pedophilic practices at the end of adolescence where what counts is more the difference in sexual maturity. However, the definition of criminal behavior varies among countries. As will become evident from reading this article, pedophile acts in Norway are sexual behavior that a person acts out towards children of 16 years or under. There is no minimum age definition for the grooming person in Norwegian criminal law, but age difference and difference in sexual maturity is included as criteria for criminal liability. $NL$$NL$Wolak et al. (2009: 4) present two case examples of crimes by online sex offenders in the United States: • Police in West Coast state found child pornography in the possession of the 22-year-old offender. The offender, who was from a North-eastern state, confessed to befriending a 13-year-old local boy online, traveling to the West Coast, and meeting him for sex. Prior to the meeting, the offender and victim had corresponded online for about six months. The offender had sent the victim nude images via web cam and e-mail and they had called and texted each other hundreds of times. When they met for sex, the offender took graphic pictures of the encounter. The victim believed he was in love with the offender. He lived alone with his father and was struggling to fit in and come to terms with being gay. The offender possessed large quantities of child pornography that he had downloaded from the Internet. He was sentenced to 10 years in prison. • A 24-year-old man met a 14-year-old girl at a social networking site. He claimed to be 19. Their online conversation became romantic and sexual and the victim believed she was in love. They met several times for sex over a period of weeks. The offender took nude pictures of the victim and gave her alcohol and drugs. Her mother and stepfather found out and reported the crime to the police. The victim was lonely, had issues with drugs and alcohol, and problems at school and with her parents. She had posted provocative pictures of herself on her social networking site. She had met other men online and had sex with them. The offender was a suspect in another online enticement case. He was found guilty but had not been sentenced at time of the interview. $NL$$NL$According to Davidson and Martellozzo (2008: 277), Internet sex offender behavior can include: "the construction of sites to be used for the exchange of information, experiences, and indecent images of children the organization of criminal activities that seek to use children for prostitution purposes and that produce indecent images of children at a professional level the organization of criminal activities that promote sexual tourism". Child grooming is a process that commences with sexual sex offenders choosing a target area that is likely to attract children. In the physical world, this could be venues visited by children such as schools, shopping malls or playgrounds. A process of grooming then commences when offenders take a particular interest in the child and make them feel special with the intention of forming a bond. The Internet has greatly facilitated this process in the virtual world. Offenders now seek out their victims by visiting Internet relay chat (IRC) rooms from their home or Internet cafés at any time. Once a child victim is identified, the offender can invite it into a private area of the IRC to engage in private conversations on intimate personal details including the predator's sex life (Australian, 2008).
10;The risk of computer crime has become a global issue affecting almost all countries. Salifu (2008) argues that the Internet is a "double-edged sword" providing many opportunities for individuals and organizations to develop and prosper, but at the same time has brought with it new opportunities to commit crime. For example, Nigeria-related financial crime is extensive and 122 out of 138 countries at an Interpol meeting complained about Nigerian involvement in financial fraud in their countries. The most notorious type attempted daily on office workers all over the world, is the so-called advance fee fraud. The sender will seek to involve the recipient in a scheme to earn millions of dollars if the recipient pays an advance fee (Ampratwum, 2009). Computer crime is an overwhelming problem worldwide. It has brought an array of new crime activities and actors and, consequently, a series of new challenges in the fight against this new threat (Picard, 2009). Policing computer crime is a knowledge-intensive challenge indeed because of the innovative aspect of many kinds of computer crime. Cyberspace presents a challenging new frontier for criminology, police science, law enforcement and policing. Virtual reality and computer-mediated communications challenge the traditional discourse of criminology and police work, introducing new forms of deviance, crime, and social control. Since the 1990s, academics and practitioners have observed how cyberspace has emerged as a new field of criminal activity. Cyberspace is changing the nature and scope of offending and victimization. A new discipline named cyber criminology is emerging. Jaishankar (2007) defines cyber criminology as the study of causation of crimes that occur in the cyberspace and its impact in the physical space. $NL$$NL$Employees of the organization commit most computer crime, and the crime occurs inside company walls (Hagen et al., 2008: Nykodym et al, 2005). However, in our perspective of financial crime introduced in this chapter, we will define computer crime as a profit-oriented crime rather than a damage-oriented crime, thereby excluding the traditional focus of dissatisfied and frustrated employees wanting to harm their own employers. $NL$$NL$Computer crime is defined as any violations of criminal law that involve knowledge of computer technology for their perpetration, investigation, or prosecution (Laudon and Laudon, 2010). The initial role of information and communication technology was to improve the efficiency and effectiveness of organizations. However, the quest of efficiency and effectiveness serves more obscure goals as fraudsters exploit the electronic dimension for personal profits. Computer crime is an overwhelming problem that has brought an array of new crime types (Picard, 2009). Examples of computer-related crimes include sabotage, software piracy, and stealing personal data (Pickett and Pickett, 2002). In computer crime terminology, the term cracker is typically used to denote a hacker with a criminal intent. No one knows the magnitude of the computer crime problem – how many systems are invaded, how many people engage in the practice, or the total economic damage. According to Laudon and Laudon (2010), the most economically damaging kinds of computer crime are denial-of-service attacks, where customer orders might be rerouted to another supplier. Eleven men in five countries carried out one of the worst data thefts for credit card fraud ever (Laudon and Laudon, 2010: 326): In early August 2008, U.S. federal prosecutors charged 11 men in five countries, including the United States, Ukraine, and China, with stealing more than 41 million credit and debit card numbers. This is now the biggest known theft of credit card numbers in history. The thieves focused on major retail chains such as OfficeMax, Barnes & Noble, BJ’s Wholesale Club, the Sports Authority, and T.J. Marxx. The thieves drove around and scanned the wireless networks of these retailers to identify network vulnerabilities and then installed sniffer programs obtained from overseas collaborators. The sniffer programs tapped into the retailers’ networks for processing credit cards, intercepting customers’ debit and credit card numbers and PINs (personal identification numbers). The thieves then sent that information to computers in the Ukraine, Latvia, and the United States. They sold the credit card numbers online and imprinted other stolen numbers on the magnetic stripes of blank cards so they could withdraw thousands of dollars from ATM machines. Albert Gonzales of Miami was identified as a principal organizer of the ring. $NL$$NL$The conspirators began their largest theft in July 2005, when they identified a vulnerable network at a Marshall’s department store in Miami and used it to install a sniffer program on the computers of the chain’s parent company, TJX. They were able to access the central TJX database, which stored customer transactions for T.J. Marxx, Marshalls, HomeGoods, and A.J. Wright stores in the United States and Puerto Rico, and for Winners and HomeSense stores in Canada. Fifteen months later, TJX reported that the intruders had stolen records with up to 45 million credit and debit card numbers. TJX was still using the old Wired Equivalent Privacy (WEP) encryption system, which is relatively easy for hackers to crack. Other companies had switched to the more secure Wi-Fi Protected Access (WPA) standard with more complex encryption, but TJX did not make the change. An auditor later found that TJX had also neglected to install firewalls and data encryption on many of the computers using the wireless network, and did not properly install another layer of security software it had purchased. TJX acknowledged in a Securities and Exchange Commission filing that it transmitted credit card data to banks without encryption, violating credit card company guidelines. Computer crime, often used synonymous with cyber crime, refers to any crime that involves a computer and a network, where the computer has played a part in the commission of a crime. Internet crime, as the third crime label, refers to criminal exploitation of the Internet. In our perspective of profit-oriented crime, crime is facilitated by computer networks or devices, where the primary target is not computer networks and devices, but rather independent of the computer network or device. $NL$$NL$Cyber crime is a term used for attacks on the cyber security infrastructure of business organizations that can have several goals. One goal pursued by criminals is to gain unauthorized access to the target’s sensitive information. Most businesses are vitally dependent on their proprietary information, including new product information, employment records, price lists and sales figures. According to Gallaher et al. (2008), an attacker may derive direct economic benefits from gaining access to and/or selling such information, or may inflict damage on an organization by impacting upon it.
10;Fake websites have become increasingly pervasive and trustworthy in their appearance, generating billions of dollars in fraudulent revenue at the expense of unsuspecting Internet users. Abbasi et al. (2010) found that the growth in profitable fake websites is attributable to several factors, including their authentic appearance, a lack of user awareness regarding them, and the ability of fraudsters to undermine many existing mechanisms for protecting against them. The design and appearance of these websites makes it difficult for users to manually identify them as fake. Distinctions can be made between spoof sites and concocted sites. A spoof site is an imitation of an existing commercial website such as eBay or PayPal. A concocted site is a deceptive website attempting to create the impression of a legitimate, unique and trustworthy entity. Detecting fake websites is difficult. There is a need for both fraud cues as well as problem-specific knowledge. Fraud cues are important design elements of fake websites that may serve as indicators of their lack of authenticity. First, fake websites often use automatic content generation techniques to mass-produce fake web pages. Next, fraud cues include information, navigation, and visual design. Information in terms of web page text often contains fraud cues stemming from information design elements. Navigation in terms of linkage information and URL names for a website can provide relevant fraud cues relating to navigation design characteristics. For example, it is argued that 70 percent of ".biz" domain pages are fake sites. Fake websites frequently use images from existing legitimate or prior fake websites. For example spoof sites copy company logos from the websites they are mimicking. The fact that it is copied can be detected in the system (Abbasi et al., 2010). In addition to fraud cues, there is a need for problem-specific knowledge. Problem-specific knowledge regarding the unique properties of fake websites includes stylistic similarities and content duplication (Abbasi et al., 2010). Abbasi et al. (2010) developed a prototype system for fake website detection. The system is based on statistical learning theory. Statistical learning theory is a computational learning theory that attempts to explain the learning process from a statistical point of view. The researchers conducted a series of experiments, comparing the prototype system against several existing fake website detection systems on a test sample encompassing 900 websites. The results indicate that systems grounded in statistical learning theory can more accurately detect various categories of fake websites by utilizing richer sets of fraud cues in combination with problem-specific knowledge. A variation of fake websites is fraudulent email solicitation where the sender of an email claims an association with known and reputable corporations or organizational entities. For example, one email from the "Microsoft/AOL Award Team" notified its winners of a sweepstake by stating, "The prestigious Microsoft and AOL has set out and successfully organized a Sweepstakes marking the end of year anniversary we rolled out over 100,000.000.00 for our new year Anniversary Draw" (Nhan et al., 2009). The email proceeded to ask for the potential victim's personal information. $NL$$NL$Nhan et al. (2009) examined 476 fraudulent email solicitations, and found that the three most frequently alleged organizational associations were Microsoft, America Online, and PayPal. Fraudsters also attempt to establish trust through associating with credit-issuing financial corporations and authoritative organizations and groups. $NL$$NL$Money laundering is an important activity for most criminal activity (Abramova, 2007 Council of Europe, 2007 Elvins, 2003). Money laundering means the securing of the proceeds of a criminal act. The proceeds must be integrated into the legal economy before the perpetrators can use it. The purpose of laundering is to make it appear as if the proceeds were acquired legally, as well as disguises its illegal origins (Financial Intelligence Unit, 2008). Money laundering takes place within all types of profit-motivated crime, such as embezzlement, fraud, misappropriation, corruption, robbery, distribution of narcotic drugs and trafficking in human beings (Økokrim, 2008). Money laundering has often been characterized as a three-stage process that requires (1) moving the funds from direct association with the crime, (2) disguising the trail to foil pursuit, and (3) making them available to the criminal once again with their occupational and geographic origins hidden from view. The first stage is the most risky one for the criminals, since money from crime is introduced into the financial system. Stage 1 is often called the placement stage. Stage 2 is often called the layering stage, in which money is moved in order to disguise or remove direct links to the offence committed. The money may be channeled through several transactions, which could involve a number of accounts, financial institutions, companies and funs as well as the use of professionals such as lawyers, brokers and consultants as intermediaries. Stage 3 is often called the integration stage, where a legitimate basis for asset origin has been created. The money is made available to the criminal and can be used freely for private consumption, luxury purchases, real estate investment or investment in legal businesses. Money laundering has also been described as a five-stage process: placement, layering, integration, justification, and embedding (Stedje, 2004). It has also been suggested that money laundering falls outside of the category of financial crime. Since money-laundering activities may use the same financial system that is used for the perpetration of core financial crime, its overlap with the latter is apparent (Stedje, 2004). According to Joyce (2005), criminal money is frequently removed from the country in which the crime occurred to be cycled through the international payment system to obscure any audit trail. The third stage of money laundering is done in different ways. For example, a credit card might be issued by offshore banks, casino 'winning' can be cashed out, capital gains on option and stock trading might occur, and real estate sale might cause profit. $NL$$NL$The proceeds of criminal acts could be generated from organized crime such as drug trafficking, people smuggling, people trafficking, proceeds from robberies or money acquired by embezzlement, tax evasion, fraud, abuse of company structures, insider trading or corruption. The Financial Intelligence Unit (2008) in Norway argues that most criminal acts are motivated by profit. When crime generates significant proceeds, the perpetrators need to find a way to control the assets without attracting attention to them selves or the offence committed. Thus, the money laundering process is decisive in order to enjoy the proceeds without arousing suspicion.
10;When a business enterprise is the potential victim of computer crime, there are a number of measures that can be implemented to protect the business. In the survey by Hagen et al. (2008), they addressed both breath and depth in defense strategies. Depth is concerned with technological as well as organizational measures, while depth is concerned with dimensions of prevention, emergency preparedness and detection. The survey addressed the use of a broad range of technical security measures relating to access control and protection of data. Technical security measures include prevention (password, physical zones, biometric authentication, and software update), emergency (backup), and detection (intrusion detection and antivirus software). Organizational security measures include prevention (access rights and user guidelines), emergency (management plans), detection (log reviews), and incident response (management reports). The survey showed that the use of personal passwords is widespread among all enterprises, even the smallest ones (Hagen et al., 2008: 364): The trend is that the use of a variety of access control mechanisms increases with enterprise size. There is also a clear tendency that large enterprises implement more and a wider range of emergency preparedness and detection measures. The findings show that small enterprises should strengthen their access control and data protection measures, in addition to security routines. Hagen et al. (2008) found it surprising that large enterprises did not perform better than small enterprises when it comes to awareness raising and education of users as organizational security measures. $NL$$NL$Profiling of criminals is based on the idea that an individual committing crime in cyberspace using a computer can fit a certain outline or profile. A profile consists of offender characteristics that represent assumptions of the offender’s personality and behavioral appearance. Characteristics can include physical build, offender sex, work ethic, mode of transportation, criminal history, skill level, race, marital status, passiveness/aggressiveness, medical history, and offender residence in relation to the crime (Nykodym et al., 2005). Nykodym et al. (2005: 413) make distinctions between four main categories of cyber crime: espionage, theft, sabotage, and personal abuse of the organizational network: Unlike saboteurs and spies, the thief is guided only by mercantile motives for his own gain. The only goal in front of the cyber thief is to steal valuable information from an organization and use it or sell it afterwards for money. In terms of criminal profiling, Nykodym et al. (2005) found that there is a strong pattern in the age of these cyber robbers. If the crime is for less than one hundred thousand dollars, then most likely the attacker is young 20-25 years old, male or female, still in the low hierarchy of the organization. If the crime involves more money, then the committer is probably an older male from a management level in the organization. His crime is not driven by hate or revenge but by greed and hunger for money. $NL$$NL$Computer crime is defined as financial crime in this book. White-collar criminals commit financial crime. Characteristics of white-collar criminals include: • Wealthy yet greedy person • Highly educated yet practical person • Socially connected yet anti-social person • Talks ethics yet acts immoral • Employed by and in a legitimate organization • A person of respectability with high social status • Member of the privileged socioeconomic class • Commit crime within the occupation based on competence • On the slippery slope from legitimate to illegitimate behavior • Often charismatic, convincing and socially skilled • So desperate to succeed that they are willing to use criminal means • Sometimes excited about the thrill of not being uncovered • Often in a position where the police is reluctant to start investigation • Applies resources to hide tracks and crime outcome • Behaves in court in a manner creating sympathy and understanding $NL$$NL$These kinds of characteristics are organized according to criteria in criminal profiling. For example, some of them are individual factors that are grounded in psychology, while others are environmental factors grounded in sociology. In terms of psychological factors, criminal profiling may ask question such as: • What kind of personality types become more easily white-collar criminals? • What are their typical background, life style and development? • What are their values, ideas and ambitions? In terms of sociological factors, criminal profiling may ask questions such as: • How do white-collar criminals look at society and their own role in society? • How do they perceive laws, and what do they consider to be crime and criminals? • How do they participate in networks, and what is associated with status and power? Not all computer criminals are white-collar criminals, but most of them are committing crime for financial gain. Cyber offenders are likely to share a broader range of social characteristics, and the cases of hacking and other Internet-related offences that have been reported in the media would suggest they are likely to be young, clever and fairly lonely individuals who are of middle-class origin, often without prior criminal records, often processing expert knowledge and often motivated by a variety of financial and non-financial goals. Some degree of technical competence is required to commit many computer-related types of crime (Salifu, 2008). $NL$$NL$Some theorists believe that crime can be reduced through the use of deterrents. The goal of deterrence, crime prevention, is based on the assumption that criminals or potential criminals will think carefully before committing a crime if the likelihood of getting caught and/or the fear of swift and severe punishment are present. Based on such belief, general deterrence theory holds that crime can be thwarted by the threat of punishment, while special deterrence theory holds that penalties for criminal acts should be sufficiently severe that convicted criminals will never repeat their acts (Lyman and Potter, 2007). Threat is an external stimulus that exists whether or not an individual perceives it (Johnson and Warkentin, 2010). If an individual perceives the threat, then is has deterrent potential. Deterrence theory postulates that people commit such crimes on the basis of rational calculations about perceived personal benefits, and that the threat of legal sanctions will deter people for fear of punishment (Yusuf and Babalola, 2009). In more recent years when executives have been seen arrested and handcuffed for the purposes of public humiliation, it sets in motion a deterrence model of crime prevention or at the very least, a shaming policy. The purpose of these public arrests are often symbolic and say more about the regulatory agencies need to appear to be legitimately prosecuting corporate wrongdoers. As such, with regulation so closely tied to the political climate, there has been no consistency in the prosecution of corporate criminals, as compared with drug war policies of the past couple of decades (Hansen, 2009).
11;Programming, in any language, involves creating named entities within the machine and manipulating them – using their values to calculate the value for a new entity, changing the values of existing entities, and so forth. Some languages recognize many different kinds of entity, and require the programmer to be very explicit and meticulous about “declaring” what entities he will use and what kind each one will be before anything is actually done with them.4 In C, for instance, if a variable represents a number, one must say what kind of number – whether an integer (a whole number) or a “floating-point number” (what in everyday life we call a decimal), and if the latter then to what degree of precision it is recorded. (Mathematically, a decimal may have any number of digits after the decimal point, but computers have to use approximations which round numbers off after some specific number of digits.) Perl is very free and easy about these things. It recognizes essentially just three types of entity: individual items, and two kinds of sets of items – arrays, and hashes. Individual entities are called scalars (for mathematical reasons which we can afford to ignore here – just think of “scalar” as Perl-ese for an individual data item) a scalar can have any kind of value – it can be a whole number, a decimal, a single character, a string of characters (for instance, an English word or sentence) … We have already seen that variable names representing scalars (the only variables we shall be considering for the time being) begin with the $ symbol for arrays and hashes, which we shall discuss in chapters 12 and 17, the corresponding symbols are @ and % respectively. $NL$$NL$Furthermore, Perl does not require us to declare entity names before using them. In the mini-program (1), the scalars $a and $b came into existence when they were assigned values we gave no prior notice that these variable names were going to be used. In program (1), the variable $b ended up with the value 4. But, if we had added a further line: $NL$$NL$then $b would have ceased to stand for a number and begun to stand for a character-string – both are scalars, so Perl is perfectly willing to switch between these different kinds of value. That does not mean that it is a good idea to do this in practice as a programmer you will need to bear in mind what your different variable names are intended to represent, which might be hard to do if some of them switch between numerical and alphabetic values. But the fact that one can do this makes the point that Perl does not force us to be finicky about housekeeping details. Indeed, it is even legal to use a variable’s value before we have given it a value. If line 1.2 of (1) were changed to $b = $a + $c, then $b would be given the sum of 2 plus the previously-unmentioned scalar $c. Because $c has not been given a value by the programmer, its value will be taken as zero (so $b will end up with the value 2). Relying on Perl to initialize our variables in this way is definitely a bad idea – even if we need a particular variable to have the initial value zero, it is much less confusing in the long run to get into the habit of always saying so explicitly. But Perl will not force us to give our variables values before we use them. Because this free-and-easy programming ethos makes it tempting to fall into bad habits, Perl gives us a way of reminding ourselves to avoid them. We ran program (1) with the command: $NL$$NL$The perl command can be modified by various options beginning with hyphens, one of which is -w for “give warnings”. If we ran the program using the command: $NL$$NL$then, when Perl encounters the line $b = $a + $c in which $c is used without having been assigned a value, it will obey the instruction but will also print out a warning: $NL$$NL$If a skilled programmer gets that warning, it is very likely to be because he thinks he has given $c a value but in fact has omitted to do so. And perl -w gives other warnings about things in our code which, while legal, might well be symptoms of programming errors. It is a good idea routinely to use perl -w to run your programs, and to modify the programs in response to warning messages until the warnings no longer appear – even if the programs seem to be giving the right results. $NL$$NL$In program (1) we saw the operator +, which as you would expect takes a pair of numerical values and gives their sum. Likewise - is used as a minus sign. Some further operators (not a complete list, but the ones you are most likely to need) include: * multiplication / division ** exponentiation: 2 ** 3 means 23, i.e. eight These operators apply to numerical values, but others apply to character-strings. Notably, the full stop . represents concatenation (making one string out of two): $NL$$NL$(Beware of possible confusion here. Some programming languages make the plus sign do double duty, to represent concatenation of strings as well as addition of numbers, but in Perl the plus sign is used only for numerical values.) Another string operator is x (the letter x), which is used to concatenate a string with itself a given number of times: "a" x 6 is equivalent to "aaaaaa", "pom" x 3 is equivalent to "pompompom". (And "pom" x 0 would yield the empty string – the length-zero string containing no characters – which is more straightforwardly specified as "".) Note, by the way, that for Perl a single character is just a string of length one – there is no difference, as there is for instance in C, between "a" and 'a', these are equivalent ways of representing the length-one string containing just the character a. However, single and double quotation marks are not always equivalent. Perl uses backslash as an escape character to create codes for string elements which would be awkward to type: for instance, \n represents a newline character, and \t a tab. Between double quotation marks these sequences are interpreted as codes:
11;Sometimes we want to repeat an action, perhaps with variations. One way to do this is with the word for. Suppose we want to print out a hundred lines containing the messages: $NL$$NL$Here is a code snippet which does that: $NL$$NL$The brackets following for contain: a variable created for the purpose of this for loop and given an initial value a condition for repeating the loop and an action to be executed after each pass. The variable $i begins with the value 1, ++$i increments it by one on each pass, and the instruction within the curly brackets is executed for each value of $i until $i reaches 101, when control moves on to whatever follows the closing curly bracket. We saw earlier that, within double quotation marks, a symbol like \n is translated into what it stands for (newline, in this case), rather than being taken literally as the two characters \ followed by n. Similarly, a variable name such as $i is translated into its current value the lines displayed by the code above read e.g. Next number is 3, not Next number is $i. If you really wanted the latter, you would need to “escape” the dollar sign: $NL$$NL$The little examples in earlier chapters often ended with statements such as $NL$$NL$In practice, it would usually be far preferable to write $NL$$NL$so that the result appears on a line of its own, rather than jammed together with the next system prompt. Within the output of the above code snippet, 1 is not a “next” number but the first number. So we might want the message on the first line to read differently. By now, we know various ways to achieve that. Here are two – a straightforward, plodding way, and a more concise way: $NL$$NL$or (quicker to type, though less clear when you come back to it weeks later): $NL$$NL$Another way to set up a repeating loop is the while construction. Here is another code snippet which achieves the same as the two we have just looked at: $NL$$NL$Here, $i is incremented within the loop body, and control falls out of the loop after the pass in which $i begins with the value 99. The while condition reads $i < 100, not $i <= 100: within the curly brackets, $i is incremented before its value is displayed, so if <= had been used in the while line, the lines displayed would have reached 101. The while construction is often used for reading input lines in from a text file, so the next chapter will show us how that is done. $NL$$NL$In general, a file you want to get data into your program from will not necessarily be in the same directory as the program itself it may have to be located by a pathname which could be long and complicated. The structure of pathnames differs between operating systems if you are working in a Unix environment, for instance, the pathname might be something like: $NL$$NL$Whatever pathnames look like in your computing environment, to read data into a Perl program you have to begin by defining a convenient handle which the program will use to stand for that pathname. For instance, if your program will be using only one input file, you might choose the handle INFILE (it is usual to use capitals for filehandles). The code: $NL$$NL$says that, from now until we hit a line close(INFILE), any reference to INFILE in the program will be reading in data from the annualRecords file specified in the pathname. $NL$$NL$Having “opened” a file for input, we use the symbol <> to actually read a line in. Thus: $NL$$NL$will read in a line from the annualRecords file and assign that string of characters as the value of $a. A line from a multi-line file will terminate in one or more line-end characters, and the identity of these may depend on the system which created the file (different operating systems use different line-end characters). Commonly, before doing anything else with the line we will want to convert it into an ordinary string by removing the line-end characters, and the built-in function chomp() does that. This is an example of a function whose main purpose is to change its argument rather than to return a value chomp() does in fact return a value, namely the number of line-end characters found and removed, but programs will often ignore that value – they will say e.g. chomp($line), rather than saying e.g. $n = chomp($line), with follow-up code using the value of $n. (If no filehandle is specified, $a = <> will read in from the keyboard – the program will wait for the user to type a sequence of characters ending in a newline, and will assign that sequence to $a.11) Assuming that we are reading data from a file rather than from the keyboard, what we often want to do is to read in the whole of the input file, line by line, doing something or other with each successive line. An easy way to achieve that is like this: $NL$$NL$The word while tests for the truth of a condition in this case, it tests whether the assignment statement, and hence the expression <INFILE>, is true or false. So long as lines are being read in from the input file, <INFILE> counts as “true”, but when the file is exhausted <INFILE> will give the value “false”. Hence while ($a = <INFILE>) assigns each line of the input file in turn to $a, and ceases reading when there is nothing more to read. (It is a good idea then to include an explicit close(INFILE) statement, though that is not strictly necessary.) Our open … statement assumed that the annualRecords file was waiting ready to be opened at the place identified by the pathname. But, of course, that kind of assumption is liable to be confounded! Even supposing we copied the pathname accurately when we typed out the program, if that was a while ago then perhaps the annualRecords file has subsequently been moved, or even deleted. In practice it is virtually mandatory, whenever we try to open a file, to provide for the possibility that it does not get opened – normally, by using a die statement, which causes the program to terminate after printing a message about the problem encountered. A good way to code the open statement will be:
11;Since its creation in 1987 Perl has become one of the most widely used programming languages. One measure of this is the frequency with which various languages are mentioned in job adverts. The site www.indeed.com monitors trends: in 2010 it shows that the only languages receiving more mentions on job sites are C and its offshoots C++ and C#, Java, and JavaScript. Perl is a general-purpose programming language, but it has outstanding strengths in processing text files: often one can easily achieve in a line or two of Perl code some text-processing task that might take half a page of C or Java. In consequence, Perl is heavily used for computer-centre system admin, and for Web development – Web pages are HTML text files. Another factor in the popularity of Perl is simply that many programmers find it fun to work with. Compared with Perl, other leading languages can feel worthy but tedious. Perl is a language in which it is easy to get started, but – because it offers handy ways to do very many different things – it takes a long time before anyone finishes learning Perl (if they do ever finish). One standard reference, Steven Holzner’s Perl Black Book (second edn, Paraglyph Press, 2001) is about 1300 dense pages long. So, for the beginner, it is important to focus on the core of the language, and avoid being distracted by all the other features which are there, but are not essential in the early stages. This book helps the reader to do that. It covers everything he or she needs to know in order to write successful Perl programs and grow in confidence with the language, while shielding him or her from confusing inessentials.1 Later chapters contain pointers towards various topics which have deliberately been omitted here. When the core of the language has been thoroughly mastered, that will be soon enough to begin broadening one’s knowledge. Many productive Perl programmers have gaps in their awareness of the full range of language features. The book is intended for beginners: readers who are new to Perl, and probably new to computer programming. The book takes care to spell out concepts that would be very familiar to anyone who already has experience of programming in some other language. However, there will be readers who use this book to begin learning Perl, but who have worked with another language in the past. For the benefit of that group, I include occasional brief passages drawing attention to features of Perl that could be confusing to someone with a background in another language. Programming neophytes can skim over those passages. $NL$$NL$The reader I had in mind as I was writing this book was a reader much like myself: someone who is not particularly interested in the fine points of programming languages for their own sake, but who wants to use a programming language because he has work he wants to get done, and programming is a necessary step towards doing it. As it happens, I am a linguist by training, and much of my own working life is spent studying patterns in the way the English language is used in everyday talk. For this I need to write software to analyse files of transcribed tape-recordings, and Perl is a very suitable language to use for this. Often I am well aware that the program I have written is not the most elegant possible solution to some task at hand, but so long as it works correctly I really don’t care. If some geeky type offered to show me how I could eliminate several lines of code, or make my program run twice as fast, by exploiting some little-known feature of the language which would yield a program delivering exactly the same results, I would not be very interested. Too many computing books are written by geeks who lose sight of the fact that, for the rest of us, computers are tools to get work done rather than ends in themselves. Making programs short is good if it makes them easier to grasp and hence easier to get right but if brevity is achieved at the cost of obscurity, it is bad. As for speed: computer programs run so fast that, for most of us, speeding them up further would be pointless. (For every second of time my programs take to run, I probably spend a day thinking about the results they produce.) That does not mean that, in writing this book, I would have been justified in focusing only on those particular elements of Perl which happen to be useful in my own work and ignoring the rest – certainly not. Readers will have their own tasks for which they want to write software, which will often be very different from my tasks and will sometimes make heavy use of aspects of Perl that I rarely exploit. I aim to cover those aspects, as well as the ones which I use frequently. But it does mean that the book is oriented towards Perl programming as a practical tool – rather than as a labyrinth of fascinating intellectual arcana. If, after working through this book, you decide to make serious use of Perl, sooner or later you will need to consult some larger-scale Perl book – one organized more as a reference manual than a teaching introduction. This short book cannot pretend to cover the reference function, but there is a wide choice of books which do. (And of course there are plenty of online reference sources.) Many Perl users will not need to go all the way to Steven Holzner’s 1300-pager quoted above. The manual which I use constantly is a shorter one by the same author, Perl Core Language Little Black Book (second edn, Paraglyph Press, 2004) – I find Holzner’s approach particularly well suited to my own style of learning, but readers whose learning styles differ might find that other titles suit them better. Because the present book deliberately limits the aspects of Perl which it covers, it is important that readers should not fall into the trap of thinking “Doesn’t Perl have a such-and-such function, then? – that sounds like an awkward gap to have to work round”. Whatever such-and-such may be, very likely Perl has got it, but it is one of the things which this book has chosen not to cover.
11;For the purposes of this textbook, I shall assume that you have access to a computer system on which Perl is available, and that you know how to log on to the system and get to a point where the system is displaying a prompt and inviting you to enter a command. Perl is free, and versions are available for all the usual operating systems, so if you are working in a multi-user environment such as a university computer centre then Perl is almost sure to be on your system already. (It would take us too far out of our way to go through the details of installing Perl on a home computer which does not already have it though, if the home computer is a Mac running OS X, it will already have Perl – available from the Terminal utility under Applications  Utilities.) Assuming, then, that you have access to Perl, let us get started by creating and running a very simple program.2 Adding two and two is perhaps as simple as it gets. This could be a very short Perl program indeed, but I’ll offer a slightly longer one which illustrates some basics of the language. First, create a file with the following contents. Use a text editor to create it, not a word-processing application such as Word – files created via WP apps contain a lot of extra, hidden material apart from the wording typed by the user and displayed on the screen, but we need a file containing just the characters shown below and no others. $NL$$NL$Save it under some suitable name – twoandtwo.pl is as good a name as any. The .pl extension is optional – Perl itself does not care about the format of filenames, and it would respond to the program just the same if you called it simply twoandtwo – but some operating systems want to see filename extensions in some circumstances, so it is probably sensible to get in the habit of including .pl in the names of your Perl programs. Your twoandtwo.pl file will contain just what is shown above. But later in this book, when we look at more extended examples of Perl code I shall give them a label in brackets and number the lines, like this: $NL$$NL$These labels will be purely for convenience in discussing the code, for instance I shall write “line 1.3” to identify the line print $b. The labels are not part of what you will type to create a program. However, when your programs grow longer you may find it helpful to create them using an editor which shows line-numbers the error messages generated by the Perl interpreter will use line numbers to identify places where it finds problems. $NL$$NL$In (1), the symbols $a and $b are variables – names for pigeonholes containing values (in this case, numbers). Line 1.1 means “assign the value 2 to the variable $a”. Line 1.2 means “assign the result of adding the value of $a to itself to the variable $b”. Line 1.3 means “display the value of $b”. Note that each instruction (the usual word is statement) ends in a semicolon. To run the program, enter the command $NL$$NL$to which the system will respond (I’ll show system responses in italics) with $NL$$NL$Actually, if your system prompt is, say, %, what you see will be $NL$$NL$– since nothing in the twoandtwo.pl program has told the system to output a newline after displaying the result and before displaying the next prompt. For that matter, nothing in our little program has told the system how much precision to include in displaying the answer rather than responding with 4, some systems might respond with 4.00000000000000 (which is a more precise way of saying the same thing). In due course we shall see how to include extra material in a program to deal with issues like these. For now, the point is that the job in hand has been correctly done. $NL$$NL$If you have typed the code exactly as shown and Perl does not respond correctly (or at all) when you try running it, various system-dependent problems may be to blame. I assume that, where you are working, there will be someone responsible for telling you what is needed to run Perl on your local system. But meanwhile, I can offer two suggestions. It may be that your program needs to tell the system where the Perl interpreter is located (this is likely if you are seeing an error message suggesting that the command perl is not recognized). In that case it is worth trying the following. Include as the first line of your program this “magic line”:3 $NL$$NL$This will not be the right “magic line” for every system, but for many systems it will be. Secondly, if Perl appears to run without generating error messages, but outputs no result, or outputs material suggesting that it stopped reading your program before the end, it may be that your editor is supplying the wrong newline symbols – so that the sequence of lines looks to the system like one long line. That will often lead to problems for instance, if the first line of your program is the above “magic line”, but Perl sees your whole program as one long line, then nothing will happen when you run it, because the Perl interpreter will only begin to operate on the line following the “magic line”. Set your editor to use Unix (decimal 10) newlines. If neither of these solutions works, then, sorry, you really will need to find that computer-support staff member to tell you how to run Perl on the particular system you are working at! $NL$$NL$Let’s now go back to the contents of program (1). One point which may have surprised you about our first program is the dollar signs in the variable names $a and $b. Why not simply name our variables a and b? In many programming languages, these latter names would be fine, but in Perl they are not. One of the rules of Perl is that any variable name must begin with a special character identifying what kind of entity it is, and for individual variables – names for single separate pigeonholes, as opposed to names for whole sets of pigeonholes – the identifying character is a dollar sign.
11;We have seen the word if used to control which instruction is executed next. Commonly, we want to do one thing in one case and another thing in a different case. An if can be followed by an elsif (or more than one elsif), with an else at the end to catch any remaining possibilities: $NL$$NL$When any one of the tests is passed, the remaining tests are ignored if $price is 200, then since 200  100 Perl will print It's expensive, and the message in 4.7 will not be printed even though it is also true that 200 > 0. Curly brackets are used to keep together the block of code to be executed if a test is passed. Notice that (unlike in some programming languages) even if the block contains just a single line of code, that line must still have curly brackets round it. The last statement before the } does not actually have to end in a semicolon, but it is sensible to include one anyway. We might want to modify our code by adding further statements, in which case it would be easy to overlook the need to add a missing semicolon. $NL$$NL$Not everyone sets out the curly brackets on separate lines, as I did in (4) above. Within reason, Perl does not care where in a program we put whitespace (spaces, tabs, and newline characters). Obviously we cannot put a space in the middle of a number – 56237 cannot be written 56 237, or Perl would have no way to tell that it was all one number 8 – and likewise putting a space in the middle of a string within quotation marks turns it into a different string. But we can set the program out on the page however we please: around the basic elements such as numbers, strings, variable names, and brackets of different types, Perl will ignore extra whitespace. Perl will even supply implied spacing in many cases where elements are run together – thus ++ $a can alternatively be written ++$a. Because Perl does not enforce layout conventions (as some languages do), you need to choose some system and use it consistently – so that you can grasp the overall structure of your program listings at a glance. The main question is about how to indent blocks different people use different conventions. First, you need to decide how much space you are going to use for one level of indentation (common choices are one tab, or two spaces). But then, where exactly should the indents go? Perl manuals often put the opening curly bracket on the line which introduces it, indent the contents of the block, and then place the closing curly bracket level with the beginning of that first line: $NL$$NL$This takes fewer lines than other conventions, but it is not particularly easy to read, and it is perhaps illogical in placing the pair of brackets at unrelated positions. Alternatively, one can give both curly brackets lines of their own – in which case they either both line up under the start of the introducing line, or are both indented to align with their contents: $NL$$NL$Whichever convention you choose, if you apply it consistently you can catch and correct programming errors as you type. You may have a block which is indented within a block that is itself indented within a top-level block. When you type what you thought was the final }, if it doesn’t align properly with the item which it ought to line up with in the first line, then something has gone wrong – perhaps one of your opening brackets has not been given a closing partner? $NL$$NL$As for which of the three styles you choose, that is entirely up to you. According to Thomas Plum, a survey of programmers working with the similar language C found a slight majority favouring the last of the three conventions.9 That is the style used in this book. Indenting consistently also has an advantage when, inevitably, one’s program as first written turns out not to run correctly. A common debugging technique is to insert instructions to print out the values of particular variables at key points, so that one can check whether their values are as expected. Once the bugs are found and eliminated, we naturally want to eliminate these diagnostic lines too – we don’t want our program spewing out a lot of irrelevancies when it is running correctly. My practice is to write diagnostic lines unindented, so that they stand out visually in the middle of an indented block, making them easy to locate and delete. $NL$$NL$The reason to adopt a consistent style for program layout is to make it easier for a human programmer to understand what is going on within a sea of program code – the computer itself does not care about the layout. Another aid to human understanding is comments: explanatory notes written by the programmer to himself (or to those who come after him and have to maintain his code) which the machine ignores. In Perl, comments begin with the hash character. A comment can be: $NL$$NL$or it can be added to a line to the right of code intended for the computer: $NL$$NL$Either way, everything from the hash symbol to the end of the line is ignored by the machine. $NL$$NL$Earlier, we saw that Perl has various “operators” represented by mathematical-type symbols. Sometimes these are the same symbols used in familiar school maths, such as + for addition and - for subtraction sometimes they are slightly different symbols adapted to the constraints of computer keyboards, such as * for multiplication and ** for raising to a power and sometimes the symbols represent operations that we do not usually come across in maths lessons, e.g. “.” for concatenation. $NL$$NL$Perl has many more built-in functions that could conveniently be represented by special symbols, though.10 Most are represented by alphabetic codes. For instance, taking the square root of a number is a standard arithmetic operation, but the usual mathematical symbol, √, is nothing like any character in the ASCII character-set, so instead Perl represents it as sqrt.
11;Readers will appreciate that the concept of property is crucial for business. A firm needs to know$NL$what it owns (and can therefore use freely, and/or charge others who want to use it), and what$NL$belongs to others (so that if it needs to use those things it must do deals with their respective$NL$owners). Business looks to law to define property rights and enable them to be enforced.$NL$Before the IT revolution, the things over which firms needed to assert ownership were usually$NL$tangible things – goods, land, and so forth. The law of “intellectual property”, under which for$NL$instance a company might own a patent on a newly-devised industrial process, was a fairly$NL$obscure legal backwater. Information technology has changed this, by hugely raising the profile$NL$of intangibles. Ever since the Industrial Revolution, the economies of nations like Britain and the$NL$USA had been dominated by manufacturing. But by the late 1980s, the share of GDP (gross$NL$domestic product) attributable to manufacturing fell below half in both nations, and it has$NL$continued to fall – outweighed partly by growth in services, but also by growth in trading of$NL$intangibles.$NL$By now, intangibles form a large proportion of the assets of a typical firm, as measured by the prices$NL$which the market sets on them. Gordon Brown, then Chancellor of the Exchequer, said in 2006:$NL$Twenty-five years ago the market value of our top companies was no more than the$NL$value of just their physical assets. Today the market value of Britain’s top companies is$NL$five times their physical assets, demonstrating the economic power of knowledge, ideas$NL$and innovation.20$NL$What Brown was saying was that most property of the “top companies” is now intellectual$NL$property. It is largely IT which has brought about this change and it naturally means that$NL$intellectual property law has become a very significant area of business law, which is having to$NL$develop in response to developments in the technology.$NL$The topic which might perhaps come first to a student reader’s mind is the way that sharing$NL$music over peer-to-peer networks has been undermining the copyrights owned by music$NL$companies, which have been struggling either to invoke the law to defend their position, or to$NL$develop novel business models that allow them to make money within the new technological$NL$environment. But for present purposes, this area is not actually very significant. The law of$NL$copyright as it applies to music is clear the only change introduced by IT lies in making the law$NL$easy to break and hard to enforce. More interesting, for this textbook, are areas where the$NL$property itself (not just the means used to reproduce it or move it around) consists of things like$NL$computer software or electronic databanks. In those areas, it is often far from clear how the$NL$existing laws of intellectual property apply. Courts are adapting laws that were written long ago$NL$for other purposes in order to develop an intellectual-property régime for the IT industry, and so$NL$far this is not working too well.$NL$The issues are not about enforcement – unlike with music filesharing, where many of the$NL$individuals involved do not care whether their activity is legal, provided they feel safe from$NL$detection! In civilized societies, most organizations by and large aim to keep within the law and$NL$respect one another’s property rights – but they need to know what those rights are. It would be$NL$hard for a business to be profitable, if it made a habit of not insisting on rights which it did$NL$legally possess.$NL$There are two longstanding legal devices for defining and protecting different sorts of intellectual$NL$property: copyright, and patent. Copyright was originally introduced to define ownership in$NL$“literary works”, such as novels, poems, or non-fiction books, but came to be extended by$NL$analogy to things like musical compositions, films, and so forth. Patents relate to newly-invented$NL$machines or industrial processes.$NL$Neither copyright nor patent law was part of the Common Law both devices were introduced by$NL$statute. (Indeed, the USA has had a general law of copyright only since the 1890s – it was a$NL$standing grievance for Victorian novelists that no sooner did the fruits of their labour emerge$NL$from the press than American publishers’ agents would rush single copies across the Atlantic,$NL$where they would be reprinted and sold without reward to the author.) The original motivation of$NL$both copyright and patent law was the same: they were intended to stimulate advances (in literature,$NL$and in industry) which would benefit society, by creating concrete incentives for the innovators.$NL$The kinds of protection offered by the two areas of law are different. Copyright is something that$NL$the author of a “literary work” acquires automatically in producing the work, and it forbids$NL$anyone else to make a copy of the work (for a set number of years into the future, and with$NL$various provisos that do not matter here) without the right-holder’s permission. Thus an author’s$NL$copyright is a piece of property which he can sell or license for money in the case of books,$NL$typically a publishing company contracts with an author for permission to publish his book in$NL$exchange for royalties paid to him on copies sold. With newer media such as films, the business$NL$models are different, but the underlying law (which is what concerns us) is essentially the same.$NL$A patent, on the other hand, is not acquired automatically by the inventor (or anyone else).$NL$Taking out a patent is a complicated and expensive undertaking, but if a patent is granted, it$NL$forbids anyone (again, for a set future period) from exploiting the process or mechanism without$NL$the patent-holder’s permission so again the patent is an economically-valuable piece of property,$NL$which can be sold or licensed to companies wanting to use the innovation in their business.$NL$The legal contrast between copyright and patent was neatly summed up by Tim Press:$NL$A document setting out a novel chemical process would attract copyright protection, but$NL$that protection would protect the document against copying, not the process from being$NL$carried out. A patent for the process would prevent it from being carried out but not from$NL$being written about or broadcast.21$NL$Computer programs are “text” which defines and controls “processes”. So on the face of it there$NL$is a question about which kind of intellectual-property protection is more relevant to software.$NL$Over the years during which IT has been economically important, the answer has been shifting.
12;The entry point into all our programs is called main() and this is a function, or a piece of code that$NL$does something, usually returning some value. We structure programs into functions to stop them$NL$become long unreadable blocks of code than cannot be seen in one screen or page and also to ensure$NL$that we do not have repeated identical chunks of code all over the place. We can call library$NL$functions like printf or strtok which are part of the C language and we can call our own or other$NL$peoples functions and libraries of functions. We have to ensure that the appropriate header file exists$NL$and can be read by the preprocessor and that the source code or compiled library exists too and is$NL$accessible.$NL$As we learned before, the scope of data is restricted to the function in which is was declared, so we$NL$use pointers to data and blocks of data to pass to functions that we wish to do some work on our data.$NL$We have seen already that strings are handled as pointers to arrays of single characters terminated with$NL$a NULL character.$NL$In this example we can repeatedly call the function “doit” that takes two integer arguments and reurns$NL$the result of some mathematical calculation.$NL$(by now you should be maintaining a Makefile as you progress, adding targets to compile examples as$NL$you go.)$NL$The result in a browser looks like this called with “func1?5:5”.$NL$In this case the arguments to our function are sent as copies and are not modified in the function but$NL$used.$NL$If we want to actual modify a variable we would have to send its pointer to a function.$NL$We send the address of the variable 'result' with &result, and in the function doit we de-reference the$NL$pointer with *result to get at the float and change its value, outside its scope inside main . This gives$NL$identical output to chapter3_1.c.$NL$C contains a number of built-in functions for doing commonly used tasks. So far we have used atoi,$NL$printf, sizeof, strtok, and sqrt. To get full details of any built-in library function all we have to do is$NL$type for example:$NL$and we will see all this:$NL$Which pretty-well tells you everything you need to know about this function and how to use it and$NL$variants of it. Most importantly it tells you which header file to include.$NL$There is no point in learning about library functions until you find you need to do something which$NL$then leads you to look for a function or a library of functions that has been written for this purpose.$NL$You will need to understand the function signature – or what the argument list means and how to use it$NL$and what will be returned by the function or done to variables passed as pointers to functions.$NL$Sometimes we wish to manage a set of variable as a group, perhaps taking all the values from a$NL$database record and passing the whole record around our program to process it. To do this we can$NL$group data into structures.$NL$This program uses a struct to define a set of properties for something called a player. The main$NL$function contains a declaration and instantiation of an array of 5 players. We pass a pointer to each$NL$array member in turn to a function to rank each one. This uses a switch statement to examine the first$NL$letter of each player name to make an arbitrary ranking. Then we pass a pointer to each array member$NL$in turn to a function that prints out the details.$NL$The results are shown here, as usual in a browser:$NL$This is a very powerful technique that is quite advanced but you will need to be aware of it. The idea$NL$of structures leads directly to the idea of classes and objects.$NL$We can see that using a struct greatly simplifies the business task of passing the data elements around$NL$the program to have different work done. If we make a change to the definition of the struct it will$NL$still work and we simply have to add code to handle new properties rather than having to change the$NL$argument lists or signatures of the functions doing the work.$NL$The definition of the structure does not actually create any data, but just sets out the formal shape of$NL$what we can instantiate. In the main function we can express this instantiation in the form shown$NL$creating a list of sequences of data elements that conform to the definition we have made.$NL$You can probably see that a struct with additional functions or methods is essentially what a class is in$NL$Java, and this is also the case in C++. Object Oriented languages start here and in fact many early$NL$systems described as “object oriented” were in fact just built using C language structs.$NL$If you take a look for example, at the Apache server development header files you will see a lot of$NL$structs for example in this fragment of httpd.h :$NL$Dont worry about what this all means – just notice that this is a very common and very powerful$NL$technique, and the design of data structures, just like the design of database tables to which it is closely$NL$related are the core, key, vital task for you to understand as a programmer.$NL$You make the philosophical decisions that the world is like this and can be modelled in this way. A$NL$heavy responsibility - in philosophy this work is called ontology (what exists?) and epistemology$NL$(how we can know about it?). I bet you never thought that this was what you were doing!$NL$We have used some simple data types to represent some information and transmit input to a program$NL$and to organise and display some visual output.$NL$We have used HTML embedded in output strings to make output visible in a web browser.$NL$We have learned about creating libraries of functions for reuse.$NL$We have learning about data structures and the use of pointers to pass them around a program.
12;Using the File Manager (in KDE, Konqueror or in Gnome, Nautilus) create a new directory$NL$somewhere in your home directory called something appropriate for all the examples in this book,$NL$perhaps “Programming_In_Linux” without any spaces in the name.$NL$Open an editor (in KDE, kate, or in Gnome, gedit) and type in (or copy from the supplied source code$NL$zip bundle) the following:$NL$Save the text as chapter1_1.c in the new folder you created in your home directory.$NL$Open a terminal window and type: gcc -o hello chapter1_1.c$NL$to compile the program into a form that can be executed.$NL$Now type “ls -l” to list the details of all the files in this directory. You should see that chapter1_2.c is$NL$there and a file called “hello” which is the compiled C program you have just written.$NL$Now type: ./hello$NL$to execute, or run the program and it should return the text:$NL$"Hello you are learning C!!".$NL$If this worked, congratulations, you are now a programmer!$NL$The part inside /*** ***/ is a comment and is not compiled but just for information and$NL$reference.$NL$The “#include...” part tells the compiler which system libraries are needed and which header files$NL$are being referenced by this program. In our case “printf” is used and this is defined in the$NL$stdio.h header.$NL$The “int main(int argc, char *argv[])” part is the start of the actual program. This is an entrypoint$NL$and most C programs have a main function.$NL$The “int argc” is an argument to the function “main” which is an integer count of the number of$NL$character string arguments passed in “char *argv[]” (a list of pointers to character strings) that$NL$might be passed at the command line when we run it.$NL$A pointer to some thing is a name given to a memory address for this kind of data type. We can$NL$have a pointer to an integer: int *iptr, or a floating point number: float *fPtr. Any list of things is$NL$described by [], and if we know exactly how big this list is we might declare it as [200]. In this$NL$case we know that the second argument is a list of pointers to character strings.$NL$Everything else in the curly brackets is the main function and in this case the entire program$NL$expressed as lines.$NL$Each line or statement end with a semi-colon “”.$NL$We have function calls like “printf(...)” which is a call to the standard input / output library$NL$defined in the header file stdio.h.$NL$At the end of the program “return 0” ends the program by returning a zero to the system.$NL$Return values are often used to indicate the success or status should the program not run$NL$correctly.$NL$Taking this example a stage further, examine the start of the program at the declaration of the entry$NL$point function: int main(int argc, char *argv[])$NL$In plain English this means:$NL$The function called “main”, which returns an integer, takes two arguments, an integer called “argc”$NL$which is a count of the number of command arguments then *argv[] which is a list or array of pointers$NL$to strings which are the actual arguments typed in when you run the program from the command line.$NL$Let's rewrite the program to see what all this means before we start to panic.$NL$Save the text as chapter1_2.c in the same folder.$NL$Open a terminal window and type:$NL$gcc -o hello2 chapter1_2.c to compile the program into a form that can be executed.$NL$Now type ls -l to list the details of all the files in this directory. You should see that chapter1_2.c is$NL$there and a file called hello2 which is the compiled C program you have just written.$NL$Now type ./hello2 to execute, or run the program and it should return the text:$NL$We can see that the name of the program itself is counted as a command line argument and that the$NL$counting of things in the list or array of arguments starts at zero not at one.$NL$Now type ./hello2 my name is David to execute the program and it should return the text:$NL$So, what is happening here? It seems we are reading back each of the character strings (words) that$NL$were typed in to run the program.$NL$Lets get real and run this in a web page. Make the extra change adding the first output printf statement$NL$“Content-type:text/plain\n\n” which tells our server what kind of MIME type is going to be$NL$transmitted.$NL$Compile using gcc -o hello3 chapter1_3.c and copy the compiled file hello3 to your$NL$public_html/cgi-bin directory (or on your own machine as superuser copy the program to$NL$/srv/www/cgi-bin (OpenSuse) or /usr/lib/cgi-bin (Ubuntu)).$NL$Open a web browser and type in the URL http://localhost/cgi-bin/hello3?david+haskins and you$NL$should see that web content can be generated by a C program.$NL$A seldom documented feature of the function signature for “main” is that it can take three arguments$NL$and the last one we will now look at is char *env[ ] which is also a list of pointers to strings, but in this$NL$case these are the system environment variables available to the program at the time it is run$NL$Compile with gcc -o hello4 chapter1_4.c and as superuser copy the program to /srv/www/cgi-bin$NL$(OpenSuse) or /usr/lib/cgi-bin (Ubuntu). You can run this from the terminal where you compiled it$NL$with ./hello4 and you will see a long list of environment variables. In the browser when you enter$NL$http://localhost/cgi-bin/hello4 you will a different set altogether.$NL$We will soon find out that QUERY_STRING is an important environment variable for us in$NL$communicating with our program and in this case we see it has a value of “david+haskins” or$NL$everything after the “?” in the URL we typed. It is a valid way to send information to a common$NL$gateway interface (CGI) program like hello4 but we should restrict this to just one string. In our case$NL$we have used a “+” to join up two strings. If we typed: “david haskins” the browser would translate$NL$this so we would see:$NL$QUERY_STRING=david%20haskins$NL$We will learn later how complex sets of input values can be transmitted to our programs.
12;When we write programs we have to make decisions or assertions about the nature of the world as we$NL$declare and describe variables to represent the kinds of things we want to include in our information$NL$processing.$NL$This process is deeply philosophical we make ontological assertions that this or that thing exists and$NL$we make epistemological assertions when we select particular data types or collections of data types$NL$to use to describe the attributes of these things. Heavy stuff with a great responsibility and not to be$NL$lightly undertaken.$NL$As a practical example we might declare something that looks like the beginnings of a database record$NL$for geography.$NL$Here we are doing the following:$NL$- asserting that all the character strings we will ever encounter in this application will be 255$NL$limited to characters so we define this with a preprocessor statement – these start with #.$NL$- assert that towns are associated with counties, and counties are associated with countries some$NL$hierarchical manner.$NL$- assert that the population is counted in whole numbers – no half-people.$NL$- assert the location is to be recorded in a particular variant (WGS84) of the convention of$NL$describing spots on the surface of the world in latitude and longitude that uses a decimal$NL$fraction for degrees, minutes, and seconds.$NL$Each of these statements allocates memory within the scope of the function in which it is declared.$NL$Each data declaration will occupy an amount of memory in bytes and give that bit of memory a$NL$label which is the variable name. Each data type has a specified size and the sizeof() library function$NL$will return this as an integer. In this case 3 x 256 characters, one integer, and two floats. The exact$NL$size is machine dependent but probably it is 780 bytes.$NL$Outside the function in which the data has been declared this data is inaccessible – this is the scope of$NL$declaration. If we had declared outside the main() function it would be global in scope and other$NL$functions could access it. C lets you do this kind of dangerous stuff if you want to, so be careful.$NL$Generally we keep a close eye on the scope of data, and pass either read-only copies, or labelled$NL$memory addresses to our data to parts of the programs that might need to do work on it and even$NL$change it. These labelled memory addresses are called pointers.$NL$We are using for output the printf family of library functions (sprintf for creating strings, fprintf for$NL$writing to files etc) which all use a common format string argument to specify how the data is to be$NL$represented.$NL$- %c character$NL$- %s string$NL$- %d integer$NL$- %f floating point number etc.$NL$The remaining series of variables in the arguments are placed in sequence into the format string as$NL$specified.$NL$In C it is a good idea to intialise any data you declare as the contents of the memory allocated for$NL$them is not cleared but may contain any old rubbish.$NL$Compile with: gcc -o data1 chapter2_1.c -lc$NL$Output of the program when called with : ./data1$NL$Some programming languages like Java and C++ have a string data type that hides some of the$NL$complexity underneath what might seem a simple thing.$NL$An essential attribute of a character string is that it is a series of individual character elements of$NL$indeterminate length.$NL$Most of the individual characters we can type into a keyboard are represented by simple numerical$NL$ASCII codes and the C data type char is used to store character data.$NL$Strings are stored as arrays of characters ending with a NULL so an array must be large enough to$NL$hold the sequence of characters plus one. Remember array members are always counted from zero.$NL$In this example we can see 5 individual characters declared and initialised with values, and an empty$NL$character array set to “”.$NL$Take care to notice the difference between single quote marks ' used around characters and double$NL$quote marks “ used around character strings.$NL$Compile with: gcc -o data2 chapter2_2.c -lc$NL$Output of the program when called with : ./data2$NL$Anything at all – name given to a variable and its meaning or its use is entirely in the mind of the$NL$beholder. Try this$NL$Download free ebooks at bookboon.com$NL$C Programming in Linux$NL$29$NL$Data and Memory$NL$Compile with: gcc -o data3 chapter2_3.c -lc$NL$As superuser copy the program to your public_html/cgi-bin directory (or /srv/www/cgi-bin$NL$(OpenSuse) or /usr/lib/cgi-bin (Ubuntu)).$NL$In the browser enter: http://localhost/cgi-bin/data3?red$NL$what you should see is this:$NL$Or if send a parameter of anything at all you will get surprising results:$NL$What we are doing here is using the string parameter argv[1] as a background colour code inside an$NL$HTML body tag. We change the Content-type specification to text/html and miraculously now our$NL$program is generating HTML content. A language being expressed inside another language. Web$NL$browsers understand a limited set of colour terms and colours can be also defined hexadecimal codes$NL$such as #FFFFFF (white) #FF0000 (red) #00FF00 (green) #0000FF (blue).$NL$This fun exercise is not just a lightweight trick, the idea that one program can generate another in$NL$another language is very powerful and behind the whole power of the internet. When we generate$NL$HTML (or XML or anything else) from a common gateway interface program like this we are$NL$creating dynamic content that can be linked to live, changing data rather than static pre-edited web$NL$pages. In practice most web sites have a mix of dynamic and static content, but here we see just how$NL$this is done at a very simple level.$NL$Throughout this book we will use the browser as the preferred interface to our programs hence we will$NL$be generating HTML and binary image stream web content purely as a means to make immediate the$NL$power of our programs. Writing code that you peer at in a terminal screen is not too impressive, and$NL$writing window-type applications is not nearly so straightforward.$NL$In practice most of the software you may be asked to write will be running on the web so we might as$NL$well start with this idea straight away. Most web applications involve multiple languages too such as$NL$CSS, (X)HTML, XML, JavaScript, PHP, JAVA, JSP, ASP, .NET, SQL. If this sounds frightening, don't$NL$panic. A knowledge of C will show you that many of these languages, which all perform different$NL$functions, have a basis of C in their syntax.
13;It is generally acknowledged in Artificial Intelligence research that search is crucial to building intelligent systems and for the design of intelligent agents. For example, Newell (1994) has stated that search is fundamental for intelligent behaviour (see the quote at the beginning of this chapter). From a behavioural perspective, search can be considered to be a meta-behaviour where the agent is making a decision on which behaviour amongst a set of possible behaviours to execute in a given situation. In other words, it can be defined as the behavioural process that the agent employs in order to make decisions about which choice of actions it should perform in order to carry out a specific task. The task may include higher-level cognitive behaviours such as learning, strategy, goal-setting, planning, and modelling (these were called Action Selection in Reynold’s Boids model). If there are no decisions to be made, then searching is not required. If the agent already knows that a particular set of actions, or behaviour, is appropriate for a given situation, then there is no need to search for the appropriate behaviour, and therefore the actions can be applied without coming to any decision. Searching can be considered to be a behaviour that an agent exhibits when it has insufficient knowledge in order to solve a problem. The problem is defined by the current situation of the agent, which is determined by the environmental conditions, its own circumstances and the knowledge the agent currently has available to it. If the agent has insufficient knowledge in order to solve a given problem, then it may choose to search for further knowledge about the problem. If it already has sufficient knowledge, it will not need to employ searching behaviour in order to solve the problem. An agent uses search behaviour in order to answer a question it does not already know the answer to or complete a task it does not know how to complete. The agent must explore an environment by following more than one path in order to obtain that knowledge. $NL$$NL$The exploration of the environment is carried out in a manner analogous to early explorers of the American or Australian continents, or people exploring a garden maze such as the Hampton Court Palace Maze or the Chevening House Maze. For the two garden mazes, the people trying to get to the centre of the maze must employ search behaviour if they do not have the knowledge about where it is. If they take a map with them, however, they no longer have to use search behaviour as the map provides them with the knowledge of which paths to take to get to the centre. The early American explorers Lewis and Clark were instructed by Thomas Jefferson to explore the Missouri and find a water route across the continent to the Pacific. They did not already know how large the continent was or what was out there and needed to physically explore the land. They chose to head in a westerly then north-westerly direction following a path along the Missouri river. Similarly, the Australian explorers Burke and Wills led an expedition starting from Melbourne with the goal of reaching the Gulf of Carpentaria. The land at the time had yet to be explored by European settlers. The expedition were prevented from reaching their ultimate goal just three miles short of the northern coastline due to mangrove swamps and, worse still, the expedition leaders died on the return journey. When performing a search, an agent can adopt different behaviours that determine the way the search is performed. The search behaviour adopted can have a significant impact on the effectiveness of the search. For example, poor leadership was blamed for the unsuccessful Burke and Wills expedition. The behaviour can be thought of as a strategy the agent adopts when performing the search. We can consider these search behaviours from an embodied agent perspective. The type of search behaviour is determined both by the embodiment of the agent, and whether the agent employs a reactive or more cognitive behaviour when executing the search. We have already seen many examples of how an agent can perform a search of an environment as a side effect of employing purely reactive behaviour (see Section 5.4 and Figures 5.2, 5.7 to 5.10 and 6.9). We can term these types of search behaviours as reactive search. We can also use the term cognitive search for cases when an agent has an ability to recognize that there is a choice to be made when a choice presents itself in the environment (such as when the agent reaches a junction in the maze). As stated in section 5.5, this act of recognition is a fundamental part of cognitive-based searching behaviour, and it is related to the situation that the agent finds itself in, the way its body is moving and interacting with the environment. It is also related to what is happening in the environment externally, and/or what is happening with other agents in the same environment if there are any. In addition, a single agent can be used to perform any given search, but there is nothing stopping us from using more than one agent to perform the search. We can adopt a multi-agent perspective to describe how each particular search can be performed in order to clarify how the searches differ. In this case, the effectiveness of a particular search can be evaluated by comparing the number of agents that are needed to perform the search and the amount of information that is communicated between them. $NL$$NL$There are many problems that require search with varying degrees of difficulty. Simplified or ‘toy’ problems are problems that are to the most part synthetic and unrelated to real life. However, these problems can be useful to designers in gaining insight into the problem of search. Once the properties of the different search behaviours have been investigated on toy problems, then they can be adapted and/or scaled up to real life problems. To illustrate different search problems, this section describes three problems in particular that have been simulated in NetLogo – these are the Searching Mazes model, the Missionaries and Cannibals model, and the Searching for Kevin Bacon model.
13;What is the nature of intelligence? That is a question that has been pondered, and debated for thousands of years. Many people over the centuries have offered their own view on the matter, as illustrated by the quotes provided in Table 10.1. $NL$$NL$It seems that everyone has their own opinion on what intelligence is or isn’t. Intelligence is a concept that everyone knows about, but understands differently. As we have seen in the previous chapter, the way each person understands a particular concept will have its own unique ‘flavour’. Perhaps one of the most interesting quotes above is by Susan Sontag that uses an analogy between taste and intelligence. Taste is a complex sensation in four dimensions – sweetness, sourness, bitterness and saltiness. Similarly, intelligence is a complex concept, with multiple dimensions. $NL$$NL$Intelligence is multi-faceted – its nature cannot be defined using one of these quotes alone it requires all of them. As an analogy, try describing the Mona Lisa. One person’s description of the painting may be anathema to another person. To imagine that we can distil the Mona Lisa down to a few written words, and then naïvely believe other people will agree with us that it is the one and only definitive description, is like believing that people should only ever eat one type of food, or enjoy looking at one type of painting, or read one type of book. The Mona Lisa painting continues to inspire people to write more and more words about it. Similarly, intelligence is not something we can elucidate definitively. But that will not stop people from continuing to do so, since in so doing further insights can be gained into its nature. Although definitions of intelligence are fraught with problems, we can look for desirable properties of intelligence that we can help us to describe the nature of intelligence. In other words, we can help define the nature of intelligence by describing what it ‘looks’ like or what it ‘tastes’ like. Using the taste analogy, we can think of these properties as being ‘ingredients’ in a recipe for intelligence – we need to mix them together in order to make a particular taste, which some people will like, while others may not, preferring alternative tastes. For example, we can use the analogy of African and Australian explorers trying to describe what a giraffe or platypus looks like to someone who has never seen it. These explorers will use words (concepts) that they are familiar with, such as ‘long neck’ and ‘fish-like tail’, but their description will be ‘flavoured’ by their own unique perspective. Whatever words they come up with, they will have over-emphasized certain features and ignored other important ingredients. $NL$$NL$Similarly, AI researchers with a background in knowledge engineering and the symbolic approach to AI will describe intelligence using ingredients such as the following: • the capacity to acquire and apply knowledge • the ability to perform reasoning and • the ability to make decisions and plan in order to achieve a specific goal. AI researchers who prefer a behavioural-based approach will describe the intelligent behaviour of embodied, situated agents using ingredients such as: • the ability to perform an action that an external intelligent agent would deem to be intelligent • the ability to demonstrate knowledge of the consequences of its actions and • the ability to demonstrate knowledge of how to influence or change its environment in order to affect outcomes and achieve its goals. If we think of intelligence using an analogy of mapping, as discussed in the previous chapter, then we might use the following ingredients to describe intelligence: • the ability of an embodied, situated agent to map environments, both real and abstract (i.e. recognize patterns to provide useful simplifications and/or characterizations of its environments) • the ability to use maps to navigate around its environments • the ability to update its maps when it finds they do not fit reality and • the ability to communicate details of its maps to other agents. It is important to realise, however, that these are not definitive descriptions, just ingredients in alternative recipes for intelligence. In the previous chapters, we have seen various examples (implemented as models in NetLogo) that have demonstrated some of these ingredients. In some respects, these models have exhibited a small degree of intelligence in the sense that if we observed a human agent with the same behaviour, we would deem that to be a sign of intelligence. In the next volume of this book series, we will also see other models that will demonstrate more advanced technologies. It can be argued, however, that these examples show no true intelligence – but of course that depends on your own perspective, and the ingredients with which you choose for your own recipe for intelligence. $NL$$NL$In the last chapter, a question was asked about whether it was possible to have knowledge without representation. Similarly, we can ask ourselves the following question: “Is it possible to have intelligence without representation?” In a seminal paper, Rodney Brooks (1991) considered exactly this same question. In another paper (Brooks, 1991), he also considered the related question: “Is it possible to have intelligence without reasoning?” As discussed in the previous chapter, Brooks favours the embodied, situated approach to AI – the sub-symbolic paradigm rather than the classical symbolic paradigm. When he talks about the possibility of intelligence without ‘representation’, he means that an embodied, situated agent does not need to explicitly represent its environment – it can simply react to it. There is no need for the agent to have an explicit knowledge base about the world it is situated in since the agent can directly ‘consult’ it by interacting with it. Brooks goes further and states that intelligence is an emergent property of certain complex systems (see quote at the beginning of this chapter). Brook’s ideas are interesting in that it raises the possibility that, in designing AI systems, we may not have to do all the work ourselves. If we can find the right way of setting up the initial conditions of the system, the system itself will do the work for us, and through self-organisation, intelligence will emerge as a result. Unfortunately, although this idea is very intriguing, no one as yet has figured out how to set up the necessary initial conditions.
13;The way an agent behaves is often used to tell them apart and to distinguish what and who they are, whether animal, human or artificial. Behaviour can also be associated with groups of agents, not just a single agent. For example, human cultural behaviour relates to behaviour that is associated with a particular nation, people or social group, and is distinct from the behaviour of an individual human being or the human body. Behaviour also has an important role to play in the survival of different species and subspecies. It has been suggested, for example, that music and art formed part of a suite of behaviours displayed by our own species that provided us with the evolutionary edge over the Neanderthals. $NL$$NL$In the two preceding chapters, we have talked about various aspects concerning behaviours of embodied, situated agents, such as how an agent’s behaviour from a design perspective can be characterised in terms of its movement it exhibits in an environment, and how agents exhibit a range of behaviours from reactive to cognitive. We have not, however, provided a more concrete definition of what behaviour is. From the perspective of designing embodied, situated agents, behaviour can be defined as follows. A particular behaviour of an embodied, situated agent is a series of actions it performs when interacting with an environment. The specific order or manner in which the actions’ movements are made and the overall outcome that occurs as a result of the actions defines the type of behaviour. We can define an action as a series of movements performed by an agent in relation to a specific outcome, either by volition (for cognitive-based actions) or by instinct (for reactive-based actions). With this definition, movement is being treated as a fundamental part of the components that characterise each type of behaviour – in other words, the actions and reactions the agent executes as it is performing the behaviour. The distinction between a movement and an action is that an action comprises one or more movements performed by an agent, and also that there is a specific outcome that occurs as a result of the action. For example, a human agent might wish to perform the action of turning a light switch on. The outcome of the action is that the light gets switched on. This action requires a series of movements to be performed such as raising the hand up to the light switch, moving a specific finger up out of the hand, then using that finger to touch the top of the switch, then applying pressure downwards until the switch moves. The distinction between an action and a particular behaviour is that a behaviour comprises one or more actions performed by an agent in a particular order or manner. For example, an agent may prefer an energy saving type of behaviour by only switching lights on when necessary (this is an example of a cognitive type of behaviour as it involves a conscious choice). Another agent may always switch on the light through habit as it enters a room (this is an example of a mostly reactive type of behaviour). Behaviour is the way an agent acts in a given situation or set of situations. The situation is defined by the environmental conditions, its own circumstances and the knowledge the agent currently has available to it. If the agent has insufficient knowledge for a given situation, then it may choose to search for further knowledge about the situation. Behaviours can be made up of sub-behaviours. The search for further knowledge is itself a behaviour, for example, and may be a component of the original behaviour. There are also various aspects to behaviour, including the following: sensing and movement (sensory-motor co-ordination) recognition of the current situation (classification) decision-making (selection of an appropriate response) performance (execution of the response). $NL$$NL$Behaviours range from the fully conscious (cognitive) to the unconscious (reactive), from overt (done in an open way) to covert (done in a secretive way), and from voluntary (the agent acts according to its own free will) to involuntary (done without conscious control or done against the will of the agent). The term ‘behaviour’ also has different meanings depending on the context (Reynolds, 1987). The above definition is applicable when the term is being used in relation to the actions of a human or animal, but it is also applicable in describing the actions of a mechanical system, or the complex actions of a chaotic system, if the agent-oriented perspective is considered (here the agents are humans, animals, mechanical systems or complex systems). However, in virtual reality and multimedia applications, the term can sometimes be used as a synonym for computer animation. In the believable agents and artificial life fields, behaviour is used “to refer to the improvisational and life-like actions of an autonomous character” (Reynolds, 1987). We also often anthropomorphically attribute human behavioural characteristics with how a computer operates when we say that a computer system or computer program is behaving in a certain way based on responses to our interaction with the system or program. Similarly, we often (usually erroneously) attribute human behavioural characteristics with animals and inanimate objects such as cars. $NL$$NL$In this section, we will further explore the important distinction between reactive and cognitive behaviour that was first highlighted in the previous chapter. Agents can be characterised by where they sit on a continuum as shown in Figure 6.1. This continuum ranges from purely reactive agents that exhibit no cognitive abilities (such as ants and termites), to agents that exhibit cognitive behaviour or have an ability to think. Table 6.1 details the differences between the two types of agents. In reality, many agents exhibit both reactive and cognitive behaviours to varying degrees, and the distinction between reactive and cognitive can be arbitrary. $NL$$NL$Comparing the abilities of reactive agents with cognitive agents listed in Table 6.1, it is clear that reactive agents are very limited in what they can do as they do not have the ability to plan, co-ordinate between themselves or set and understand specific goals they simply react to events when they occur. This does not preclude them from having a role to play in producing intelligent behaviour. The reactive school of thought is that it is not necessary for agents to be individually intelligent. However, they can work together collectively to solve complex problems.
13;Communication may be defined as the process of sharing or exchanging of information between agents. An agent exhibits communicating behaviour when it attempts to transmit information to another agent. A sender agent or agents transmits a message through some medium to a receiver agent or agents. The term communication in common English usage can also refer to interactions between people that involve the sharing of information, ideas and feelings. Communication is not unique to humans, though, since animals and even plants also have the ability to communicate with each other. Language can be defined as the set of symbols that agents communicate with in order to convey information. In Artificial Intelligence, human language is often called ‘natural language’ in order to distinguish it from computer programming languages. Communicating using language is often considered to be a uniquely human behavioural trait. Human language, such as spoken, written or sign, is distinguished from animal communication systems in that it is learned rather than inherited biologically. Although various animals exhibit the ability to communicate, and some animals such as orangutans and chimpanzees even have the ability to use certain features of human language, it is the degree of sophistication and complexity in human language that distinguishes it from animal communication systems. Human language is based on the unique ability of humans to think abstractly, using symbols to represent concepts and ideas. Language is defined by a set of socially shared rules that define the commonly accepted symbols, their meaning and their structural relationships specified by rules of grammar. These rules describe how the symbols can be manipulated to create a potentially infinite number of grammatically correct symbol sequences. The specific symbols chosen are arbitrary and can be associated with any particular phoneme, grapheme or sign. $NL$$NL$Linguistics is the scientific study of language which can be split into separate areas of study: grammar is the study of language structure morphology is the study of how words are formed and put together phonology is the study of systems of sounds syntax concerns the rules governing how words combine into phrases and sentences semantics is the study of meaning and pragmatics concerns the study of language and use and the contexts in which it is used. $NL$$NL$In all natural languages, there is a wide variation in usage as well as frequent lack of agreement amongst language users. For example, Table 7.1 lists some examples of acceptable ‘English’ sentences from various regions of the world (Newbrook, 2009). Each of the sentences is regarded as ‘normal’ English for the region shown on the right, and yet most people outside those regions would argue differently, and in many cases have difficulty in understanding their meaning. $NL$$NL$Concerning the English language, David Crystal (1988) states: “The English language stretches around the world: from Australia to Zimbabwe, over 300 million people speak it as their mother tongue alone… And yet, despite its astonishingly widespread use as a medium of communication, every profession and every province – indeed, every individual person – uses a slightly different variant.” English has many different regional dialects (such as American, British, Australian and New Zealand English), as well as many sub-dialects within those regions. There are also dialects that cut across regional lines, for example, “Public School English” in Britain, Black English in America and Maori English in New Zealand. And in every country, there are countless social variations that “possess their own bewildering variety of cants, jargons and lingoes” (Claiborne 1990, page 20). One of the more colourful examples is a dictionary on Wall Street slang entitled High steppers, fallen angels, and lollipops (Odean, 1989). It illustrates how such language can become almost unintelligible to the uninitiated. (For example, what is a ‘high stepper’ or a ‘fallen angel’?) Hudson (1983, page 69) writes the following in The language of the teenage revolution about the resentment of older people to the language used by contemporary teenagers : “… perhaps, they dislike the fact that teenagers speak another kind of language, using a considerable number of expressions which they themselves find either incomprehensible or repulsive.” As well as language being diverse, there are many different ways that language is expressed and used. Spoken language is markedly different from written language, as illustrated from the following example taken from Crystal (1981): “This is part of a lecture, and I chose it because it shows that even a professional speaker uses structure that would rarely if ever occur in written English, and displays a ‘disjointedness’ of speech that would be altogether lacking there. (Everyday conversation provides even more striking differences.) The dot (.) indicates a short pause, the dash a longer pause, and the erm is an attempt to represent the noises the speaker made when he was audibly hesitating. … – and I want . very arbitrarily if I may to divide this into three headings --- and to ask . erm . three questions . assessment why – assessment of what – and assessment how . so this is really . means I want to talk about . first of all the purposes of assessment – why we are assessing at all – erm secondly the kind of functions and processes that are being assessed – and thirdly I want to talk about techniques – …” Baugh (1957, page 17) reminds us that language is not just “the printed page” relatively uniform and fixed, as many people think it to be. Language is “primarily speech” and writing “only a conventional device for recoding sounds.” He further states that as the repeated muscular movements which generate speech are subject to gradual alteration on the part of the speaker: “each individual is constantly and quite unconsciously introducing slight changes in his speech. There is no such thing as uniformity in language. Not only does the speech of one community differ from that of another, but the speech of different individuals of a single community, even different members of the same family, is marked by individual peculiarities.” $NL$$NL$Some other distinctive forms of language are, for example, poetry, legal documents, newspaper reporting, advertising, letter writing, office correspondence, telegrams, telephone conversations, electronic mail, Usenet news articles, scientific papers and political speeches. Each has their own flavour, quirks and style. And within each form there are individual authors who have their own distinctive styles of language. The plays of Shakespeare and the science fiction novels of H. G. Wells are two examples of very distinct literary styles. In fact, every single person has their own style of language, or idiolect with its own unique characteristics that can be readily discerned by other people (Fromkin et al., 1990).
13;Knowledge is essential for intelligent behaviour. Without knowledge, an intelligent agent cannot make informed decisions, and instead must rely on using some form of searching type behaviour involving exploration and/or communication in order to gain the missing knowledge. Humans rely on knowledge every moment of their life – knowledge of how to communicate with other humans, knowledge of where they and other people live and work, knowledge of where things are, knowledge of how to behave in different situations, knowledge of how to perform different tasks and so on. Without the ability to store and process knowledge, the cognitive abilities of a human is seriously curtailed. An illness such as Alzheimer’s, for example, can be debilitating when memory loss occurs such as the difficulty in remembering recently learned facts. We all know (or think we know) what we mean when we use the term ‘knowledge’. But what exactly is knowledge? Bertrand Russell (1926) acknowledged the difficult question of how to define the meaning of ‘knowledge’: “It is perhaps unwise to begin with a definition of the subject, since, as elsewhere in philosophical discussions, definitions are controversial, and will necessarily differ for different schools”. $NL$$NL$A definition of knowledge is the subject of ongoing philosophical debate and presently there are many competing theories with no single definition universally agreed upon. Consequently, treatment of knowledge from an Artificial Intelligence perspective has often consciously avoided the definition of what knowledge is. However, this avoidance of providing a definition of knowledge upfront results in a lack of preciseness in the literature and research. The following argument will illustrate why. A knowledge-based system is a term used in Artificial Intelligence to refer to a system that processes knowledge in some manner. We can make the analogy of a knowledge-based system as being a repository of knowledge, whereas a database is a repository of data. However, in this definition, we have neglected to define the meaning of the term ‘knowledge’ and how it is different to data. For example, we can ask ourselves the following question – “What constitutes a knowledge-based system, and how does it differ from a database system?” This is a difficult question that cannot readily be answered in a straightforward way. A common approach taken in the literature is that a knowledge-based system can perform reasoning using some form of inferencing (whether rule-based, frame-based see below). Modern database systems, however, now employ most of these standard ‘knowledge-based’ techniques and more. Clearly, the addition of inferencing capabilities alone is not sufficient to define what a knowledge-based system is. However, a great deal of A.I. literature makes such an assumption. By avoiding a definition of knowledge, the problem becomes that it is no longer clear that what we are building really is in fact ‘knowledge-based’. In Chapter 1, it was stated that early A.I. systems in the 1970s and 1980s suffered from a lack of evaluation – there was a rush to build new systems, but often very little evaluation was undertaken of how well the systems worked. Without a working definition of knowledge, the same problem occurs now with current knowledge-based systems – how can we evaluate how effective our knowledge-base system might be if we do not have a definition of what it should be (or even achieve or do)? We can, however, avoid the philosophical pitfalls, and rather than attempting to define knowledge, and making a claim that this definition is the “right” one, instead we can propose design principles for our knowledge-based system. Hence, we can decide what principles we wish our knowledge-based system to adhere to, and we, as designers, are free to change them as we see fit based on knowledge we gain during the design process. Also, we are no longer standing on shaky ground in the sense that we do not have to provide one particular definition of knowledge which is open to philosophical debate, although we are still open to criticism about whether our principles are worthwhile from an engineering perspective (i.e. whether they produce “good” programs, or aren’t as good as other approaches). But evaluation becomes much simpler – all we need to do is evaluate whether our design principles are met. $NL$$NL$The following are some design principles for knowledge-based systems. $NL$$NL$The argument for this design principle is that if we design from an embodied, situated agent perspective, then all knowledge cannot exist independently of the agents. That is, knowledge cannot exist by itself – it can only be found in the ‘minds’ of the agents that are embodied and situated in an environment. We also wish to define and use the term ‘knowledge’ in a way similar to the way the term is used in natural language. The root of the word ‘knowledge’ comes from the verb “to know”. From a natural language perspective, ‘knowing’ and ‘knowledge’ are related. A rock, for example, does not ‘know’ anything. But a dog can ‘know’ where it has buried a bone and it makes sense to say in natural language that the dog has ‘knowledge’ of where the bone is buried. The dog, in this case, is the agent, and the rock is an object in the environment. In other words, knowing behaviour is associated with an agent who has knowledge. A knowledge-based system can then be thought of as an agent whose role is to convey the knowledge that it contains to the users of the system. The interaction between the user agent and the system agent can be characterised by the actions that determine each agent’s behaviour, and whether the user agent perceives the system agent to be acting in a knowledgeable way. This leads to the next design principle. $NL$$NL$The following design principles are based on properties of ‘good’ knowledge-base systems proposed by Russell and Norvig (2002): $NL$$NL$A behavioural approach to knowledge places the emphasis not on building a specific independent system, but on building agents that exhibit behaviour that demonstrates they have knowledge of their environment and of other agents. In this approach, the act of ‘knowing’ occurs when an agent has information that might potentially aid the performance of an action taken by itself or by another agent. Further, an agent can be considered to have ‘knowledge’ if it knows what the likely outcomes will be of an action it may perform, or of an action another agent is performing, or what is likely to happen to an object in the environment.
14;In Chapter Two, we see that class attributes are implemented in Java programmes as variables, whose$NL$values determine the state of an object. To some extent Chapter Two addresses the question of how we$NL$name variables this question is explored further in this chapter.$NL$Chapter Three explores some of the basic elements of the Java language. Given the nature of this guide, it$NL$is not the intention to make this chapter exhaustive with respect to all of the basic elements of the Java$NL$language. Further details can be found in the on-line Java tutorial.$NL$We see in Chapter Two that the two broad categories of Java types are primitives and classes. There are$NL$eight of the former and a vast number of classes, including several thousand classes provided with the$NL$Java language development environment and an infinitude of classes written by the worldwide community$NL$of Java developers. This chapter examines aspects of both categories of types.$NL$An identifier is a meaningful name given to a component in a Java programme. Identifiers are used to$NL$name the class itself – where the name of the class starts with an upper case letter – and to name its$NL$instances, its methods and their parameters. While class identifiers always – by convention – start with an$NL$upper case letter, everything else is identified with a word (or compound word) that starts with a lower$NL$case letter. Identifiers should be made as meaningful as possible, in the context of the application$NL$concerned. Thus compound words or phrases are used in practice.$NL$Referring to elements of the themed application, we can use the following identifiers for variables in the$NL$Member class:$NL$because we wouldn’t name a class membershipCard and spaces are not permitted in identifiers.$NL$We could have declared other variables in the class definition as follows:$NL$We cannot use what are known as keywords for identifiers. These words are reserved and cannot be$NL$used solely as an identifier, but can be used as part of an identifier. Thus we cannot identify a variable$NL$as follows:$NL$// not permitted because int is a keyword$NL$but we could write$NL$The table below lists the keywords in the Java language.$NL$Java is case-sensitive: this means that we cannot expect the following statement to compile:$NL$if we have not previously declared the identifier newint. On the other hand, if we write$NL$as the last statement of the getNewInt method, it will compile because the identifier named newInt has$NL$been declared previously.$NL$Similarly we cannot expect the compiler to recognise identifiers such as the following$NL$if they have not been declared before we refer to them later in our code.$NL$In one of the declarations in Section 3.2, we declared a variable with the identifier newInt to be of the int$NL$type, in the following statement:$NL$Let us deconstruct this simple statement from right to left: we declare that we are going to use an$NL$identifier named newInt to refer to integer values and ensure that access to this variable is private.$NL$This kind of declaration gives rise to an obvious question: what primitive data types are there in the Java$NL$language? The list on the next page summarises the primitive data types supported in Java.$NL$Before we move on to discuss assignment of actual values to variables, it will be instructive to find out if$NL$Java can convert between types automatically or whether this is left to the developer and if compile-time$NL$and run-time rules for conversion between types are different.$NL$In some situations, the JRE implicitly changes the type without the need for the developer to do this. All$NL$conversion of primitive data types is checked at compile-time in order to establish whether or not the$NL$conversion is permissible.$NL$Consider, for example, the following code snippet:$NL$A value of 10.0 is displayed when d is output.$NL$Evidently the implicit conversion from an int to a double is permissible.$NL$Consider this code snippet:$NL$The first statement compiles this means that the implicit conversion from an int to a double is permissible$NL$when we assign a literal integer value to a double. However the second statement does not compile: the$NL$compiler tells us that there is a possible loss of precision. This is because we are trying to squeeze, as it$NL$were, an eight byte value into a four byte value (see Table 3.2) the compiler won’t let us carry out such a$NL$narrowing conversion.$NL$On the other hand, if we write:$NL$// the cast ( int ) forces d to be an int we will examine the concept of casting$NL$// or explicit conversion later in this section$NL$Both statements compile and a value of 10 is displayed when i is output.$NL$The general rules for implicit assignment conversion are as follows:$NL$􀁸a boolean cannot be converted to any other type$NL$􀁸a non-boolean type can be converted to another non-boolean type provided that$NL$the conversion is a widening conversion$NL$􀁸a non-boolean type cannot be converted to another non-boolean type if the$NL$conversion is a narrowing conversion.$NL$Another kind of conversion occurs when a value is passed as an argument to a method when the method$NL$defines a parameter of some other type.$NL$For example, consider the following method declaration:$NL$The method is expecting a value of a double to be passed to it when it is invoked. If we pass a float to the$NL$method when it is invoked, the float will be automatically converted to a double.$NL$Fortunately the rules that govern this kind of conversion are the same as those for implicit assignment$NL$conversion listed above.$NL$The previous sub-section shows that Java is willing to carry out widening conversions implicitly. On the$NL$other hand, a narrowing conversion generates a compiler error. Should we actually intend to run the risk of$NL$the possible loss of precision when carrying out a narrowing conversion, we must make what is known as$NL$an explicit cast. Let us recall the following code snippet from the previous sub-section:$NL$Casting means explicitly telling Java to force a conversion that the compiler would otherwise not carry out$NL$implicitly. To make a cast, the desired type is placed between brackets, as in the second statement above,$NL$where the type of d – a double - is said to be cast (i.e. flagged by the compiler to be converted at run-time)$NL$into an int type.
14;There are several examples in previous chapters that illustrate how constructors are used to instantiate$NL$objects of a class. Let us recall the overall technique before we bring together a number of features of$NL$constructors in this chapter.$NL$One of the constructors for Member objects in the themed application is as follows:$NL$An object’s constructors have the same name as the class they$NL$instantiate.$NL$To access an object of the class Member in an application, we first declare a variable of the Member type$NL$in a main method in a test class as follows:$NL$The statement above does not create a Member object it merely declares a variable of the required type$NL$that can subsequently be initialised to refer to an instance of the Member type. The variable that refers to$NL$an object is known as its object reference. The object that an object reference refers to must be created$NL$explicitly, in a statement that instantiates a Member object as follows.$NL$The two statements above can be combined as follows.$NL$When the Member object is created by using ‘new’, the type of object required to be constructed is$NL$specified and the required arguments are passed to the constructor. The JRE allocates sufficient memory$NL$to store the fields of the object and initialises its state. When initialisation is complete, the JRE returns a$NL$reference to the new object. Thus, we can regard a constructor as returning an object reference to the$NL$object stored in memory.$NL$While objects are explicitly instantiated using ‘new’, as shown above for a Member object, there is no$NL$need to explicitly destroy them (as is required in some OO run-time systems). The Java Virtual Machine$NL$(JVM) manages memory on behalf of the developer so that memory for objects that is no longer used in an$NL$application is automatically reclaimed without the intervention of the developer.$NL$In general, an object’s fields can be initialised when they are declared or they can be declared without$NL$being initialised. For example, the code snippet on the next page shows part of the class declaration for a$NL$version of the Member class:$NL$The code snippet illustrates an example where some of the instance variables are initialised and some are$NL$only declared. In the case of the latter type of declaration, the instance variable is initialised to its default$NL$value when the constructor returns an object reference to the newly-created object. For example, the$NL$instance variable noOfCards is initialised to 0 when the object is created.$NL$Declaring and initialising none, some or all instance variables in this way if often sufficient to establish$NL$the initial state of an object. On the other hand, where more than simple initialisation to literals or default$NL$values is required and where other tasks are required to be performed, the body of a constructor can be$NL$used to do the work of establishing the initial state of an object. Consider the following part of the$NL$constructor for the Member class.$NL$This constructor is used when simple initialisation of Member objects is insufficient. Thus, in the code$NL$block of the constructor above, the arguments passed to the constructor are associated with four of the$NL$fields of the Member class. The effect of the four statements inside the constructor’s code block is to$NL$initialise the four fields before the constructor returns a reference to the object.$NL$Constructors can, like methods, generate or throw special objects that represent error conditions. These$NL$special objects are instances of Java’s in-built Exception class. We will explore how to throw and detect$NL$Exception objects in Chapter Four in An Introduction to Java Programming 2: Classes in Java$NL$Applications.$NL$It is worthwhile being reminded at this point in the discussion about constructors that the compiler inserts$NL$a default constructor if the developer has not defined any constructors for a class.$NL$The default constructor takes no arguments and contains no code. It$NL$is provided automatically only if the developer has not provided any$NL$constructors in a class definition.$NL$We saw in the previous chapter that methods can be overloaded. Constructors can be similarly overloaded$NL$to provide flexibility in initialising the state of objects of a class. For example, the following class$NL$definition includes more than one constructor.$NL$The example class – SetTheTime – is a simple illustration of a class which provides more than one$NL$constructor. The example also shows that a constructor can be called from the body of another constructor$NL$by using the ‘this’ invocation as the first executable statement in the constructor. Thus, in the example$NL$above, the two argument constructor is called in the first statement of the three argument constructor.$NL$Complex initialisation of fields can be achieved by using what is known as an initialisation block. An$NL$initialisation block is a block of statements, delimited by braces, that appears near the beginning of a class$NL$definition outside of any constructor definitions. The position of such a block can be generalised in the$NL$following simple template for a typical class definition:$NL$An initialisation block is executed as if it were placed at the beginning of every constructor of a class. In$NL$other words, it represents a common block of code that every constructor executes.$NL$Thus far, in this study guide, we have only been able to work with single values of primitive data types$NL$and object references. In the next chapter, we will find out how we can associate multiple values of types$NL$with a single variable so that we can work with multiple values of primitives or object references in$NL$an application.
14;While there is a study guide (available from Ventus) that focuses largely on objects and their$NL$characteristics, it will be instructive to the learner (of the Java programming language) to understand how$NL$the concept of an object is applied to their construction and use in Java applications. Therefore, Chapter$NL$One (of this guide) introduces the concept of an object from a language-independent point of view and$NL$examines the essential concepts associated with object-oriented programming (OOP) by briefly comparing$NL$how OOP and non-OOP approach the representation of data and information in an application. The$NL$chapter goes on to explain classes, objects and messages and concludes with an explanation of how a class$NL$is described with a special diagram known as a class diagram.$NL$$NL$Despite the wide use of OOP languages such as Java, C++ and C#, non-OOP languages continue to be$NL$used in specific domains such as for some categories of embedded applications. In a conventional,$NL$procedural language such as C, data is sent to a procedure for processing this paradigm of information$NL$processing is illustrated in Figure 1.1 below.$NL$$NL$The figure shows that the number 4 is passed to the function (SQRT) which is ‘programmed’ to calculate$NL$the result and output it (to the user of the procedure). In general, we can think of each procedure in an$NL$application as ready and waiting for data items to be sent to them so that they can do whatever they are$NL$programmed to do on behalf of the user of the application. Thus an application written in C will typically$NL$comprise a number of procedures along with ways and means to pass data items to them.$NL$$NL$The way in which OOP languages process data, on the other hand, can be thought of as the inverse of the$NL$procedural paradigm. Consider Figure 1.2 below.$NL$$NL$In the figure, the data item – the number 4 – is represented by the box (with the label ‘4’ on its front face).$NL$This representation of the number 4 can be referred to as the object of the number 4. This simple object$NL$doesn’t merely represent the number 4, it includes a button labeled sqrt which, when pressed, produces$NL$the result that emerges from the slot labeled return.$NL$Whilst it is obvious that the object-oriented example is expected to produce the same result as that for the$NL$procedural example, it is apparent that the way in which the result is produced is entirely different when$NL$the object-oriented paradigm considered. In short, the latter approach to producing the result 2 can be$NL$expressed as follows.$NL$$NL$A message is sent to the object to tell it what to do. Other messages might press other buttons associated$NL$with the object. However for the present purposes, the object that represents the number 4 is a very simple$NL$one in that it has only one button associated with it. The result of sending a message to the object to press$NL$its one and only button ‘returns’ another object. Hence in Figure 1.2, the result that emerges from the$NL$‘return’ slot - the number 2 – is an object in its own right with its own set of buttons.$NL$Despite the apparent simplicity of the way in which the object works, the question remains: how does it$NL$calculate the square root of itself? The answer to this question enshrines the fundamental concept$NL$associated with objects, which is to say that objects carry their programming code around with them.$NL$Applying this concept to the object shown in Figure 1.2, it has a button which gives access to the$NL$programming code which calculates the square root (of the number represented by the object). This$NL$amalgam of data and code is further illustrated by an enhanced version of the object shown in Figure$NL$1.3 below.$NL$$NL$The enhanced object (representing the number 4) has two buttons: one to calculate the square root of itself$NL$– as before - and a second button that adds a number to the object. In the figure, a message is sent to the$NL$object to press the second button – the button labeled ‘+’ – to add the object that represents the number 3$NL$to the object that represents the number 4. For the ‘+’ button to work, it requires a data item to be sent to it$NL$as part of the message to the object. This is the reason why the ‘+’ button is provided with a slot into$NL$which the object representing the number 3 is passed. The format of the message shown in the figure can$NL$be expressed as follows.$NL$$NL$When this message is received and processed by the object, it returns an object that represents the number$NL$7. In this case, the message has accessed the code associated with the ‘+’ button. The enhanced object can$NL$be thought of as having two buttons, each of which is associated with its own programming code that is$NL$available to users of the object.$NL$$NL$Extrapolating the principal of sending messages to the object depicted in Figure 1.3 gives rise to the$NL$notion that an object can be thought of as comprising a set of buttons that provide access to operations$NL$which are carried out depending on the details in the messages sent to that object.$NL$$NL$In summary:$NL$􀁸in procedural programming languages, data is sent to a procedure$NL$􀁸in an object-oriented programming language, messages are sent to an object$NL$􀁸an object can be thought of as an amalgam of data and programming code: this is known as$NL$encapsulation.$NL$$NL$Whilst the concept of encapsulation is likely to appear rather strange to learners who are new to OOP,$NL$working with objects is a much more natural way of designing applications compared to designing them$NL$with procedures. Objects can be constructed to represent anything in the world around us and, as such,$NL$they can be easily re-used or modified. Given that we are surrounded by things or objects in the world$NL$around us, it seems natural and logical that we express this in our programming paradigm.$NL$$NL$The next section takes the fundamental concepts explored in this section and applies them to a simple$NL$object.
14;The aim of Chapter Two is to take the simple class diagram shown at the end of Chapter One and explain$NL$how it is translated into Java source code. The code is explained in terms of its attributes, constructor and$NL$behaviour and a test class is used to explain how its constructor and behaviour elements are used.$NL$Before we embark on our first Java programme, let us recall the class diagram with which we concluded$NL$Chapter One. The class diagram is reproduced in Figure 2.1 below, with the omission of the constructor:$NL$this is to keep the code simple to begin with. We will replace the constructor in the class diagram and$NL$provide code for it later in this chapter.$NL$In Figure 2.1, let us be reminded that the qualifier ‘-‘ means private and the qualifier ‘+’ means public.$NL$The purpose of these qualifiers will be revealed when we write the code for the class.$NL$The next section explains how the information in the class diagram shown in Figure 2.1 is translated into$NL$Java source code.$NL$Remember that, in general, a class definition declares attributes and defines constructors and behaviour.$NL$The Java developer concentrates on writing types called classes, as a result of interpreting class diagrams$NL$and other elements of the OOA & D of an application’s domain. The Java developer also makes extensive$NL$use of the thousands of classes provided by the originators of the Java language (Sun Microsystems Inc.)$NL$that are documented in the Java Applications Programming Interface (API).$NL$We have established that classes typically comprise attributes and the behaviour that is used to manipulate$NL$these data. Attributes are implemented, in Java, as variables, whose value determines the condition or$NL$state of an object of that class and behaviour elements are implemented using a construct known as a$NL$method. When a method is executed, it is said to be called or invoked.$NL$As has been mentioned earlier, an instance of a class is also called an object, such that, perhaps somewhat$NL$confusingly, the terms instance and object are interchangeable in Java. The requirement to create an$NL$instance of a class from the definition of the class gives rise to a fundamental question: how do we actually$NL$create an instance of a class so that its methods can be executed? We will address this question in$NL$this section.$NL$One of the components of a class, which we haven’t explained fully so far in the discussion of the Member$NL$class, is its constructor. A constructor is used to create or construct an instance of that class. Object$NL$construction is required so that the Java run-time environment (JRE) can respond to a call to an object’s$NL$constructor to create an actual object and store it in memory. An instance does not exist in memory until$NL$its constructor is called only its class definition is loaded by the (JRE). We will meet the constructor for$NL$the Member class later.$NL$Broadly, then, we can think of the Java developer as writing Java classes, from which objects can be$NL$constructed (by calling their constructors). Classes are to objects as an architect’s plan is to a house, i.e.$NL$we can produce many houses from a single plan and we can construct or instantiate many instances from a$NL$single template known as a class. Given that objects can communicate with other objects, this gives the$NL$developer the means to re-use classes from one application in another application. Therefore, with Java$NL$object technology, we can build software applications by combining re-useable and interchangeable$NL$objects, some of which can be standardised in terms of their interface. This is probably the single-most$NL$important advantage of object-oriented programming (OOP) compared with non-OOP in$NL$application development.$NL$We are now at the stage when we can translate the class diagram for the Member class into Java source$NL$code, often shortened to ‘code’. The code that follows is the class definition of the class named Member$NL$but includes only some of the attributes and methods that do not involve object types: this is to keep the$NL$example straightforward. The reason for this restriction is that if we were to declare attributes or$NL$parameters of the MembershipCard class type in the class Member, as required by the class diagram, the$NL$Java compiler would look for the class definition of the class MembershipCard. In order to keep the$NL$example straightforward, we will only write the class definition for the class Member for the time being$NL$we will refer to the class definition of the class MembershipCard in a later chapter. Thus, in this section,$NL$we will work with a single class that includes only primitive data types there are no class types included$NL$in the simplified class diagram.$NL$In order to make the example code even more straightforward, the class diagram is further simplified as$NL$shown in the next diagram. The class diagram that we will translate into Java code declares two variables$NL$and their corresponding ‘setter’ (or mutator) and ‘getter’ (or accessor) methods, as follows.$NL$The reason for the simplification (of the full class diagram) is so that the class definition can be more$NL$easily understood, compared to its full definition. In short, we well keep our first Java programme as$NL$simple as possible.$NL$In the class definition that follows below, ‘ // ‘ is a single-line comment and ‘ /** … */ ‘ is a block$NL$comment and, as such, are ignored by the Java compiler. For the purposes of the example, Java statements$NL$are written in bold and comments in normal typeface.$NL$// Class definition for the class diagram shown in Figure 2.2. Note that the name of$NL$// the class starts, by convention, with a capital letter and that it is declared as public.$NL$// The first Java statement is the class declaration. Note that the words public and$NL$// class must begin with a lower case letter.$NL$public class Member { // The class declaration.$NL$// Declare instance variables first. Things to note:$NL$// String types in Java are objects and are declared as ‘String’, not ‘string’.$NL$// The qualifier 'private' is used for variables.$NL$// 'String' is a type and 'userName' and ‘password’ are variable names, also$NL$// known as identifiers. Thus, we write the following:
14;By now the learner will be familiar, to some extent, with method invocation from earlier chapters, when$NL$objects of the Member class in the themed application are used to give some examples of passing$NL$arguments to methods. Chapter Four goes into more detail about methods and gives a further explanation$NL$about how methods are defined and used. Examples from the themed application are used to illustrate the$NL$principal concepts associated with an object’s methods.$NL$Chapter Three examines an object’s variables, i.e. its state or what it knows what its values are. An$NL$object’s methods represent the behaviour of an object, or what is knows what it can do, and surround, or$NL$encapsulate, an object’s variables. This section answers the question about how we get computable values$NL$into methods.$NL$As we know from previous chapters, a method is invoked by selecting the object reference for the instance$NL$required. The general syntax of a method invocation can be summarised as follows.$NL$Referring, again, to the Member class of the themed application, we could instantiate a number of Member$NL$objects (in a main method) and call their methods as in the following code snippet.$NL$// Instantiate three members call the no-arguments constructor for the Member class.$NL$// Call one of the set methods of these objects.$NL$// Call one of the get methods of these objects in a print statement.$NL$The screen output from executing this fragment of main is:$NL$In short, we must ensure that we know which method we are calling on which object and in which order.$NL$In the code snippet above, it is evident that setUserName expects a String argument to be passed to it this$NL$is because its definition is written as:$NL$The single parameter is replaced by a computable value, i.e. an argument, when the method is invoked.$NL$􀁸The general syntax of a method’s declaration is modifier return_type$NL$method_name( parameter_list ) exception_list$NL$􀁸The method’s definition is its declaration, together with the body of the method’s$NL$implementation between braces, as follows:$NL$􀁸The method’s signature is its name and parameter list.$NL$It is in the body of a method where application logic is executed, using statements such as:$NL$􀁸invocations: calls to other methods$NL$􀁸assignments: changes to the values of fields or local variables$NL$􀁸selection: cause a branch$NL$􀁸repetition: cause a loop$NL$􀁸detect exceptions, i.e. error conditions.$NL$If the identifier of a parameter is the same as that of an instance variable, the former is said to hide the$NL$latter. The compiler is able to distinguish between the two identifiers by the use of the keyword ‘this’, as$NL$in the following method definition that we met in Chapter One:$NL$If, on the other hand, we wish to avoid hiding, we could write the method definition as follows:$NL$where the identifier of the parameter is deliberately chosen to be different from that of the instance$NL$variable. In this case, the keyword ‘this’ can be included but it is not necessary to do so.$NL$In both versions of the method setUserName, the value of the parameter’s argument has scope only within$NL$the body of the method. Thus, in general, arguments cease to exist when a method completes its execution.$NL$A final point to make concerning arguments is that a method cannot be passed as an argument to another$NL$method or a constructor. Instead, an object reference is passed to the method or constructor so that the$NL$object reference is made available to that method or constructor or to other members of the class that$NL$invoke that method. For example, consider the following code snippet from the graphical version of the$NL$themed application shown on the next page.$NL$The examples and discussion in this section are meant to raise a question in the mind of the learner: are$NL$arguments passed by value or by reference? This question is addressed in the next sub-section.$NL$All arguments to methods (and constructors) are, in Java, passed by value. This means that a copy of the$NL$argument is passed in to a method (or a constructor) call.$NL$The example that follows aims to illustrate what pass by value semantics means in practice: detailed code$NL$documentation is omitted for the sake of clarity.$NL$The method changeValue changes the value of the argument passed to it – a copy of x – but it does not$NL$change the original value of x, as shown by the output. Thus the integer values 1235 and 1234 are output$NL$according to the semantics of pass by value as they apply to arguments.$NL$When a parameter is an object reference, it is a copy of the object reference that is passed to the method.$NL$You can change which object the argument refers to inside the method, without affecting the original$NL$object reference that was passed. However if the body of the method calls methods of the original object –$NL$via the copy of its reference - that change the state of the object, the object’s state is changed for the$NL$duration of its scope in a programme.$NL$Thus, in the example above, the strings “Bonjour” and “Hello there!” are output according to the$NL$semantics of pass by value as they apply to object references.$NL$A common misconception about passing object references to methods or constructors is that Java uses$NL$pass by reference semantics. This is incorrect: pass by reference would mean that if used by Java, the$NL$original reference to the object would be passed to the method or constructor, rather than a copy of the$NL$reference, as is the case in Java. The Java language passes object references by value, in that a copy of the$NL$object reference is passed to the method or constructor.$NL$The statement in the box isn’t true when objects are passed amongst objects in a distributed application.$NL$However, such applications are beyond the scope of this guide. For the purposes of the present guide,$NL$the learner should use the examples above to understand the consequences of Java’s use of pass by$NL$value semantics.$NL$In previous chapters, we have encountered a number of references to a method’s return type. In the$NL$definition of a method, the return type is declared as part of the method’s declaration and its value is$NL$returned by the final statement of the method.
15;While we focus on white-collar financial crime in this book on computer crime, we must not forget that there are a number of other types of crime that are typical for cyber crime and Internet crime as well. Typical examples are hacking, child pornography and online child grooming. In this chapter, we present the case of child grooming as computer crime. Internet use has grown considerably in the last decade. Information technology now forms a core part of the formal education system in many countries, ensuring that each new generation of Internet users is more adept than the last. Research studies in the UK suggest that the majority of young people aged 9-19 accessed the Internet at least once a day. The Internet provides the opportunity to interact with friends on social networking sites such as Myspace and Bebo and enables young people to access information in a way that previous generations would not have thought possible. The medium also allows users to post detailed personal information, which may be accessed by any site visitor and provides a platform for peer communication hitherto unknown (Davidson and Martellozzo, 2008). There is, however, increasing evidence that the Internet is used by some adults to access children and young people in order to groom them for the purposes of sexual abuse. Myspace have recently expelled 29,000 suspected sex offenders and is being sued in the United States by parents who claim that their children were contacted by sex offenders on the site and consequently abused (BBC, 2007). The Internet also plays a role in facilitating the production and distribution of indecent illegal images of children, which may encourage and complement online grooming. $NL$$NL$Recent advances in computer technology have been aiding sexual sex offenders, stalkers, child pornographers, child traffickers, and others with the intent of exploiting children (Kierkegaard, 2008: 41): Internet bulletin boards, chat rooms, private websites, and peer-to-peer networks are being used daily by pedophiles to meet unsuspecting children. Compounding the problem is the lack of direct governance by an international body, which will curb the illegal content and activity. Most countries already have laws protecting children, but what is needed is a concerted law enforcement and international legislation to combat child sex abuse. Men who target young people online for sex are pedophiles (Kierkegaard, 2008 Wolak et al., 2008). According to Dunaigre (2001), the pedophile is an emblematic figure, made into a caricature and imbued with all the fears, anxieties and apprehensions rocking our society today. Pedophile acts are - according to the World Health Organization (WHO) - sexual behavior that an adult major (16 years or over), overwhelmingly of the male sex, acts out towards prepubescent children (13 years or under). According to the WHO, there must normally be a five-year age difference between the two, except in the case of pedophilic practices at the end of adolescence where what counts is more the difference in sexual maturity. However, the definition of criminal behavior varies among countries. As will become evident from reading this article, pedophile acts in Norway are sexual behavior that a person acts out towards children of 16 years or under. There is no minimum age definition for the grooming person in Norwegian criminal law, but age difference and difference in sexual maturity is included as criteria for criminal liability. $NL$$NL$Wolak et al. (2009: 4) present two case examples of crimes by online sex offenders in the United States: • Police in West Coast state found child pornography in the possession of the 22-year-old offender. The offender, who was from a North-eastern state, confessed to befriending a 13-year-old local boy online, traveling to the West Coast, and meeting him for sex. Prior to the meeting, the offender and victim had corresponded online for about six months. The offender had sent the victim nude images via web cam and e-mail and they had called and texted each other hundreds of times. When they met for sex, the offender took graphic pictures of the encounter. The victim believed he was in love with the offender. He lived alone with his father and was struggling to fit in and come to terms with being gay. The offender possessed large quantities of child pornography that he had downloaded from the Internet. He was sentenced to 10 years in prison. • A 24-year-old man met a 14-year-old girl at a social networking site. He claimed to be 19. Their online conversation became romantic and sexual and the victim believed she was in love. They met several times for sex over a period of weeks. The offender took nude pictures of the victim and gave her alcohol and drugs. Her mother and stepfather found out and reported the crime to the police. The victim was lonely, had issues with drugs and alcohol, and problems at school and with her parents. She had posted provocative pictures of herself on her social networking site. She had met other men online and had sex with them. The offender was a suspect in another online enticement case. He was found guilty but had not been sentenced at time of the interview. $NL$$NL$According to Davidson and Martellozzo (2008: 277), Internet sex offender behavior can include: "the construction of sites to be used for the exchange of information, experiences, and indecent images of children the organization of criminal activities that seek to use children for prostitution purposes and that produce indecent images of children at a professional level the organization of criminal activities that promote sexual tourism". Child grooming is a process that commences with sexual sex offenders choosing a target area that is likely to attract children. In the physical world, this could be venues visited by children such as schools, shopping malls or playgrounds. A process of grooming then commences when offenders take a particular interest in the child and make them feel special with the intention of forming a bond. The Internet has greatly facilitated this process in the virtual world. Offenders now seek out their victims by visiting Internet relay chat (IRC) rooms from their home or Internet cafés at any time. Once a child victim is identified, the offender can invite it into a private area of the IRC to engage in private conversations on intimate personal details including the predator's sex life (Australian, 2008).
15;The risk of computer crime has become a global issue affecting almost all countries. Salifu (2008) argues that the Internet is a "double-edged sword" providing many opportunities for individuals and organizations to develop and prosper, but at the same time has brought with it new opportunities to commit crime. For example, Nigeria-related financial crime is extensive and 122 out of 138 countries at an Interpol meeting complained about Nigerian involvement in financial fraud in their countries. The most notorious type attempted daily on office workers all over the world, is the so-called advance fee fraud. The sender will seek to involve the recipient in a scheme to earn millions of dollars if the recipient pays an advance fee (Ampratwum, 2009). Computer crime is an overwhelming problem worldwide. It has brought an array of new crime activities and actors and, consequently, a series of new challenges in the fight against this new threat (Picard, 2009). Policing computer crime is a knowledge-intensive challenge indeed because of the innovative aspect of many kinds of computer crime. Cyberspace presents a challenging new frontier for criminology, police science, law enforcement and policing. Virtual reality and computer-mediated communications challenge the traditional discourse of criminology and police work, introducing new forms of deviance, crime, and social control. Since the 1990s, academics and practitioners have observed how cyberspace has emerged as a new field of criminal activity. Cyberspace is changing the nature and scope of offending and victimization. A new discipline named cyber criminology is emerging. Jaishankar (2007) defines cyber criminology as the study of causation of crimes that occur in the cyberspace and its impact in the physical space. $NL$$NL$Employees of the organization commit most computer crime, and the crime occurs inside company walls (Hagen et al., 2008: Nykodym et al, 2005). However, in our perspective of financial crime introduced in this chapter, we will define computer crime as a profit-oriented crime rather than a damage-oriented crime, thereby excluding the traditional focus of dissatisfied and frustrated employees wanting to harm their own employers. $NL$$NL$Computer crime is defined as any violations of criminal law that involve knowledge of computer technology for their perpetration, investigation, or prosecution (Laudon and Laudon, 2010). The initial role of information and communication technology was to improve the efficiency and effectiveness of organizations. However, the quest of efficiency and effectiveness serves more obscure goals as fraudsters exploit the electronic dimension for personal profits. Computer crime is an overwhelming problem that has brought an array of new crime types (Picard, 2009). Examples of computer-related crimes include sabotage, software piracy, and stealing personal data (Pickett and Pickett, 2002). In computer crime terminology, the term cracker is typically used to denote a hacker with a criminal intent. No one knows the magnitude of the computer crime problem – how many systems are invaded, how many people engage in the practice, or the total economic damage. According to Laudon and Laudon (2010), the most economically damaging kinds of computer crime are denial-of-service attacks, where customer orders might be rerouted to another supplier. Eleven men in five countries carried out one of the worst data thefts for credit card fraud ever (Laudon and Laudon, 2010: 326): In early August 2008, U.S. federal prosecutors charged 11 men in five countries, including the United States, Ukraine, and China, with stealing more than 41 million credit and debit card numbers. This is now the biggest known theft of credit card numbers in history. The thieves focused on major retail chains such as OfficeMax, Barnes & Noble, BJ’s Wholesale Club, the Sports Authority, and T.J. Marxx. The thieves drove around and scanned the wireless networks of these retailers to identify network vulnerabilities and then installed sniffer programs obtained from overseas collaborators. The sniffer programs tapped into the retailers’ networks for processing credit cards, intercepting customers’ debit and credit card numbers and PINs (personal identification numbers). The thieves then sent that information to computers in the Ukraine, Latvia, and the United States. They sold the credit card numbers online and imprinted other stolen numbers on the magnetic stripes of blank cards so they could withdraw thousands of dollars from ATM machines. Albert Gonzales of Miami was identified as a principal organizer of the ring. $NL$$NL$The conspirators began their largest theft in July 2005, when they identified a vulnerable network at a Marshall’s department store in Miami and used it to install a sniffer program on the computers of the chain’s parent company, TJX. They were able to access the central TJX database, which stored customer transactions for T.J. Marxx, Marshalls, HomeGoods, and A.J. Wright stores in the United States and Puerto Rico, and for Winners and HomeSense stores in Canada. Fifteen months later, TJX reported that the intruders had stolen records with up to 45 million credit and debit card numbers. TJX was still using the old Wired Equivalent Privacy (WEP) encryption system, which is relatively easy for hackers to crack. Other companies had switched to the more secure Wi-Fi Protected Access (WPA) standard with more complex encryption, but TJX did not make the change. An auditor later found that TJX had also neglected to install firewalls and data encryption on many of the computers using the wireless network, and did not properly install another layer of security software it had purchased. TJX acknowledged in a Securities and Exchange Commission filing that it transmitted credit card data to banks without encryption, violating credit card company guidelines. Computer crime, often used synonymous with cyber crime, refers to any crime that involves a computer and a network, where the computer has played a part in the commission of a crime. Internet crime, as the third crime label, refers to criminal exploitation of the Internet. In our perspective of profit-oriented crime, crime is facilitated by computer networks or devices, where the primary target is not computer networks and devices, but rather independent of the computer network or device. $NL$$NL$Cyber crime is a term used for attacks on the cyber security infrastructure of business organizations that can have several goals. One goal pursued by criminals is to gain unauthorized access to the target’s sensitive information. Most businesses are vitally dependent on their proprietary information, including new product information, employment records, price lists and sales figures. According to Gallaher et al. (2008), an attacker may derive direct economic benefits from gaining access to and/or selling such information, or may inflict damage on an organization by impacting upon it.
15;Fake websites have become increasingly pervasive and trustworthy in their appearance, generating billions of dollars in fraudulent revenue at the expense of unsuspecting Internet users. Abbasi et al. (2010) found that the growth in profitable fake websites is attributable to several factors, including their authentic appearance, a lack of user awareness regarding them, and the ability of fraudsters to undermine many existing mechanisms for protecting against them. The design and appearance of these websites makes it difficult for users to manually identify them as fake. Distinctions can be made between spoof sites and concocted sites. A spoof site is an imitation of an existing commercial website such as eBay or PayPal. A concocted site is a deceptive website attempting to create the impression of a legitimate, unique and trustworthy entity. Detecting fake websites is difficult. There is a need for both fraud cues as well as problem-specific knowledge. Fraud cues are important design elements of fake websites that may serve as indicators of their lack of authenticity. First, fake websites often use automatic content generation techniques to mass-produce fake web pages. Next, fraud cues include information, navigation, and visual design. Information in terms of web page text often contains fraud cues stemming from information design elements. Navigation in terms of linkage information and URL names for a website can provide relevant fraud cues relating to navigation design characteristics. For example, it is argued that 70 percent of ".biz" domain pages are fake sites. Fake websites frequently use images from existing legitimate or prior fake websites. For example spoof sites copy company logos from the websites they are mimicking. The fact that it is copied can be detected in the system (Abbasi et al., 2010). In addition to fraud cues, there is a need for problem-specific knowledge. Problem-specific knowledge regarding the unique properties of fake websites includes stylistic similarities and content duplication (Abbasi et al., 2010). Abbasi et al. (2010) developed a prototype system for fake website detection. The system is based on statistical learning theory. Statistical learning theory is a computational learning theory that attempts to explain the learning process from a statistical point of view. The researchers conducted a series of experiments, comparing the prototype system against several existing fake website detection systems on a test sample encompassing 900 websites. The results indicate that systems grounded in statistical learning theory can more accurately detect various categories of fake websites by utilizing richer sets of fraud cues in combination with problem-specific knowledge. A variation of fake websites is fraudulent email solicitation where the sender of an email claims an association with known and reputable corporations or organizational entities. For example, one email from the "Microsoft/AOL Award Team" notified its winners of a sweepstake by stating, "The prestigious Microsoft and AOL has set out and successfully organized a Sweepstakes marking the end of year anniversary we rolled out over 100,000.000.00 for our new year Anniversary Draw" (Nhan et al., 2009). The email proceeded to ask for the potential victim's personal information. $NL$$NL$Nhan et al. (2009) examined 476 fraudulent email solicitations, and found that the three most frequently alleged organizational associations were Microsoft, America Online, and PayPal. Fraudsters also attempt to establish trust through associating with credit-issuing financial corporations and authoritative organizations and groups. $NL$$NL$Money laundering is an important activity for most criminal activity (Abramova, 2007 Council of Europe, 2007 Elvins, 2003). Money laundering means the securing of the proceeds of a criminal act. The proceeds must be integrated into the legal economy before the perpetrators can use it. The purpose of laundering is to make it appear as if the proceeds were acquired legally, as well as disguises its illegal origins (Financial Intelligence Unit, 2008). Money laundering takes place within all types of profit-motivated crime, such as embezzlement, fraud, misappropriation, corruption, robbery, distribution of narcotic drugs and trafficking in human beings (Økokrim, 2008). Money laundering has often been characterized as a three-stage process that requires (1) moving the funds from direct association with the crime, (2) disguising the trail to foil pursuit, and (3) making them available to the criminal once again with their occupational and geographic origins hidden from view. The first stage is the most risky one for the criminals, since money from crime is introduced into the financial system. Stage 1 is often called the placement stage. Stage 2 is often called the layering stage, in which money is moved in order to disguise or remove direct links to the offence committed. The money may be channeled through several transactions, which could involve a number of accounts, financial institutions, companies and funs as well as the use of professionals such as lawyers, brokers and consultants as intermediaries. Stage 3 is often called the integration stage, where a legitimate basis for asset origin has been created. The money is made available to the criminal and can be used freely for private consumption, luxury purchases, real estate investment or investment in legal businesses. Money laundering has also been described as a five-stage process: placement, layering, integration, justification, and embedding (Stedje, 2004). It has also been suggested that money laundering falls outside of the category of financial crime. Since money-laundering activities may use the same financial system that is used for the perpetration of core financial crime, its overlap with the latter is apparent (Stedje, 2004). According to Joyce (2005), criminal money is frequently removed from the country in which the crime occurred to be cycled through the international payment system to obscure any audit trail. The third stage of money laundering is done in different ways. For example, a credit card might be issued by offshore banks, casino 'winning' can be cashed out, capital gains on option and stock trading might occur, and real estate sale might cause profit. $NL$$NL$The proceeds of criminal acts could be generated from organized crime such as drug trafficking, people smuggling, people trafficking, proceeds from robberies or money acquired by embezzlement, tax evasion, fraud, abuse of company structures, insider trading or corruption. The Financial Intelligence Unit (2008) in Norway argues that most criminal acts are motivated by profit. When crime generates significant proceeds, the perpetrators need to find a way to control the assets without attracting attention to them selves or the offence committed. Thus, the money laundering process is decisive in order to enjoy the proceeds without arousing suspicion.
15;When a business enterprise is the potential victim of computer crime, there are a number of measures that can be implemented to protect the business. In the survey by Hagen et al. (2008), they addressed both breath and depth in defense strategies. Depth is concerned with technological as well as organizational measures, while depth is concerned with dimensions of prevention, emergency preparedness and detection. The survey addressed the use of a broad range of technical security measures relating to access control and protection of data. Technical security measures include prevention (password, physical zones, biometric authentication, and software update), emergency (backup), and detection (intrusion detection and antivirus software). Organizational security measures include prevention (access rights and user guidelines), emergency (management plans), detection (log reviews), and incident response (management reports). The survey showed that the use of personal passwords is widespread among all enterprises, even the smallest ones (Hagen et al., 2008: 364): The trend is that the use of a variety of access control mechanisms increases with enterprise size. There is also a clear tendency that large enterprises implement more and a wider range of emergency preparedness and detection measures. The findings show that small enterprises should strengthen their access control and data protection measures, in addition to security routines. Hagen et al. (2008) found it surprising that large enterprises did not perform better than small enterprises when it comes to awareness raising and education of users as organizational security measures. $NL$$NL$Profiling of criminals is based on the idea that an individual committing crime in cyberspace using a computer can fit a certain outline or profile. A profile consists of offender characteristics that represent assumptions of the offender’s personality and behavioral appearance. Characteristics can include physical build, offender sex, work ethic, mode of transportation, criminal history, skill level, race, marital status, passiveness/aggressiveness, medical history, and offender residence in relation to the crime (Nykodym et al., 2005). Nykodym et al. (2005: 413) make distinctions between four main categories of cyber crime: espionage, theft, sabotage, and personal abuse of the organizational network: Unlike saboteurs and spies, the thief is guided only by mercantile motives for his own gain. The only goal in front of the cyber thief is to steal valuable information from an organization and use it or sell it afterwards for money. In terms of criminal profiling, Nykodym et al. (2005) found that there is a strong pattern in the age of these cyber robbers. If the crime is for less than one hundred thousand dollars, then most likely the attacker is young 20-25 years old, male or female, still in the low hierarchy of the organization. If the crime involves more money, then the committer is probably an older male from a management level in the organization. His crime is not driven by hate or revenge but by greed and hunger for money. $NL$$NL$Computer crime is defined as financial crime in this book. White-collar criminals commit financial crime. Characteristics of white-collar criminals include: • Wealthy yet greedy person • Highly educated yet practical person • Socially connected yet anti-social person • Talks ethics yet acts immoral • Employed by and in a legitimate organization • A person of respectability with high social status • Member of the privileged socioeconomic class • Commit crime within the occupation based on competence • On the slippery slope from legitimate to illegitimate behavior • Often charismatic, convincing and socially skilled • So desperate to succeed that they are willing to use criminal means • Sometimes excited about the thrill of not being uncovered • Often in a position where the police is reluctant to start investigation • Applies resources to hide tracks and crime outcome • Behaves in court in a manner creating sympathy and understanding $NL$$NL$These kinds of characteristics are organized according to criteria in criminal profiling. For example, some of them are individual factors that are grounded in psychology, while others are environmental factors grounded in sociology. In terms of psychological factors, criminal profiling may ask question such as: • What kind of personality types become more easily white-collar criminals? • What are their typical background, life style and development? • What are their values, ideas and ambitions? In terms of sociological factors, criminal profiling may ask questions such as: • How do white-collar criminals look at society and their own role in society? • How do they perceive laws, and what do they consider to be crime and criminals? • How do they participate in networks, and what is associated with status and power? Not all computer criminals are white-collar criminals, but most of them are committing crime for financial gain. Cyber offenders are likely to share a broader range of social characteristics, and the cases of hacking and other Internet-related offences that have been reported in the media would suggest they are likely to be young, clever and fairly lonely individuals who are of middle-class origin, often without prior criminal records, often processing expert knowledge and often motivated by a variety of financial and non-financial goals. Some degree of technical competence is required to commit many computer-related types of crime (Salifu, 2008). $NL$$NL$Some theorists believe that crime can be reduced through the use of deterrents. The goal of deterrence, crime prevention, is based on the assumption that criminals or potential criminals will think carefully before committing a crime if the likelihood of getting caught and/or the fear of swift and severe punishment are present. Based on such belief, general deterrence theory holds that crime can be thwarted by the threat of punishment, while special deterrence theory holds that penalties for criminal acts should be sufficiently severe that convicted criminals will never repeat their acts (Lyman and Potter, 2007). Threat is an external stimulus that exists whether or not an individual perceives it (Johnson and Warkentin, 2010). If an individual perceives the threat, then is has deterrent potential. Deterrence theory postulates that people commit such crimes on the basis of rational calculations about perceived personal benefits, and that the threat of legal sanctions will deter people for fear of punishment (Yusuf and Babalola, 2009). In more recent years when executives have been seen arrested and handcuffed for the purposes of public humiliation, it sets in motion a deterrence model of crime prevention or at the very least, a shaming policy. The purpose of these public arrests are often symbolic and say more about the regulatory agencies need to appear to be legitimately prosecuting corporate wrongdoers. As such, with regulation so closely tied to the political climate, there has been no consistency in the prosecution of corporate criminals, as compared with drug war policies of the past couple of decades (Hansen, 2009).
15;Cyber crime investigations have both similarities and differences when compared to traditional crime such as burglary and robbery. Traditional crime generally concern personal or property offences that law enforcement has continued to combat for centuries. Cyber crime is characterized by being technologically advanced, it can occur almost instantaneously, and it is extremely difficult to observe, detect, or track. These problems are compounded by the relative anonymity afforded by the Internet as well as the transcendence of geographical and physical limitations in cyberspace. Criminals are able to take advantage of a virtually limitless pool of potential victims (Hinduja, 2007). Policing financial crime generally – according to Pickett and Pickett (2002) – is concerned with whistle blowing and detection, roles of shareholders and main board and chief executive officer and senior executives, investigations, forensics. Policing financial crime – according to Levi (2007) – is concerned with the organization of policing deception, the contexts of police undercover work, covert investigations of white-collar crime, prosecution and relationship to policing fraud. Covert activity is restricted mainly to the informal obtaining of financial information or the official obtaining of information about suspected bank accounts without the knowledge of the account-holder. Policing cyber crime is concerned with all these issues as well as a tight surveillance of relevant activities on the Internet. Within crime investigations, IT forensics and cyber crime investigations are an extremely complicated field (Callanan and Jones, 2009). Kao and Wang (2009) suggest an approach to improving cyber crime investigation consisting of three stages: independent verification of digital clues, corresponding information from different sources, and preparation of a valid argument. Furthermore, covert investigations in the workplace represent a debated practice when investigating financial crime (Tackett, 2008). $NL$$NL$Investigation and prevention of cyber crime and building corporate reputation have the value configuration of a value shop. As can be seen in Figure 1, the five activities of a value shop are interlocking and while they follow a logical sequence, much like the management of any project, the difference from a knowledge management perspective is the way in which knowledge is used as a resource to create value in terms of results for the organization. Hence, the logic of the five interlocking value shop activities in this example is of a policing unit and how it engages in its core business of conducting reactive and proactive investigations. $NL$$NL$The sequence of activities starts with problem understanding, moves into alternative investigation approaches, investigation decision, and investigation implementation, and ends up with criminal investigation evaluation (Sheehan and Stabell, 2007). However, these five sequential activities tend to overlap and link back to earlier activities, especially in relation to activity 5 (control and evaluation) in policing units when the need for control and command structures are a daily necessity because of the legal obligations that policing unit authority entails. Hence, the diagram is meant to illustrate the reiterative and cyclical nature of these five primary activities for managing the knowledge collected during and applied to a specific investigation in a value shop manner. Furthermore, Figure 1 illustrates the expanding domain of the knowledge work performed in financial crime investigations, starting in the centre with problem understanding and ending at the edge with evaluation of all parts of the investigation process. These five primary activities of the value shop in relation to a financial crime investigation and prevention unit can be outlined as (Sheehan and Stabell, 2007): 1. Problem Definition. This involves working with parties to determine the exact nature of the crime and hence how it will be defined. For example, a physical assault in a domestic violence situation depending on how the responding officers choose and/or perceive to define it can be either upgraded to the status of grievous bodily harm to the female spouse victim or it may be downgraded to a less serious common, garden variety assault where a bit of rough handing took place towards the spouse. This concept of making crime, a term used on how detectives choose to make incidents into a crime or not, is highly relevant here and is why this first activity has been changed from the original problem finding term used in the business management realm to a problem definition process here in relation to policing work. Moreover, this first investigative activity involves deciding on the overall investigative approach for the case not only in terms of information acquisition but also as indicated on Figure 1 in undertaking the key task, usually by a senior investigative officer in a serious or major incident, of forming an appropriate investigative team to handle the case. 2. Investigation Approaches. This second activity of identifying problem solving approaches involves the actual generation of ideas and action plans for the investigation. As such it is a key process for it sets the direction and tone of the investigation and is very much influenced by the composition of the members of the investigative team. For example, the experience level of investigators and their preferred investigative thinking style might be a critical success factor in this second primary activity of the value shop. $NL$$NL$3. Approach Decision. This solution choice activity represents the decision of choosing between alternatives generated in the second activity. While the least important primary activity of the value shop in terms of time and effort, it might be the most important in terms of value. In this case, trying to ensure as far as is possible that what is decided on to do is the best option to follow to get an effective investigative result. A successful solution choice is dependent on two requirements. First, alternative investigation steps were identified in the problem solving approaches activity. It is important to think in terms of alternatives. Otherwise, no choices can be made. Next, criteria for decision-making have to be known and applied to the specific investigation. 4. Investigation Implementation. As the name implies, solution execution represents communicating, organizing, investigating, and implementing decisions. This is an equally important process or phase in an investigation as it involves sorting out from the mass of information coming into the incident room about a case and directing the lines of enquiry as well as establishing the criteria used to eliminate a possible suspect from further scrutiny in the investigation. A miscalculation here can stall or even ruin the whole investigation. Most of the resources spent on an investigation are used here in this fourth activity of the value shop.
16;Programming, in any language, involves creating named entities within the machine and manipulating them – using their values to calculate the value for a new entity, changing the values of existing entities, and so forth. Some languages recognize many different kinds of entity, and require the programmer to be very explicit and meticulous about “declaring” what entities he will use and what kind each one will be before anything is actually done with them.4 In C, for instance, if a variable represents a number, one must say what kind of number – whether an integer (a whole number) or a “floating-point number” (what in everyday life we call a decimal), and if the latter then to what degree of precision it is recorded. (Mathematically, a decimal may have any number of digits after the decimal point, but computers have to use approximations which round numbers off after some specific number of digits.) Perl is very free and easy about these things. It recognizes essentially just three types of entity: individual items, and two kinds of sets of items – arrays, and hashes. Individual entities are called scalars (for mathematical reasons which we can afford to ignore here – just think of “scalar” as Perl-ese for an individual data item) a scalar can have any kind of value – it can be a whole number, a decimal, a single character, a string of characters (for instance, an English word or sentence) … We have already seen that variable names representing scalars (the only variables we shall be considering for the time being) begin with the $ symbol for arrays and hashes, which we shall discuss in chapters 12 and 17, the corresponding symbols are @ and % respectively. $NL$$NL$Furthermore, Perl does not require us to declare entity names before using them. In the mini-program (1), the scalars $a and $b came into existence when they were assigned values we gave no prior notice that these variable names were going to be used. In program (1), the variable $b ended up with the value 4. But, if we had added a further line: $NL$$NL$then $b would have ceased to stand for a number and begun to stand for a character-string – both are scalars, so Perl is perfectly willing to switch between these different kinds of value. That does not mean that it is a good idea to do this in practice as a programmer you will need to bear in mind what your different variable names are intended to represent, which might be hard to do if some of them switch between numerical and alphabetic values. But the fact that one can do this makes the point that Perl does not force us to be finicky about housekeeping details. Indeed, it is even legal to use a variable’s value before we have given it a value. If line 1.2 of (1) were changed to $b = $a + $c, then $b would be given the sum of 2 plus the previously-unmentioned scalar $c. Because $c has not been given a value by the programmer, its value will be taken as zero (so $b will end up with the value 2). Relying on Perl to initialize our variables in this way is definitely a bad idea – even if we need a particular variable to have the initial value zero, it is much less confusing in the long run to get into the habit of always saying so explicitly. But Perl will not force us to give our variables values before we use them. Because this free-and-easy programming ethos makes it tempting to fall into bad habits, Perl gives us a way of reminding ourselves to avoid them. We ran program (1) with the command: $NL$$NL$The perl command can be modified by various options beginning with hyphens, one of which is -w for “give warnings”. If we ran the program using the command: $NL$$NL$then, when Perl encounters the line $b = $a + $c in which $c is used without having been assigned a value, it will obey the instruction but will also print out a warning: $NL$$NL$If a skilled programmer gets that warning, it is very likely to be because he thinks he has given $c a value but in fact has omitted to do so. And perl -w gives other warnings about things in our code which, while legal, might well be symptoms of programming errors. It is a good idea routinely to use perl -w to run your programs, and to modify the programs in response to warning messages until the warnings no longer appear – even if the programs seem to be giving the right results. $NL$$NL$In program (1) we saw the operator +, which as you would expect takes a pair of numerical values and gives their sum. Likewise - is used as a minus sign. Some further operators (not a complete list, but the ones you are most likely to need) include: * multiplication / division ** exponentiation: 2 ** 3 means 23, i.e. eight These operators apply to numerical values, but others apply to character-strings. Notably, the full stop . represents concatenation (making one string out of two): $NL$$NL$(Beware of possible confusion here. Some programming languages make the plus sign do double duty, to represent concatenation of strings as well as addition of numbers, but in Perl the plus sign is used only for numerical values.) Another string operator is x (the letter x), which is used to concatenate a string with itself a given number of times: "a" x 6 is equivalent to "aaaaaa", "pom" x 3 is equivalent to "pompompom". (And "pom" x 0 would yield the empty string – the length-zero string containing no characters – which is more straightforwardly specified as "".) Note, by the way, that for Perl a single character is just a string of length one – there is no difference, as there is for instance in C, between "a" and 'a', these are equivalent ways of representing the length-one string containing just the character a. However, single and double quotation marks are not always equivalent. Perl uses backslash as an escape character to create codes for string elements which would be awkward to type: for instance, \n represents a newline character, and \t a tab. Between double quotation marks these sequences are interpreted as codes:
16;Sometimes we want to repeat an action, perhaps with variations. One way to do this is with the word for. Suppose we want to print out a hundred lines containing the messages: $NL$$NL$Here is a code snippet which does that: $NL$$NL$The brackets following for contain: a variable created for the purpose of this for loop and given an initial value a condition for repeating the loop and an action to be executed after each pass. The variable $i begins with the value 1, ++$i increments it by one on each pass, and the instruction within the curly brackets is executed for each value of $i until $i reaches 101, when control moves on to whatever follows the closing curly bracket. We saw earlier that, within double quotation marks, a symbol like \n is translated into what it stands for (newline, in this case), rather than being taken literally as the two characters \ followed by n. Similarly, a variable name such as $i is translated into its current value the lines displayed by the code above read e.g. Next number is 3, not Next number is $i. If you really wanted the latter, you would need to “escape” the dollar sign: $NL$$NL$The little examples in earlier chapters often ended with statements such as $NL$$NL$In practice, it would usually be far preferable to write $NL$$NL$so that the result appears on a line of its own, rather than jammed together with the next system prompt. Within the output of the above code snippet, 1 is not a “next” number but the first number. So we might want the message on the first line to read differently. By now, we know various ways to achieve that. Here are two – a straightforward, plodding way, and a more concise way: $NL$$NL$or (quicker to type, though less clear when you come back to it weeks later): $NL$$NL$Another way to set up a repeating loop is the while construction. Here is another code snippet which achieves the same as the two we have just looked at: $NL$$NL$Here, $i is incremented within the loop body, and control falls out of the loop after the pass in which $i begins with the value 99. The while condition reads $i < 100, not $i <= 100: within the curly brackets, $i is incremented before its value is displayed, so if <= had been used in the while line, the lines displayed would have reached 101. The while construction is often used for reading input lines in from a text file, so the next chapter will show us how that is done. $NL$$NL$In general, a file you want to get data into your program from will not necessarily be in the same directory as the program itself it may have to be located by a pathname which could be long and complicated. The structure of pathnames differs between operating systems if you are working in a Unix environment, for instance, the pathname might be something like: $NL$$NL$Whatever pathnames look like in your computing environment, to read data into a Perl program you have to begin by defining a convenient handle which the program will use to stand for that pathname. For instance, if your program will be using only one input file, you might choose the handle INFILE (it is usual to use capitals for filehandles). The code: $NL$$NL$says that, from now until we hit a line close(INFILE), any reference to INFILE in the program will be reading in data from the annualRecords file specified in the pathname. $NL$$NL$Having “opened” a file for input, we use the symbol <> to actually read a line in. Thus: $NL$$NL$will read in a line from the annualRecords file and assign that string of characters as the value of $a. A line from a multi-line file will terminate in one or more line-end characters, and the identity of these may depend on the system which created the file (different operating systems use different line-end characters). Commonly, before doing anything else with the line we will want to convert it into an ordinary string by removing the line-end characters, and the built-in function chomp() does that. This is an example of a function whose main purpose is to change its argument rather than to return a value chomp() does in fact return a value, namely the number of line-end characters found and removed, but programs will often ignore that value – they will say e.g. chomp($line), rather than saying e.g. $n = chomp($line), with follow-up code using the value of $n. (If no filehandle is specified, $a = <> will read in from the keyboard – the program will wait for the user to type a sequence of characters ending in a newline, and will assign that sequence to $a.11) Assuming that we are reading data from a file rather than from the keyboard, what we often want to do is to read in the whole of the input file, line by line, doing something or other with each successive line. An easy way to achieve that is like this: $NL$$NL$The word while tests for the truth of a condition in this case, it tests whether the assignment statement, and hence the expression <INFILE>, is true or false. So long as lines are being read in from the input file, <INFILE> counts as “true”, but when the file is exhausted <INFILE> will give the value “false”. Hence while ($a = <INFILE>) assigns each line of the input file in turn to $a, and ceases reading when there is nothing more to read. (It is a good idea then to include an explicit close(INFILE) statement, though that is not strictly necessary.) Our open … statement assumed that the annualRecords file was waiting ready to be opened at the place identified by the pathname. But, of course, that kind of assumption is liable to be confounded! Even supposing we copied the pathname accurately when we typed out the program, if that was a while ago then perhaps the annualRecords file has subsequently been moved, or even deleted. In practice it is virtually mandatory, whenever we try to open a file, to provide for the possibility that it does not get opened – normally, by using a die statement, which causes the program to terminate after printing a message about the problem encountered. A good way to code the open statement will be:
16;Since its creation in 1987 Perl has become one of the most widely used programming languages. One measure of this is the frequency with which various languages are mentioned in job adverts. The site www.indeed.com monitors trends: in 2010 it shows that the only languages receiving more mentions on job sites are C and its offshoots C++ and C#, Java, and JavaScript. Perl is a general-purpose programming language, but it has outstanding strengths in processing text files: often one can easily achieve in a line or two of Perl code some text-processing task that might take half a page of C or Java. In consequence, Perl is heavily used for computer-centre system admin, and for Web development – Web pages are HTML text files. Another factor in the popularity of Perl is simply that many programmers find it fun to work with. Compared with Perl, other leading languages can feel worthy but tedious. Perl is a language in which it is easy to get started, but – because it offers handy ways to do very many different things – it takes a long time before anyone finishes learning Perl (if they do ever finish). One standard reference, Steven Holzner’s Perl Black Book (second edn, Paraglyph Press, 2001) is about 1300 dense pages long. So, for the beginner, it is important to focus on the core of the language, and avoid being distracted by all the other features which are there, but are not essential in the early stages. This book helps the reader to do that. It covers everything he or she needs to know in order to write successful Perl programs and grow in confidence with the language, while shielding him or her from confusing inessentials.1 Later chapters contain pointers towards various topics which have deliberately been omitted here. When the core of the language has been thoroughly mastered, that will be soon enough to begin broadening one’s knowledge. Many productive Perl programmers have gaps in their awareness of the full range of language features. The book is intended for beginners: readers who are new to Perl, and probably new to computer programming. The book takes care to spell out concepts that would be very familiar to anyone who already has experience of programming in some other language. However, there will be readers who use this book to begin learning Perl, but who have worked with another language in the past. For the benefit of that group, I include occasional brief passages drawing attention to features of Perl that could be confusing to someone with a background in another language. Programming neophytes can skim over those passages. $NL$$NL$The reader I had in mind as I was writing this book was a reader much like myself: someone who is not particularly interested in the fine points of programming languages for their own sake, but who wants to use a programming language because he has work he wants to get done, and programming is a necessary step towards doing it. As it happens, I am a linguist by training, and much of my own working life is spent studying patterns in the way the English language is used in everyday talk. For this I need to write software to analyse files of transcribed tape-recordings, and Perl is a very suitable language to use for this. Often I am well aware that the program I have written is not the most elegant possible solution to some task at hand, but so long as it works correctly I really don’t care. If some geeky type offered to show me how I could eliminate several lines of code, or make my program run twice as fast, by exploiting some little-known feature of the language which would yield a program delivering exactly the same results, I would not be very interested. Too many computing books are written by geeks who lose sight of the fact that, for the rest of us, computers are tools to get work done rather than ends in themselves. Making programs short is good if it makes them easier to grasp and hence easier to get right but if brevity is achieved at the cost of obscurity, it is bad. As for speed: computer programs run so fast that, for most of us, speeding them up further would be pointless. (For every second of time my programs take to run, I probably spend a day thinking about the results they produce.) That does not mean that, in writing this book, I would have been justified in focusing only on those particular elements of Perl which happen to be useful in my own work and ignoring the rest – certainly not. Readers will have their own tasks for which they want to write software, which will often be very different from my tasks and will sometimes make heavy use of aspects of Perl that I rarely exploit. I aim to cover those aspects, as well as the ones which I use frequently. But it does mean that the book is oriented towards Perl programming as a practical tool – rather than as a labyrinth of fascinating intellectual arcana. If, after working through this book, you decide to make serious use of Perl, sooner or later you will need to consult some larger-scale Perl book – one organized more as a reference manual than a teaching introduction. This short book cannot pretend to cover the reference function, but there is a wide choice of books which do. (And of course there are plenty of online reference sources.) Many Perl users will not need to go all the way to Steven Holzner’s 1300-pager quoted above. The manual which I use constantly is a shorter one by the same author, Perl Core Language Little Black Book (second edn, Paraglyph Press, 2004) – I find Holzner’s approach particularly well suited to my own style of learning, but readers whose learning styles differ might find that other titles suit them better. Because the present book deliberately limits the aspects of Perl which it covers, it is important that readers should not fall into the trap of thinking “Doesn’t Perl have a such-and-such function, then? – that sounds like an awkward gap to have to work round”. Whatever such-and-such may be, very likely Perl has got it, but it is one of the things which this book has chosen not to cover.
16;For the purposes of this textbook, I shall assume that you have access to a computer system on which Perl is available, and that you know how to log on to the system and get to a point where the system is displaying a prompt and inviting you to enter a command. Perl is free, and versions are available for all the usual operating systems, so if you are working in a multi-user environment such as a university computer centre then Perl is almost sure to be on your system already. (It would take us too far out of our way to go through the details of installing Perl on a home computer which does not already have it though, if the home computer is a Mac running OS X, it will already have Perl – available from the Terminal utility under Applications  Utilities.) Assuming, then, that you have access to Perl, let us get started by creating and running a very simple program.2 Adding two and two is perhaps as simple as it gets. This could be a very short Perl program indeed, but I’ll offer a slightly longer one which illustrates some basics of the language. First, create a file with the following contents. Use a text editor to create it, not a word-processing application such as Word – files created via WP apps contain a lot of extra, hidden material apart from the wording typed by the user and displayed on the screen, but we need a file containing just the characters shown below and no others. $NL$$NL$Save it under some suitable name – twoandtwo.pl is as good a name as any. The .pl extension is optional – Perl itself does not care about the format of filenames, and it would respond to the program just the same if you called it simply twoandtwo – but some operating systems want to see filename extensions in some circumstances, so it is probably sensible to get in the habit of including .pl in the names of your Perl programs. Your twoandtwo.pl file will contain just what is shown above. But later in this book, when we look at more extended examples of Perl code I shall give them a label in brackets and number the lines, like this: $NL$$NL$These labels will be purely for convenience in discussing the code, for instance I shall write “line 1.3” to identify the line print $b. The labels are not part of what you will type to create a program. However, when your programs grow longer you may find it helpful to create them using an editor which shows line-numbers the error messages generated by the Perl interpreter will use line numbers to identify places where it finds problems. $NL$$NL$In (1), the symbols $a and $b are variables – names for pigeonholes containing values (in this case, numbers). Line 1.1 means “assign the value 2 to the variable $a”. Line 1.2 means “assign the result of adding the value of $a to itself to the variable $b”. Line 1.3 means “display the value of $b”. Note that each instruction (the usual word is statement) ends in a semicolon. To run the program, enter the command $NL$$NL$to which the system will respond (I’ll show system responses in italics) with $NL$$NL$Actually, if your system prompt is, say, %, what you see will be $NL$$NL$– since nothing in the twoandtwo.pl program has told the system to output a newline after displaying the result and before displaying the next prompt. For that matter, nothing in our little program has told the system how much precision to include in displaying the answer rather than responding with 4, some systems might respond with 4.00000000000000 (which is a more precise way of saying the same thing). In due course we shall see how to include extra material in a program to deal with issues like these. For now, the point is that the job in hand has been correctly done. $NL$$NL$If you have typed the code exactly as shown and Perl does not respond correctly (or at all) when you try running it, various system-dependent problems may be to blame. I assume that, where you are working, there will be someone responsible for telling you what is needed to run Perl on your local system. But meanwhile, I can offer two suggestions. It may be that your program needs to tell the system where the Perl interpreter is located (this is likely if you are seeing an error message suggesting that the command perl is not recognized). In that case it is worth trying the following. Include as the first line of your program this “magic line”:3 $NL$$NL$This will not be the right “magic line” for every system, but for many systems it will be. Secondly, if Perl appears to run without generating error messages, but outputs no result, or outputs material suggesting that it stopped reading your program before the end, it may be that your editor is supplying the wrong newline symbols – so that the sequence of lines looks to the system like one long line. That will often lead to problems for instance, if the first line of your program is the above “magic line”, but Perl sees your whole program as one long line, then nothing will happen when you run it, because the Perl interpreter will only begin to operate on the line following the “magic line”. Set your editor to use Unix (decimal 10) newlines. If neither of these solutions works, then, sorry, you really will need to find that computer-support staff member to tell you how to run Perl on the particular system you are working at! $NL$$NL$Let’s now go back to the contents of program (1). One point which may have surprised you about our first program is the dollar signs in the variable names $a and $b. Why not simply name our variables a and b? In many programming languages, these latter names would be fine, but in Perl they are not. One of the rules of Perl is that any variable name must begin with a special character identifying what kind of entity it is, and for individual variables – names for single separate pigeonholes, as opposed to names for whole sets of pigeonholes – the identifying character is a dollar sign.
16;We have seen the word if used to control which instruction is executed next. Commonly, we want to do one thing in one case and another thing in a different case. An if can be followed by an elsif (or more than one elsif), with an else at the end to catch any remaining possibilities: $NL$$NL$When any one of the tests is passed, the remaining tests are ignored if $price is 200, then since 200  100 Perl will print It's expensive, and the message in 4.7 will not be printed even though it is also true that 200 > 0. Curly brackets are used to keep together the block of code to be executed if a test is passed. Notice that (unlike in some programming languages) even if the block contains just a single line of code, that line must still have curly brackets round it. The last statement before the } does not actually have to end in a semicolon, but it is sensible to include one anyway. We might want to modify our code by adding further statements, in which case it would be easy to overlook the need to add a missing semicolon. $NL$$NL$Not everyone sets out the curly brackets on separate lines, as I did in (4) above. Within reason, Perl does not care where in a program we put whitespace (spaces, tabs, and newline characters). Obviously we cannot put a space in the middle of a number – 56237 cannot be written 56 237, or Perl would have no way to tell that it was all one number 8 – and likewise putting a space in the middle of a string within quotation marks turns it into a different string. But we can set the program out on the page however we please: around the basic elements such as numbers, strings, variable names, and brackets of different types, Perl will ignore extra whitespace. Perl will even supply implied spacing in many cases where elements are run together – thus ++ $a can alternatively be written ++$a. Because Perl does not enforce layout conventions (as some languages do), you need to choose some system and use it consistently – so that you can grasp the overall structure of your program listings at a glance. The main question is about how to indent blocks different people use different conventions. First, you need to decide how much space you are going to use for one level of indentation (common choices are one tab, or two spaces). But then, where exactly should the indents go? Perl manuals often put the opening curly bracket on the line which introduces it, indent the contents of the block, and then place the closing curly bracket level with the beginning of that first line: $NL$$NL$This takes fewer lines than other conventions, but it is not particularly easy to read, and it is perhaps illogical in placing the pair of brackets at unrelated positions. Alternatively, one can give both curly brackets lines of their own – in which case they either both line up under the start of the introducing line, or are both indented to align with their contents: $NL$$NL$Whichever convention you choose, if you apply it consistently you can catch and correct programming errors as you type. You may have a block which is indented within a block that is itself indented within a top-level block. When you type what you thought was the final }, if it doesn’t align properly with the item which it ought to line up with in the first line, then something has gone wrong – perhaps one of your opening brackets has not been given a closing partner? $NL$$NL$As for which of the three styles you choose, that is entirely up to you. According to Thomas Plum, a survey of programmers working with the similar language C found a slight majority favouring the last of the three conventions.9 That is the style used in this book. Indenting consistently also has an advantage when, inevitably, one’s program as first written turns out not to run correctly. A common debugging technique is to insert instructions to print out the values of particular variables at key points, so that one can check whether their values are as expected. Once the bugs are found and eliminated, we naturally want to eliminate these diagnostic lines too – we don’t want our program spewing out a lot of irrelevancies when it is running correctly. My practice is to write diagnostic lines unindented, so that they stand out visually in the middle of an indented block, making them easy to locate and delete. $NL$$NL$The reason to adopt a consistent style for program layout is to make it easier for a human programmer to understand what is going on within a sea of program code – the computer itself does not care about the layout. Another aid to human understanding is comments: explanatory notes written by the programmer to himself (or to those who come after him and have to maintain his code) which the machine ignores. In Perl, comments begin with the hash character. A comment can be: $NL$$NL$or it can be added to a line to the right of code intended for the computer: $NL$$NL$Either way, everything from the hash symbol to the end of the line is ignored by the machine. $NL$$NL$Earlier, we saw that Perl has various “operators” represented by mathematical-type symbols. Sometimes these are the same symbols used in familiar school maths, such as + for addition and - for subtraction sometimes they are slightly different symbols adapted to the constraints of computer keyboards, such as * for multiplication and ** for raising to a power and sometimes the symbols represent operations that we do not usually come across in maths lessons, e.g. “.” for concatenation. $NL$$NL$Perl has many more built-in functions that could conveniently be represented by special symbols, though.10 Most are represented by alphabetic codes. For instance, taking the square root of a number is a standard arithmetic operation, but the usual mathematical symbol, √, is nothing like any character in the ASCII character-set, so instead Perl represents it as sqrt.
17;In this chapter we are going to discuss graph search algorithms and applications thereof for finding a minimum$NL$cost path from a start node to the goal node.$NL$The network search problem in Sect. 2.2 (Fig. 2.1) was devoid of any cost information. Let us now assume that$NL$the costs to traverse the edges of the graph in Fig. 2.1 are as indicated in Fig. 3.1.$NL$There are two possible interpretations of the figures in Fig. 3.1: they can be thought of as costs of edge$NL$traversal or, alternatively, as edge lengths. (We prefer the latter interpretation in which case, of course, Fig. 3.1$NL$is not to scale.) The task is to determine a minimum length path connecting s and g, or, more generally,$NL$minimum length paths connecting any two nodes.$NL$The algorithms considered in this chapter assume the knowledge of an heuristic distance measure, H, between$NL$nodes. Values of H for the network in Fig. 3.1 are shown in Table 3.1. They are taken to be the estimated$NL$straight line distances between nodes and may be obtained by drawing the network in Fig. 3.1 to scale and$NL$taking measurements.$NL$Three algorithms will be introduced here: the A–Algorithm, Iterative Deepening A∗ and Iterative Deepening$NL$A∗–_.$NL$An estimated overall cost measure, calculated by the heuristic evaluation function F, will be attached to every$NL$path it is represented as$NL$where G is the actual cost incurred thus far by travelling from the start node to the current node and H, the$NL$heuristic, is the estimated cost of getting from the current node to the goal node. Assume, for example, that$NL$in the network shown in Fig. 3.1 we start in d and want to end up in c. Equation (3.1) then reads for the path$NL$d → s → a (with obvious notation) as follows$NL$We know from Chap. 2 that for blind search algorithms the updating of the agenda is crucial: Breadth First$NL$comes about by appending the list of extended paths to the list of open paths Depth First requires these lists$NL$to be concatenated the other way round.$NL$For the A–Algorithm, the updating of the agenda is equally important. The new agenda is obtained from$NL$the old one in the steps 1 _ and 2 _ below.$NL$1 _$NL$Extend the head of the old agenda to get a list of successor paths. An intermediate, ‘working’ list will be$NL$formed by appending the tail of the old agenda to this list.$NL$2 _$NL$The new agenda is obtained by sorting the paths in the working list from 1 _ in ascending order of their$NL$F–values.$NL$3 _$NL$The steps 1 _ and 2 _ are iterated until the path at the head of the agenda leads to the goal node.$NL$In the example shown in Fig. 3.2, the paths are prefixed by their respective F–values and postfixed by their$NL$respective G–values. Using this notation and the cost information, the example path in (3.2) is now denoted$NL$by 242 − [a, s, d] − 147. Notice that this path also features in Fig. 3.2.$NL$It can be shown (e.g. [23]) that if the heuristic H is admissible, i.e. it never overestimates the actual$NL$minimum distance travelled between two nodes, the A–Algorithm will deliver a minimum cost path if such a$NL$path exists.1In this case the A–Algorithm is referred to as an A∗–Algorithm and is termed admissible. (As the$NL$straight line distance is a minimum, the heuristic defined by Table 3.1 is admissible.)$NL$The predicate a search(+Start,+Goal,-PathFound) in asearches.pl implements the A–Algorithm. A few$NL$salient features of a search/3 will be discussed only for details, the reader is referred to the source code which$NL$broadly follows the pattern of implementation of the blind search algorithms (Fig. 2.15, p. 65 and Fig. 2.20,$NL$p. 69).$NL$The implementation of the A–Algorithm in asearches.pl uses the built-in predicate keysort/2 to implement$NL$step 2 _ (see inset on p. 108).$NL$The module invoking a search/3 should have defined (or imported) the following predicates.$NL$• The connectivity predicate link/2 . For the network search problem, this is imported from links.pl$NL$(Fig. 2.2, p. 49).$NL$• The estimated cost defined by e cost/3 . For the network search problem, this is defined in graph a.pl$NL$by$NL$with dist/3 essentially implementing Table 3.1,$NL$The actual edge costs defined by edge cost/3 . For the network search problem, this is defined in$NL$graph a.pl by$NL$Application of the A–Algorithm to a more substantial example in Sect. 3.2 will reveal that the A–Algorithm$NL$may fail due to excessive memory requirements.2 Clearly, there is scope for improvement.$NL$In the mid 1980s, a new algorithm was conceived by Korf [20] combining the idea of Iterative Deepening$NL$(Sect. 2.6) with a heuristic evaluation function the resulting algorithm is known as Iterative Deepening A∗$NL$(IDA∗).3 The underlying idea is as follows.$NL$• Use Depth First as the ‘core’ of the algorithm.$NL$• Convert the core into a kind of Bounded Depth First Search with the bound (the horizon) now not being$NL$imposed on the length of the paths but on their F-values.$NL$• Finally, imbed this ‘modified’ Bounded Depth First Search into a framework which repeatedly invokes it$NL$with a sequence of increasing bounds. The corresponding sequence of bounds in Iterative Deepening was$NL$defined as a sequence of multiples of some constant increment a unit increment in the model implementation.$NL$The approach here is more sophisticated. Now, in any given phase of the iteration, the next value$NL$of the bound is obtained as the minimum of the F-values of all those paths which had to be ignored in$NL$the present phase. This approach ensures that in the new iteration cycle the least number of paths is$NL$extended.$NL$The pseudocode of IDA∗ won’t be given here it should be possible to reconstruct it from the above informal$NL$description. It can be shown that IDA∗ is admissible under the same assumptions as A∗.$NL$The so-called _–admissible version of IDA∗ (IDA∗–_) is a generalization of IDA∗. It is obtained by extending$NL$the F-horizon to
17;A regular feature in the New Scientist magazine is Enigma, a weekly puzzle entry which readers are invited to$NL$solve. In the 8 February 2003 issue [1] the following puzzle was published.$NL$First, draw a chessboard. Now number the horizontal rows 1, 2, ..., 8, from top to bottom and$NL$number the vertical columns 1, 2, ..., 8, from left to right.You have to put a whole number in each of$NL$the sixty-four squares, subject to the following:$NL$1. No two rows are exactly the same.$NL$2. Each row is equal to one of the columns, but not to the column with the same number as the$NL$row.$NL$3. If N is the largest number you write on the chessboard then you must also write 1, 2, ...,N −1$NL$on the chessboard.$NL$The sum of the sixty-four numbers you write on the chessboard is called your total. What is the$NL$largest total you can obtain?$NL$We are going to solve this puzzle here using Prolog. The solution to be described will illustrate two techniques:$NL$unification and generate-and-test.$NL$Unification is a built-in pattern matching mechanism in Prolog which has been used in [9] for example, the$NL$difference list technique essentially depended on it. For our approach here, unification will again be crucial in$NL$that the proposed method of solution hinges on the availability of built-in unification. It will be used as a kind$NL$of concise symbolic pattern generating facility without which the current approach wouldn’t be viable.$NL$Generate-and-test is easily implemented in Prolog. Prolog’s backtracking mechanism is used to generate candidate$NL$solutions to the problem which then are tested to see whether certain of the problem-specific constraints$NL$are satisfied.$NL$Fig. 1.1 shows a board arrangement with all required constraints satisfied. It is seen that the first requirement$NL$is satisfied since the rows are all distinct. The second condition is also seen to hold whereby rows and columns$NL$are interrelated in the following fashion:$NL$We use the permutation$NL$to denote the corresponding column–to–row transformation. The board also satisfies the latter part of the second$NL$condition since no row is mapped to a column in the same position. In terms of permutations, this requirement$NL$implies that no entry remains fixed these are those permutations which in our context are permissible. 2 The$NL$third condition is obviously also satisfied with N = 6. The board’s total is 301, not the maximum, which, as$NL$we shall see later, is 544.$NL$The solution scheme described below in i–v is based on first generating all feasible solutions (an example of$NL$which was seen in Sect. 1.2) and then choosing a one with the maximum total.$NL$i. Take an admissible permutation, such as π in (1.1).$NL$ii. Find an 8 ×8 matrix with symbolic entries whose rows and columns are interrelated by the permutation$NL$in i. As an example, let us consider for the permutation π two such matrices, M1 and M2, with$NL$M1 and M2 both satisfy conditions 1 and 2. We also observe that the pattern of M2 may be obtained$NL$from that of M1 by specialization (by matching the variables X1 and X6). Thus, any total achievable for$NL$M2 is also achievable for M1. For any given permissible permutation, we can therefore concentrate on$NL$the most general pattern of variables, M. (We term a pattern of variables most general if it cannot be$NL$obtained by specialization from a more general one.) All this is reminiscent of ‘unification’ and the ‘most$NL$general unifier’, and we will indeed be using Prolog’s unification mechanism in this step.$NL$iii. Verify condition 1 for the symbolic matrix M. 3 Once this test is passed, we are sure that also the latter$NL$part of condition 2 is satisfied. 4$NL$iv. We now evaluate the pattern M. If N symbols have been used in M, assign the values 1, ...,N to them$NL$in reverse order by first assigning N to the most frequently occurring symbol, N − 1 to the second most$NL$frequently occurring symbol etc. The total thus achieved will be a maximum for the given pattern M.$NL$v. The problem is finally solved by generating and evaluating all patterns according to i–iv and selecting a$NL$one with the maximum total.$NL$The original formulation from the New Scientist uses a chessboard but the problem can be equally set with$NL$a square board of any size. In our implementation, we shall allow for any board size since this will allow the$NL$limitations of the method employed to be explored.$NL$We write matrices in Prolog as lists of their rows which themselves are lists. Permutations will be represented$NL$by the list of the bottom entries of their two-line representation thus, [2, 3, 1, 5, 6, 7, 8, 4] stands for$NL$π in (1.1).$NL$First, we want to generate all permutations of a list. Let us assume that we want to do this by the predicate$NL$permute(+List,-Perm) and let us see how List = [1, 2, 3, 4] might be permuted. A permuted list, Perm$NL$= [3, 4, 1, 2] say, may be obtained by$NL$• Removing from List the entry E = 3, leaving the reduced list$NL$R = [1, 2, 4]$NL$• Permuting the reduced list R to get P = [4, 1, 2]$NL$• Assembling the permuted list as [E|P] = [3, 4, 1, 2] .$NL$Lists with a single entry are left unchanged. This gives rise to the definition$NL$with the predicate remove one(+List,?Entry,?Reduced) defined by$NL$(Here we remove either the head or an entry from the tail.) For a permutation to be admissible, all entries must$NL$have changed position. We implement this by$NL$To generate a list of N unbound variables, L, we use var list(+N,-L) which is defined in terms of length(-L,+N)$NL$By$NL$Matrices with distinct symbolic entries may now be produced by mapping for$NL$example, a 3 × 2 matrix is obtained by$NL$It is now that Prolog shows its true strength: we use unification to generate symbolic square matrices with$NL$certain patterns.5 For example, we may produce a 3 × 3 symmetric matrix thus
17;Many problems in Artificial Intelligence (AI) can be formulated as network search problems. The crudest$NL$algorithms for solving problems of this kind, the so called blind search algorithms, use the network’s connectivity$NL$information only. We are going to consider examples, applications and Prolog implementations of blind search$NL$algorithms in this chapter.$NL$Since implementing solutions of problems based on search usually involves code of some complexity, modularization$NL$will enhance clarity, code reusability and readibility. In preparation for these more complex tasks in$NL$this chapter, Prolog’s module system will be discussed in the next section.$NL$In some (mostly larger) applications there will be a need to use several input files for a Prolog project. We have$NL$met an example thereof already in Fig. 3.5 of [9, p. 85] where consult/1 was used as a directive to include in$NL$the database definitions of predicates from other than the top level source file. As a result, all predicates thus$NL$defined became visible to the user: had we wished to introduce some further predicates, we would have had to$NL$choose the names so as to avoid those already used. Clearly, there are situations where it is preferable to make$NL$available (that is, to export ) only those predicates to the outside world which will be used by other non-local$NL$predicates and to hide the rest. This can be achieved by the built-in predicates module/2 and use module/1 .$NL$As an illustrative example, consider the network in Fig. 2.1.1 The network connectivity in links.pl is$NL$defined by the predicate link/2 which uses the auxiliary predicate connect/2 (Fig. 2.2).$NL$The first line of links.pl is the module directive indicating that the module name is edges and that the$NL$predicate link/2 is to be exported. All other predicates defined in links.pl (here: connect/2) are local to$NL$the module and (normally) not visible outside this module.$NL$Suppose now that in some other source file, link/2 is used in the definition of some new predicate (Fig. 2.3).$NL$Then, the (visible) predicates from links.pl will be imported by means of the directive$NL$The new predicate thus defined may be used as usual:$NL$In our example, the predicate connect/2 will not be available for use (since it is local to the module edges$NL$that resides in links.pl). A local predicate may be accessed, however, by prefixing its name by the module$NL$name in the following fashion:3$NL$Let us assume that for the network in Fig. 2.1 we want to find a path from the start node s to the goal node$NL$g. The search may be conducted by using the (associated) search tree shown in Fig. 2.4. It is seen that the$NL$search tree is infinite but highly repetitive. The start node s is at the root node (level 0). At level 1, all tree$NL$nodes are labelled by those network nodes which can be reached in one step from the start node. In general, a$NL$node labelled n in the tree at level _ has successor (or child ) nodes labelled s1, s2, . . . if the nodes s1, s2, . . . in$NL$the network can be reached in one step from node n. These successor nodes are said to be at level _ + 1. The$NL$node labelled n is said to be a parent of the nodes s1, s2, . . .. In Fig. 2.4, to avoid repetition, those parts of the$NL$tree which can be generated by expanding a node from some level above have been omitted.$NL$Some Further Terminology$NL$• The connections between the nodes in a network are called links.$NL$• The connections in a tree are called branches.$NL$• In a tree, a node is said to be the ancestor of another if there is a chain of branches (upwards) which$NL$connects the latter node to the former. In a tree, a node is said to be a descendant of another node if the$NL$latter is an ancestor of the former.$NL$In Fig. 2.5 we show, for later reference, the fully developed (and ’pruned’) search tree. It is obtained from$NL$Fig. 2.4 by arranging that in any chain of branches (corresponding to a path in the network) there should be$NL$no two nodes with the same label (implying that in the network no node be visited more than once). All$NL$information pertinent to the present problem is recorded thus in the file links.pl (Fig. 2.2) by link/2. Notice$NL$that the order in which child nodes are generated by link/2 will govern the development of the trees in Figs. 2.4$NL$and 2.5: children of the same node are written down from left to right in the order as they would be obtained$NL$by backtracking for example, the node labelled d at level 1 in Fig. 2.4 is expanded by$NL$(The same may be deduced, of course, by inspection from links.pl, Fig. 2.2.) link/2 will serve as input to$NL$the implementations of the search algorithms to be discussed next.$NL$The most concise and easy to remember illustration of Depth First is by the conduit model (Fig. 2.6). We start$NL$with the search tree in Fig. 2.5 which is assumed to be a network of pipes with inlet at the root node s. The$NL$tree is rotated by 90◦ counterclockwise and connected to a valve which is initially closed. The valve is then$NL$opened and the system is observed as it gets flooded under the influence of gravity. The order in which the$NL$nodes are wetted corresponds to Depth First.$NL$We may be tempted to use Prolog’s backtracking mechanism to furnish a solution by recursion our attempt is$NL$shown in Fig. 2.7.4 However, it turns out that the implementation does not work due to cycling in the network.$NL$The query shown below illustrates the problems arising.$NL$We implement Depth First search incrementally using a new approach. The idea is keeping track of the nodes$NL$to be visited by means of a list, the so called list of open nodes, also called the agenda. This book–keeping$NL$measure will turn out to be amenable to generalization in fact, it will be seen that the various search algorithms$NL$differ only in the way the agenda is updated.
17;Whereas the problems considered thus far were taken from Artificial Intelligence, we are going now to apply$NL$Prolog to problems in text processing.$NL$The present chapter is in three parts.$NL$First, the Prolog implementation is described of a tool for removing from a file sections of text situated$NL$between marker strings. (The tool is therefore a primitive static program slicer [32] and [12].) This tool then is$NL$used in a practical context for removing sample solutions from the LATEX source code of a solved exam script.$NL$It is also shown in this context how SWI-Prolog code can be embedded into a Linux shell script.$NL$The second part addresses the question of how Prolog can be used to generate LATEX code for drawing$NL$parametric curves. Some new features of Prolog will thereby also be introduced.$NL$The final part comprises a sequence of solved Prolog exercises, implementing a tool for drawing families of$NL$parametric curves in LATEX. The exercises are of increasing complexity and finally describe how SWI-Prolog$NL$can interact with Linux through a shell script.$NL$I use LATEX on Linux for preparing examination papers. This is done in the following steps.$NL$1. Create a LATEX source file in a text editor.$NL$2. Translate the LATEX file into a a DVI file.$NL$3. Translate the DVI file into a PDF file.$NL$4. View the PDF file.$NL$These steps are performed for exam.tex by running the Linux commands in Fig. 4.1.1 Upon execution of the$NL$last line in Fig. 4.1, a new window will pop up and the exam paper may be viewed.$NL$External examiners require examination papers with model answers. I create therefore a PDF file with model$NL$solutions in the first instance where answers are appended to each subquestion. The answers are placed between$NL$some marker strings enabling me eventually to locate and remove all text between them when creating the final$NL$LATEX source leading to the printed PDF for students. It is this text removal process which is automated by the$NL$Prolog implementation to be discussed here.$NL$Write a predicate sieve(+Infile,-Outfile,+Startmarker,+Endmarker) of arity 4 for removing all text in$NL$the file named in Infile in between all occurrences of lines starting with text in Startmarker and those$NL$starting with text in Endmarker. The result should be saved in the file named in Outfile . Outfile is without$NL$marker lines. If Outfile already exists, its old version should be overwritten, if it does not exist, it should be$NL$newly created. The file shown in Fig. 4.2 is an example of Infile with the marker phrases ‘water st’ and$NL$‘water e’, say. (The file comprises a random collection of geographical names.) After the Prolog query$NL$the file without_waters will have been created. This is shown in Fig. 4.3.$NL$The main predicate sieve/4 is defined in terms of sieve/2 , both are shown in (P-4.1).$NL$The predicates get line/1 (and its auxiliary get line/2 ), switch off/1 and switch on/1 are defined in$NL$(P-4.2).$NL$For the SWI-Prolog built-ins atom chars/2 and atom codes/2 , the reader is referred respectively to pages$NL$126 and 19 of [9].$NL$Noteworthy are three more built-in predicates used here: the standard Prolog predicates see/1 , seen/0 (respectively$NL$for directing the input stream to a file and redirecting it) and get char/1 for reading a character$NL$the example below illustrates their use by reading the first three characters of the file with_waters in Fig. 4.2.$NL$The predicate get line/1 in (P-4.2) is defined in terms of get line/2 by the accumulator technique. It$NL$reads into its argument the next line from the input stream. Example:$NL$The following observations apply.$NL$1. It is seen from the above query that a line read by get line/1 is represented as a list of the characters$NL$it is composed of.$NL$2. By definition the last character of each line in a file is the new line character ‘\n’. That explains the$NL$line break seen in the above query.$NL$3. Finally (not demonstrated here), each file ends with the end-of-file marker ‘end_of_file’. The$NL$one-entry list [end_of_file] is deemed to be the last line of every file by the definition in (P-4.2).$NL$• The switches switch off/0 and switch on/0 are used, writing respectively switch(off) and switch(on)$NL$in the Prolog database, respectively for removal and retention of lines from the input file.$NL$• The main predicates are sieve/4 and sieve/2 in (P-4.1), the latter defined by recursion and called by$NL$the former.$NL$sieve/4 : this is the top level predicate.$NL$1. Line 2 opens the input file.$NL$2. The goals in lines 3-4 in (P-4.1) make sure that the earlier version of the output file (if there is such$NL$a file) is deleted.$NL$3. In line 5, the new output stream is opened via append/1 3.$NL$4. In line 6, the switch is set to the position (‘off’), anticipating that initially lines will be retained.$NL$5. In line 7, sieve/2 is invoked and processing is carried out.$NL$6. Lines 8 and 9 close respectively output and input.$NL$sieve/2 : this is called from sieve/4 .$NL$1. Lines 14 and 18 contain the most interesting feature of this predicate: append/3 is used in them for$NL$pattern matching. For example, the goal$NL$succeeds if the initial segment of the list Line is Start_List.$NL$2. atom chars/2 is used in sieve/2 to disassemble the start and end markers into lists in preparation$NL$for pattern matching.$NL$3. Notice that the built-in predicate atom codes/2 can be used in two roles as the interactive session$NL$below demonstrates.$NL$In line 16 of (P-4.1), atom codes/2 is used in its first role, i.e. to convert a list of characters to an$NL$atom. This atom is the current line, it is written to the output file.$NL$4. Recursion is stopped in sieve/2 (and control is returned to line 8 of sieve/4 ) when the end-of-file$NL$marker is read (line 15).$NL$Imbed the Prolog implementation from Sect. 4.1.3 into a Linux shell script for providing the same functionality$NL$as the predicate sieve/4 does. The application obtained thereby will run without explicitly having to use the$NL$SWI-Prolog system. The intended behaviour of the script is illustrated in Fig. 4.4.
18;As mentioned previously a major reason for wishing to obtain a mathematical model of a device$NL$is to be able to evaluate the output in response to a given input. Using the transfer function and$NL$Laplace transforms provides a particularly elegant way of doing this. This is because for a block$NL$with input U(s) and transfer function G(s) the output Y(s) = G(s)U(s). When the input, u(t), is a$NL$unit impulse which is conventionally denoted by 􀄯(t), U(s) = 1 so that the output Y(s) = G(s).$NL$Thus in the time domain, y(t) = g(t), the inverse Laplace transform of G(s), which is called the$NL$impulse response or weighting function of the block. The evaluation of y(t) for any input u(t) can$NL$be done in the time domain using the convolution integral (see Appendix A, theorem (ix))$NL$but it is normally much easier to use the transform relationship Y(s) = G(s)U(s). To do this one$NL$needs to find the Laplace transform of the input u(t), form the product G(s)U(s) and then find its$NL$inverse Laplace transform. G(s)U(s) will be a ratio of polynomials in s and to find the inverse$NL$Laplace transform, the roots of the denominator polynomial must be found to allow the$NL$expression to be put into partial fractions with each term involving one denominator root (pole).$NL$Assuming, for example, the input is a unit step so that U(s) = 1/s then putting G(s)U(s) into$NL$partial fractions will result in an expression for Y(s) of the form$NL$where in the transfer function G(s) = B(s)/A(s), the n poles of G(s) [zeros of A(s)] are 􀄮i, i = 1…n$NL$and the coefficients C0 and Ci, i = 1…n, will depend on the numerator polynomial B(s), and are$NL$known as the residues at the poles. Taking the inverse Laplace transform yields$NL$The first term is a constant C0, sometimes written C0u0(t) because the Laplace transform is$NL$defined for t 􀂕 0, where u0(t) denotes the unit step at time zero. Each of the other terms is an$NL$exponential, which provided the real part of 􀄮i is negative will decay to zero as t becomes large.$NL$In this case the transfer function is said to be stable as a bounded input has produced a bounded$NL$output. Thus a transfer function is stable if all its poles lie in the left hand side (lhs) of the s plane$NL$zero-pole plot illustrated in Figure 2.1. The larger the negative value of 􀄮i the more rapidly the$NL$contribution from the ith term decays to zero. Since any poles which are complex occur in$NL$complex pairs, say of the form 􀄮1,􀄮2 = 􀄱 ± j􀈦, then the corresponding two residues C1 and C2 will$NL$be complex pairs and the two terms will combine to give a term of the form Ce􀁖t sin(􀁚t 􀀎􀁍) .$NL$This is a damped oscillatory exponential term where 􀄱, which will be negative for a stable$NL$transfer function, determines the damping and 􀈦 the frequency [strictly angular frequency] of the$NL$oscillation. For a specific calculation most engineers, as mentioned earlier, will leave a complex$NL$pair of roots as a quadratic factor in the partial factorization process, as illustrated in the Laplace$NL$transform inversion example given in Appendix A. For any other input to G(s), as with the step$NL$input, the poles of the Laplace transform of the input will occur in a term of the partial fraction$NL$expansion (3.2), [as for the C0/s term above], and will therefore produce a bounded output for a$NL$bounded input.$NL$In control engineering the major deterministic input signals that one may wish to obtain$NL$responses to are a step, an impulse, a ramp and a constant frequency input. The purpose of this$NL$section is to discuss step responses of specific transfer functions, hopefully imparting an$NL$understanding of what can be expected from a knowledge of the zeros and poles of the transfer$NL$function without going into detailed mathematics.$NL$A transfer function with a single pole is$NL$s a$NL$G s K$NL$􀀎$NL$( ) 􀀠1 , which may also be written in the socalled$NL$time constant form$NL$sT$NL$G s K$NL$􀀎$NL$􀀠$NL$1$NL$( ) , where K K / a 1 􀀠and T 􀀠1/ a The steady state$NL$gainG(0) 􀀠K , that is the final value of the response, and T is called the time constant as it$NL$determines the speed of the response. K will have units relating the input quantity to the output$NL$quantity, for example °C/V, if the input is a voltage and the output temperature. T will have the$NL$same units of time as s-1, normally seconds. The output, Y(s), for a unit step input is given by$NL$Taking the inverse Laplace transform gives the result$NL$The larger the value of T (i.e. the smaller the value of a), the slower the exponential response. It$NL$can easily be shown that y(T) 􀀠0.632K , T$NL$dt$NL$dy(0) 􀀠$NL$and y(5T) 􀀠0.993K or in words, the$NL$output reaches 63.2% of the final value after a time T, the initial slope of the response is T and$NL$the response has essentially reached the final value after a time 5T. The step response in$NL$MATLAB can be obtained by the command step(num,den). The figure below shows the step$NL$response for the transfer function with K = 1 on a normalised time scale.$NL$Here the transfer function G(s) is often assumed to be of the form$NL$It has a unit steady state gain, i.e G(0) = 1, and poles at 􀀠􀀐􀁝􀁚􀁲􀁚1􀀐􀁝2 o o s j , which are$NL$complex when 􀁝􀀟1. For a unit step input the output Y(s), can be shown after some algebra,$NL$which has been done so that the inverse Laplace transforms of the second and third terms are$NL$damped cosinusoidal and sinusoidal expressions, to be given by$NL$Taking the inverse Laplace transform it yields, again after some algebra,$NL$where 􀁍􀀠cos􀀐1􀁝. 􀁝is known as the damping ratio. It can also be seen that the angle to the$NL$negative real axis from the origin to the pole with positive imaginary part is$NL$tan􀀐1 (1􀀐􀁝2 )1/ 2 /􀁝􀀠cos􀀐1􀁝􀀠􀁍.
18;As its name implies control engineering involves the design of an engineering product or system$NL$where a requirement is to accurately control some quantity, say the temperature in a room or the$NL$position or speed of an electric motor. To do this one needs to know the value of the quantity$NL$being controlled, so that being able to measure is fundamental to control. In principle one can$NL$control a quantity in a so called open loop manner where ‘knowledge’ has been built up on what$NL$input will produce the required output, say the voltage required to be input to an electric motor$NL$for it to run at a certain speed. This works well if the ‘knowledge’ is accurate but if the motor is$NL$driving a pump which has a load highly dependent on the temperature of the fluid being pumped$NL$then the ‘knowledge’ will not be accurate unless information is obtained for different fluid$NL$temperatures. But this may not be the only practical aspect that affects the load on the motor and$NL$therefore the speed at which it will run for a given input, so if accurate speed control is required$NL$an alternative approach is necessary.$NL$This alternative approach is the use of feedback whereby the quantity to be controlled, say C, is$NL$measured, compared with the desired value, R, and the error between the two,$NL$E = R - C used to adjust C. This gives the classical feedback loop structure of Figure 1.1.$NL$In the case of the control of motor speed, where the required speed, R, known as the reference is$NL$either fixed or moved between fixed values, the control is often known as a regulatory control, as$NL$the action of the loop allows accurate speed control of the motor for the aforementioned situation$NL$in spite of the changes in temperature of the pump fluid which affects the motor load. In other$NL$instances the output C may be required to follow a changing R, which for example, might be the$NL$required position movement of a robot arm. The system is then often known as a$NL$servomechanism and many early textbooks in the control engineering field used the word$NL$servomechanism in their title rather than control.$NL$The use of feedback to regulate a system has a long history [1.1, 1.2], one of the earliest concepts,$NL$used in Ancient Greece, was the float regulator to control water level, which is still used today in$NL$water tanks. The first automatic regulator for an industrial process is believed to have been the$NL$flyball governor developed in 1769 by James Watt. It was not, however, until the wartime period$NL$beginning in 1939, that control engineering really started to develop with the demand for$NL$servomechanisms for munitions fire control and guidance. With the major improvements in$NL$technology since that time the applications of control have grown rapidly and can be found in all$NL$walks of life. Control engineering has, in fact, been referred to as the ‘unseen technology’ as so$NL$often people are unaware of its existence until something goes wrong. Few people are, for$NL$instance, aware of its contribution to the development of storage media in digital computers$NL$where accurate head positioning is required. This started with the magnetic drum in the 50’s and$NL$is required today in disk drives where position accuracy is of the order of 1μm and movement$NL$between tracks must be done in a few ms.$NL$Feedback is, of course, not just a feature of industrial control but is found in biological, economic$NL$and many other forms of system, so that theories relating to feedback control can be applied to$NL$many walks of life.$NL$The book is concerned with theoretical methods for continuous linear feedback control system$NL$design, and is primarily restricted to single-input single-output systems. Continuous linear time$NL$invariant systems have linear differential equation mathematical models and are always an$NL$approximation to a real device or system. All real systems will change with time due to age and$NL$environmental changes and may only operate reasonably linearly over a restricted range of$NL$operation. There is, however, a rich theory for the analysis of linear systems which can provide$NL$excellent approximations for the analysis and design of real world situations when used within$NL$the correct context. Further simulation is now an excellent means to support linear theoretical$NL$studies as model errors, such as the affects of neglected nonlinearity, can easily be assessed.$NL$There are total of 11 chapters and some appendices, the major one being Appendix A on Laplace$NL$transforms. The next chapter provides a brief description of the forms of mathematical model$NL$representations used in control engineering analysis and design. It does not deal with$NL$mathematical modelling of engineering devices, which is a huge subject and is best dealt with in$NL$the discipline covering the subject, since the devices or components could be electrical,$NL$mechanical, hydraulic etc. Suffice to say that one hopes to obtain an approximate linear$NL$mathematical model for these components so that their effect in a system can be investigated$NL$using linear control theory. The mathematical models discussed are the linear differential$NL$equation, the transfer function and a state space representation, together with the notations used$NL$for them in MATLAB.$NL$Chapter 3 discusses transfer functions, their zeros and poles, and their responses to different$NL$inputs. The following chapter discusses in detail the various methods for plotting steady state$NL$frequency responses with Bode, Nyquist and Nichols plots being illustrated in MATLAB.$NL$Hopefully sufficient detail, which is brief when compared with many textbooks, is given so that$NL$the reader clearly understands the information these plots provide and more importantly$NL$understands the form of frequency response expected from a specific transfer function.$NL$The material of chapters 2-4 could be covered in other courses as it is basic systems theory, there$NL$having been no mention of control, which starts in chapter 5. The basic feedback loop structure$NL$shown in Figure 1.1 is commented on further, followed by a discussion of typical performance$NL$specifications which might have to be met in both the time and frequency domains. Steady state$NL$errors are considered both for input and disturbance signals and the importance and properties of$NL$an integrator are discussed from a physical as well as mathematical viewpoint. The chapter$NL$concludes with a discussion on stability and a presentation of several results including the$NL$Mikhailov criterion, which is rarely mentioned in English language texts.
18;Control systems exist in many fields of engineering so that components of a control system may$NL$be electrical, mechanical, hydraulic etc. devices. If a system has to be designed to perform in a$NL$specific way then one needs to develop descriptions of how the outputs of the individual$NL$components, which make up the system, will react to changes in their inputs. This is known as$NL$mathematical modelling and can be done either from the basic laws of physics or from$NL$processing the input and output signals in which case it is known as identification. Examples of$NL$physical modelling include deriving differential equations for electrical circuits involving$NL$resistance, inductance and capacitance and for combinations of masses, springs and dampers in$NL$mechanical systems. It is not the intent here to derive models for various devices which may be$NL$used in control systems but to assume that a suitable approximation will be a linear differential$NL$equation. In practice an improved model might include nonlinear effects, for example Hooke’s$NL$Law for a spring in a mechanical system is only linear over a certain range or account for time$NL$variations of components. Mathematical models of any device will always be approximate, even$NL$if nonlinear effects and time variations are also included by using more general nonlinear or time$NL$varying differential equations. Thus, it is always important in using mathematical models to have$NL$an appreciation of the conditions under which they are valid and to what accuracy.$NL$Starting therefore with the assumption that our model is a linear differential equation then in$NL$general it will have the form:-$NL$where D denotes the differential operator d/dt. A(D) and B(D) are polynomials in D with$NL$Di 􀀠d i / dt i , the ith derivative, u(t) is the model input and y(t) its output. So that one can write$NL$where the a and b coefficients will be real numbers. The orders of the polynomials A and B are$NL$assumed to be n and m, respectively, with n 􀂕 m.$NL$Thus, for example, the differential equation$NL$with the dependence of y and u on t assumed can be written $NL$$NL$In order to solve an nth order differential equation, that is determine the output y for a given input$NL$u, one must know the initial conditions of y and its first n-1 derivatives. For example if a$NL$projectile is falling under gravity, that is constant acceleration, so that D2y= constant, where y is$NL$the height, then in order to find the time taken to fall to a lower height, one must know not only$NL$the initial height, normally assumed to be at time zero, but the initial velocity, dy/dt, that is two$NL$initial conditions as the equation is second order (n = 2). Control engineers typically study$NL$solutions to differential equations using either Laplace transforms or a state space representation.$NL$A short introduction to the Laplace transformation is given in Appendix A for the reader who is$NL$not familiar with its use. It is an integral transformation and its major, but not sole use, is for$NL$differential equations where the independent time variable t is transformed to the complex$NL$variable s by the expression$NL$Since the exponential term has no units the units of s are seconds-1, that is using mks notation s$NL$has units of s-1. If denotes the Laplace transform then one may write$NL$[f(t)] = F(s) and -1[F(s)] = f(t). The relationship is unique in that for every f(t), [F(s)], there is a$NL$unique F(s), [f(t)]. It is shown in Appendix A that when the n-1 initial conditions, Dn-1y(0) are$NL$zero the Laplace transform of Dny(t) is snY(s). Thus the Laplace transform of the differential$NL$equation (2.1) with zero initial conditions can be written$NL$with the assumed notation that signals as functions of time are denoted by lower case letters and$NL$as functions of s by the corresponding capital letter.$NL$If equation (2.8) is written$NL$then this is known as the transfer function, G(s), between the input and output of the ‘system’,$NL$that is whatever is modelled by equation (2.1). B(s), of order m, is referred to as the numerator$NL$polynomial and A(s), of order n, as the denominator polynomial and are from equations (2.2) and$NL$(2.3)$NL$Since the a and b coefficients of the polynomials are real numbers the roots of the polynomials$NL$are either real or complex pairs. The transfer function is zero for those values of s which are the$NL$roots of B(s), so these values of s are called the zeros of the transfer function. Similarly, the$NL$transfer function will be infinite at the roots of the denominator polynomial A(s), and these values$NL$are called the poles of the transfer function. The general transfer function (2.9) thus has m zeros$NL$and n poles and is said to have a relative degree of n-m, which can be shown from physical$NL$realisation considerations cannot be negative. Further for n > m it is referred to as a strictly$NL$proper transfer function and for n 􀂕 m as a proper transfer function.$NL$When the input u(t) to the differential equation of (2.1) is constant the output y(t) becomes$NL$constant when all the derivatives of the output are zero. Thus the steady state gain, or since the$NL$input is often thought of as a signal the term d.c. gain (although it is more often a voltage than a$NL$current!) is used, and is given by$NL$If the n roots of A(s) are 􀄮i , i = 1….n and of B(s) are 􀈕j, j = 1….m, then the transfer function may$NL$be written in the zero-pole form$NL$When the transfer function is known in the zero-pole form then the location of its zeros and poles$NL$can be shown on an s plane zero-pole plot, where the zeros are marked with a circle and the poles$NL$by a cross. The information on this plot then completely defines the transfer function apart from$NL$the gain K. In most instances engineers prefer to keep any complex roots in quadratic form, thus$NL$for example writing
18;The frequency response of a transfer function G(j􀈦) was introduced in the last chapter. As G(j􀈦)$NL$is a complex number with a magnitude and argument (phase) if one wishes to show its behaviour$NL$over a frequency range then one has 3 parameters to deal with the frequency, 􀈦, the magnitude,$NL$M, and the phase 􀄳. Engineers use three common ways to plot the information, which are known$NL$as Bode diagrams, Nyquist diagrams and Nichols diagrams in honour of the people who$NL$introduced them. All portray the same information and can be readily drawn in MATLAB for a$NL$system transfer function object G(s).$NL$One diagram may prove more convenient for a particular application, although engineers often$NL$have a preference. In the early days when computing facilities were not available Bode diagrams,$NL$for example, had some popularity because of the ease with which they could, in many instances,$NL$be rapidly approximated. All the plots will be discussed below, quoting many results without$NL$going into mathematical detail, in the hope that the reader will obtain enough knowledge to know$NL$whether MATLAB plots obtained are of the general shape expected.$NL$A Bode diagram consists of two separate plots the magnitude, M, as a function of frequency and$NL$the phase 􀄳 as a function of frequency. For both plots the frequency is plotted on a logarithmic$NL$(log) scale along the x axis. A log scale has the property that the midpoint between two$NL$frequencies 􀈦1 and 􀈦2 is the frequency 1 2 􀁚􀀠􀁚􀁚. A decade of frequency is from a value to$NL$ten times that value and an octave from a value to twice that value. The magnitude is plotted$NL$either on a log scale or in decibels (dB), where dB M 10 􀀠20log . The phase is plotted on a linear$NL$scale. Bode showed that for a transfer function with no right hand side (rhs) s-plane zeros the$NL$phase is related to the slope of the magnitude characteristic by the relationship$NL$It can be further shown from this expression that a relatively good approximation is that the$NL$phase at any frequency is 15° times the slope of the magnitude curve in dB/octave. This was a$NL$useful concept to avoid drawing both diagrams when no computer facilities were available.$NL$For two transfer functions G1 and G2 in series the resultant transfer function, G, is their product,$NL$this means for their frequency response$NL$which in terms of their magnitudes and phases can be written$NL$Thus since a log scale is used on the magnitude of a Bode diagram this means Bode magnitude$NL$plots for two transfer functions in series can be added, as also their phases on the phase diagram.$NL$Hence a transfer function in zero-pole form can be plotted on the magnitude and phase Bode$NL$diagrams simple by adding the individual contributions from each zero and pole. It is thus only$NL$necessary to know the Bode plots of single roots and quadratic factors to put together Bode plots$NL$for a complicated transfer function if it is known in zero-pole form.$NL$The single pole transfer function is normally considered in time constant form with unit steady$NL$state gain, that is$NL$It is easy to show that this transfer function can be approximated by two straight lines, one$NL$constant at 0 dB, as G(0) = 1, until the frequency, 1/T, known as the break point, and then from$NL$that point by a line with slope -6dB/octave. The actual curve and the approximation are shown in$NL$Figure 4.1 together with the phase curve. The differences between the exact magnitude curve and$NL$the approximation are symmetrical, that is a maximum at the breakpoint of 3dB, 1dB one octave$NL$each side of the breakpoint, 0.3 dB two octaves away etc. The phase changes between 0° and -$NL$90° again with symmetry about the breakpoint phase of -45°. Note a steady slope of -6 dB/octave$NL$has a corresponding phase of -90°$NL$The Bode magnitude plot of a single zero time constant, that is$NL$is simply a reflection in the 0 dB axis of the pole plot. That is the approximate magnitude curve is$NL$flat at 0 dB until the break point frequency, 1/T, and then increases at 6 dB/octave. Theoretically$NL$as the frequency tends to infinity so does its gain so that it is not physically realisable. The phase$NL$curve goes from 0° to +90°$NL$The transfer function of an integrator, which is a pole at the origin in the zero-pole plot, is 1/s. It$NL$is sometimes taken with a gain K, i.e.K/s. Here K will be replaced by 1/T to give the transfer$NL$function$NL$On a Bode diagram the magnitude is a constant slope of -6 dB/octave passing through 0 dB at the$NL$frequency 1/T. Note that on a log scale for frequency, zero frequency where the integrator has$NL$infinite gain (the transfer function can only be produced electronically by an active device) is$NL$never reached. The phase is -90° at all frequencies. A differentiator has a transfer function of sT$NL$which gives a gain characteristic with a slope of 6 dB/octave passing through 0dB at a frequency$NL$of 1/T. Theoretically it produces infinite gain at infinite frequency so again it is not physically$NL$realisable. It has a phase of +90° at all frequencies.$NL$The quadratic factor form is again taken for two complex poles with 􀈗 < 1 as in equation (3.7),$NL$that is$NL$Again G(0) = 1 so the response starts at 0 dB and can be approximated by a straight line at 0 dB$NL$until 􀈦o and by a line from 􀈦o at -12 dB/octave. However, this is a very coarse approximation as$NL$the behaviour around 􀈦o is highly dependent on 􀈗. It can be shown that the magnitude reaches a$NL$maximum value of$NL$which is approximately 1/2􀈗 for small 􀈗, at a frequency$NL$of$NL$This frequency is thus always less than 􀈦o and only exists for 􀈗 < 0.707.$NL$The response with 􀈗 = 0.707 always has magnitude, M < 1. The phase curve goes from 0° to -$NL$180° as expected from the original and final slopes of the magnitude curve, it has a phase shift of$NL$-90° at the frequency 􀈦o independent of 􀈗 and changes more rapidly near 􀈦o for smaller 􀈗, as$NL$expected due to the more rapid change in the slope of the corresponding magnitude curve.
18;State space modelling was briefly introduced in chapter 2. Here more coverage is provided of$NL$state space methods before some of their uses in control system design are covered in the next$NL$chapter. A state space model, or representation, as given in equation (2.26), is denoted by the two$NL$equations$NL$where equations (10.1) and (10.2) are respectively the state equation and output equation.$NL$The representation can be used for both single-input single-output systems (SISO) and multipleinput$NL$multiple-output systems (MIMO). For the MIMO representation A, B, C and D will all be$NL$matrices. If the state dimension is n and there are r inputs and m outputs then A, B , C and D will$NL$be matrices of order, n x n, n x r, m x n and m x r, respectively. For SISO systems B will be an n$NL$x 1 column vector, often denoted by b, C a 1 x n row vector, often denoted by cT, and D a scalar$NL$often denoted by d. Here the capital letter notation will be used, even though only SISO systems$NL$are considered, and B, C, and D will have the aforementioned dimensions. As mentioned in$NL$chapter 2 the choice of states is not unique and this will be considered further in section 10.3.$NL$First, however, obtaining a solution of the state equation is discussed in the next section.$NL$Obtaining the time domain solution to the state equation is analogous to the classical approach$NL$used to solve the simple first order equation$NL$The procedure in this case is to take u = 0, initially, and to assume a solution for x(t) of eatx(0)$NL$where x(0) is the initial value of x(t). Differentiating this expression gives$NL$so that the assumed solution is valid. Now if the input u is considered$NL$this is assumed to yield a solution of the form$NL$which on differentiating gives$NL$Thus the differential equation is satisfied if$NL$which has the solution$NL$where 􀄲 is a dummy variable. This$NL$solution can be written$NL$so that the complete solution for x(t) consists of$NL$the sum of the two solutions, known as the complimentary function (or initial condition response)$NL$and particular integral (or forced response), respectively and is$NL$For equation (10.1) x is an n vector and A an n x n matrix not a scalar a and to obtain the$NL$complimentary function one assumes x(t) 􀀠e At x(0) . eAt is now a function of a matrix, which is$NL$defined by an infinite power series in exactly the same way as the scalar expression, so that$NL$where I is the n x n identity matrix. Term by term differentiation of equation (10.5) shows that$NL$the derivative of eAt is AeAt and that x(t) 􀀠e At x(0) satisfies the differential equation with u = 0.$NL$eAt is often denoted by 􀄳(t) and is known as the state transition matrix. Using the same approach$NL$as for the scalar case to get the forced response the total solution is found to be$NL$It is easily shown that the state transition matrix 􀁍(􀁗) 􀀠e A􀁗has the property that$NL$so that equation (10.6) can be written alternatively as$NL$This time domain solution of equation (10.1) is useful but most engineers prefer to make use of$NL$the Laplace transform approach. Taking the Laplace transform of equation (10.1) gives$NL$which on rearranging as X(s) is an n vector and A a n x n matrix gives$NL$Taking the inverse Laplace transform of this and comparing with equation (10.7) indicates that$NL$Also taking the Laplace transform of the output equation (10.2) and substituting for X(s) gives$NL$so that the transfer function, G(s), between the input u and output y is$NL$This will, of course, be the same independent of the choice of the states.$NL$Obviously there must be an algebraic relationship between different possible choices of state$NL$variables. Let this relationship be$NL$where x is the original choice in equations (10.1) and (10.2) and z is the new choice. Substituting$NL$this relationship in equation (10.2) givesTz􀀆 􀀠ATz 􀀎Bu which can be written$NL$Also substituting in the output equation (10.2) gives$NL$Thus under the state transformation of equation (10.13) a different state space representation$NL$(T 􀀐1AT,T 􀀐1B,CT,D) is obtained. If the new A matrix is denoted by A T AT z$NL$􀀠􀀐1 then it is$NL$easy to show that A and Az have the following properties$NL$(i) The same eigenvalues$NL$(ii) The same determinant$NL$(iii) The same trace (Sum of elements on the main diagonal)$NL$There are some specific forms of the A matrix which are often commonly used in control$NL$engineering and not unsurprisingly these relate to how one might consider obtaining a state space$NL$representation for a transfer function, the topic of the next section.$NL$This topic was introduced in section 2.3 where the controllable canonical form for a differential$NL$equation was considered. Here this and some other forms will be considered by making use of$NL$block diagrams where every state will be an integrator output. To develop some representations$NL$consider the transfer function$NL$As seen from equation (2.20) the first n-1 state variables are integrals of the next state, that is$NL$􀀠􀂳􀀐x x dx ( j 1) j , or as shown in the equation by j j x 􀀠x ( 􀀐1) 􀀆 , for j = 2 to n. Thus the block$NL$diagram to represent this is n integrators in series. The input to the first integrator is n x􀀆 and its$NL$value is given by x a x a x a x u n 􀀠􀀐􀀐􀀐..... 􀀎􀀆 0 1 1 2 2 3 , the last row of the matrix representation of$NL$equation (2.20). The numerator terms are provided by feeding forward from the states to give the$NL$required output. Thus, for our simple example, this can be shown in the block diagram of Figure$NL$10.1, done in SIMULINK, where since the transfer function is third order n = 3, there are three$NL$integrators, blocks with transfer functions 1/s, in series. Feedback from the states, where the$NL$integrator outputs from left to right are the states x3, x2, and x1, respectively, is by the coefficients$NL$-8, -14 and -7. (negative and in the reverse order of the transfer function denominator). The$NL$numerator coefficients provide feedforward from the states, with the s2 term from x3.
19;Many kinds of things in the world fall into related groups of ‘families’. ‘Inheritance’ is the idea ‘passing down’ characteristics from parent to child, and plays an important part in Object Oriented design and programming.$NL$While you are probably already familiar with constructors, and access control (public/private), there are particular issues in relating these to inheritance we need to consider.$NL$Additionally we need to consider the use of Abstract classes and method overriding as these are important concepts in the context of inheritance.$NL$Finally we will look at the ‘Object’ class which has a special role in relation to all other classes in C#.$NL$Classes are a generalized form from which objects with differing details can be created. Objects are thus ‘instances’ of their class. For example Student 051234567 is an instance of class Student. More concisely, 051234567 is a Student. Constructors are special methods that create an object from the class definition.$NL$Classes themselves can often be organised by a similar kind of relationship.$NL$One hierarchy, that we all have some familiarity with, is that which describes the animal kingdom :-$NL$• Kingdom (e.g. animals)$NL$• Phylum (e.g. vertebrates)$NL$• Class (e.g. mammal)$NL$• Order (e.g. carnivore)$NL$• Family (e.g. cat)$NL$• Genus (e.g. felix)$NL$• Species (e.g. felix leo)$NL$We can represent this hierarchy graphically ….$NL$Of course to draw the complete diagram would take more time and space than we have available.$NL$Here we can see one specific animal shown here :-‘Fred’. Fred is not a class of animal but an actual animal.$NL$Fred is a felix leo is a felix is a cat is a carnivore$NL$Carnivores eat meat so Fred has the characteristic ‘eats meat’.$NL$Fred is a felix leo is a felix is a cat is a carnivore is a mammal is a vertebrate$NL$Vertebrates have a backbone so Fred has the characteristic ‘has a backbone’.$NL$The ‘is a’ relationship links an individual to a hierarchy of characteristics. This sort of relationship applies to many real world entities, e.g. BonusSuperSaver is a SavingsAccount is a BankAccount.$NL$We specify the general characteristics high up in the hierarchy and more specific characteristics lower down. An important principle in OO – we call this generalization and specialization.$NL$All the characteristics from classes above in a class/object in the hierarchy are automatically featured in it – we call this inheritance.$NL$Consider books and magazines - both are specific types of publication.$NL$We can show classes to represent these on a UML class diagram. In doing so we can see some of the instance variables and methods these classes may have.$NL$Attributes ‘title’, ‘author’ and ‘price’ are obvious. Less obvious is ‘copies’ this is how many are currently in stock. $NL$For books, OrderCopies() takes a parameter specifying how many extra copies are added to stock.$NL$For magazines, orderQty is the number of copies received of each new issue and currIssue is the date/period of the current issue (e.g. “January 2011”, “Fri 6 Jan”, “Spring 2011” etc.) When a new issue is received the old issues are discarded and orderQty copies are placed in stock. Therefore RecNewIssue() sets currIssue to the date of new issue and restores copies to orderQty. AdjustQty() modifies orderQty to alter how many copies of subsequent issues will be stocked.$NL$We can separate out (‘factor out’) these common members of the classes into a superclass called Publication. In C# a superclass is often called a base class.$NL$The differences will need to be specified as additional members for the ‘subclasses’ Book and Magazine.$NL$In this is a UML Class Diagram the hollow-centred arrow denotes inheritance.$NL$Note the subclass has the generalized superclass (or base class) characteristics + additional specialized characteristics. Thus the Book class has four instance variables (title, price, copies and author) it also has two methods (SellCopy() and OrderCopies()).$NL$The inherited characteristics are not listed in subclasses. The arrow shows they are acquired from the superclass.$NL$No special features are required to create a superclass. Thus any class can be a superclass unless specifically prevented.$NL$A subclass specifies it is inheriting features from a superclass using the : symbol. For example….$NL$Constructors are methods that create objects from a class. Each class (whether sub or super) should encapsulate its own initialization in a constructor, usually relating to setting the initial state of its instance variables. Constructors are methods given the same name as the class.$NL$A constructor for a superclass, or base class, should deal with general initialization.$NL$Each subclass can have its own constructor for specialised initialization but it must often invoke the behaviour of the base constructor. It does this using the keyword base.$NL$Usually some of the parameters passed to MySubClass will be initializer values for superclass instance variables, and these will simply be passed on to the superclass constructor as parameters. In other words super-parameters will be some (or all) of sub-parameters. $NL$Shown below are two constructors, one for the Publication class and one for Book. The book constructor requires four parameters three of which are immediately passed on to the base constructor to initialize its instance variables.$NL$Thus in creating a book object we first create a publication object. The constructor for Book does this by calling the constructor for Publication.$NL$Rules exist that govern the invocation of a superconstructor.$NL$If the superclass has a parameterless (or default) constructor this will be called automatically if no explicit call to base is made in the subclass constructor though an explicit call is still better style for reasons of clarity.$NL$However if the superclass has no parameterless constructor but does have a parameterized one, this must be called explicitly using : base.$NL$To illustrate this….$NL$On the left above:- it is legal, though bad practice, to have a subclass with no constructor because superclass has a parameterless constructor.$NL$In the centre:- if subclass constructor doesn’t call the base constructor then the parameterless superclass constructor will be called.$NL$On the right:- because superclass has no paramterless constructor, subclass must have a constructor, it must call the super constructor using the keyword base and it must pass on the required paramter. This is simply because a (super) class with only a parameterized constructor can only be initialized by providing the required parameter(s).
19;Historically in computer programs method names were required to be unique. Thus the compiler could identify which method was being invoked just by looking at its name.$NL$However several methods were often required to perform very similar functionality for example a method could add two integer numbers together and another method may be required to add two floating point numbers. If you have to give these two methods unique names which one would you call ‘Add()’? $NL$In order to give each method a unique name the names would need to be longer and more specific. We could therefore call one method AddInt() and the other AddFloat() but this could lead to a proliferation of names each one describing different methods that are essentially performing the same operation i.e. adding two numbers.$NL$To overcome this problem in C# you are not required to give each method a unique name – thus both of the methods above could be called Add(). However if method names are not unique the C# must have some other way of deciding which method to invoke at run time. i.e. when a call is made to Add(number1, number2) the machine must decide which of the two methods to use. It does this by looking at the parameter list.$NL$While the two methods may have the same name they can still be distinguished by looking at the parameter list. :-$NL$Add(int number1, int number2)$NL$Add(float number1, float number2)$NL$This is resolved at run time by looking at the method call and the actual parameters being passed. If two integers are being passed then the first method is invoked. However if two floating point numbers are passed then the second method is used.$NL$Overloading refers to the fact that several methods may share the same name. As method names are no longer uniquely identify the method then the name is ‘overloaded’.$NL$Having several methods that essentially perform the same operation, but which take different parameter lists, can lead to enhanced flexibility and robustness in a system. $NL$Imagine a University student management system. A method would probably be required to enrol, or register, a new student. Such a method could have the following signature …$NL$EnrollStudent(String name, String address, String coursecode) $NL$However if a student had just arrived in the city and had not yet sorted out where they were living would the University want to refuse to enrol the student? They could do so but would it not be better to allow such a student to enrol (and set the address to ‘unkown’)?$NL$To allow this the method EnrollStudent() could be overloaded and an alternative method provided as…$NL$EnrollStudent(String name, String coursecode) $NL$At run time the method invoked will depend upon the parameter list provided. Thus given a call to $NL$EnrollStudent(“Fred”, “123 Abbey Gardens”, “G700”)$NL$the first method would be used.$NL$Overloading methods don’t just provide more flexibility for the user they also provide more flexibility for programmers who may have the job of extending the system in the future and thus overloading methods can make the system more future proof and robust to changing requirements.$NL$Constructors can be overloaded as well as ordinary methods.$NL$We can make our programs more adaptable by overloading constructors and other methods. Even if we don’t initially use all of the different constructors, or methods, by providing them we are making our programs more flexible and adaptable to meet changing requirements.$NL$Method overloading is the name given to the concept that several methods may exist that essentially perform the same operation and thus have the same name.$NL$The CLR engine distinguishes these by looking at the parameter list. If two or more methods have the same name then their parameter list must be different.$NL$At run time each method call, which may be ambiguous, is resolved by the CLR engine by looking at the parameters passed and matching the data types with the method signatures defined in the class. $NL$By overloading constructors and ordinary methods we are providing extra flexibility to the programmers who may use our classes in the future. Even if these are not all used initially, providing these can help make the program more flexible to meet changing user requirements.$NL$The development of any computer program starts by identifying a need :- $NL$• An engineer who specialises in designing bridges may need some software to create three dimensional models of the designs so people can visualise the finished bridge long before it is actually built.$NL$• A manager may need a piece of software to keep track of personnel, what projects they are assigned to, what skills they have and what skills need to be developed etc.$NL$But how do we get from a ‘need’ for some software to an object oriented software design that will meet this need?$NL$Some software engineers specialise in the task of Requirement Analysis which is the task of clarifying exactly what is required of the software. Often this is done by iteratively performing the following tasks :- $NL$1) interviewing clients and potential users of the system to find out what they say about the system needed$NL$2) documenting the results of these conversations,$NL$3) identifying the essential features of the required system$NL$4) producing preliminary designs (and possibly prototypes of the system)$NL$5) evaluating these initial plans with the client and potential users$NL$6) repeating the steps above until a finished design has evolved.$NL$Performing requirements analysis is a specialised skill that is outside the scope of this text but here we will focus on steps three and four above ie. given a description of a system how do we convert this into a potential OO design.$NL$While we can hope to develop preliminary design skills experience is a significant factor in this task. Producing simple and elegant designs is important if we want the software to work well and be easy to develop however identifying good designs from weaker designs is not simple and experience is a key factor.$NL$A novice chess player may know all the rules but it takes experience to learn how to choose good moves from bad moves and experience is essential to becoming a skilled player. Similarly experience is essential to becoming skilled at performing user requirements analysis and in producing good designs.
19;Computing is a constantly changing our world and our environment. In the 1960s large machines called mainframes were created to manage large volumes of data (numbers) efficiently. Bank account and payroll programs changed the way organisations worked and made parts of these organisations much more efficient. In the 1980s personal computers became common and changed the way many individuals worked. People started to own their own computers and many used word processors and spreadsheets applications (to write letters and to manage home accounts). In the 1990s email became common and the world wide web was born. These technologies revolutionised communications allowing individuals to publish information that could easily be accessed on a global scale. The ramifications of these new technologies are still not fully understood as society is adapting to opportunities of internet commerce, new social networking technologies (twitter, facebook, myspace, online gaming etc) and the challenges of internet related crime.$NL$Just as new computing technologies are changing our world so too are new techniques and ideas changing the way we develop computer systems. In the 1950s the use machine code (unsophisticated, complex and machine specific) languages were common. $NL$In the 1960s high level languages, which made programming simpler, became common. However these led to the development of large complex programs that were difficult to manage and maintain. $NL$In the 1970s the structured programming paradigm became the accepted standard for large complex computer programs. The structured programming paradigm proposed methods to logically structure the programs developed into separate smaller, more manageable components. Furthermore methods for analysing data were proposed that allowed large databases to be created that were efficient, preventing needless duplication of data and protected us against the risks associated with data becoming out of sync. However significant problems still persisted in a) understanding the systems we need to create and b) changing existing software as users requirements changed. $NL$In the 1980s ‘modular’ languages, such as Modula-2 and ADA were developed that became the precursor to modern Object Oriented languages. $NL$In the 1990s the Object Oriented paradigm and component-based software development ideas were developed and Object Oriented languages became the norm from 2000 onwards. $NL$The object oriented paradigm is based on many of the ideas developed over the previous 30 years of abstraction, encapsulation, generalisation and polymorphism and led to the development of software components where the operation of the software and the data it operates on are modelled together. Proponents of the Object Oriented software development paradigm argue that this leads to the development of software components that can be re-used in different applications thus saving significant development time and cost savings but more importantly allow better software models to be produced that make systems more maintainable and easier to understand.$NL$It should perhaps be noted that software development ideas are still evolving and new agile methods of working are being proposed and tested. Where these will lead us in 2020 and beyond remains to be seen.$NL$The structured programming paradigm proposed that programs could be developed in sensible blocks that make the program more understandable and easier to maintain.$NL$Activity 1 $NL$Assume you undertake the following activities on a daily basis. Arrange this list into a sensible order then split this list into three blocks of related activities and give each block a heading to summarise the activities carried out in that block.$NL$Get out of bed$NL$Eat breakfast$NL$Park the car$NL$Get dressed$NL$Get the car out of the garage$NL$Drive to work$NL$Find out what your boss wants you to do today$NL$Feedback to the boss on today’s results.$NL$Do what the boss wants you to do $NL$Feedback 1$NL$You should have been able to organise these into groups of related activities and give each group a title that summarises those activities.$NL$Get up :-$NL$Get out of bed$NL$Get dressed$NL$Eat breakfast$NL$Go to Work :-$NL$Get the car out of the garage$NL$Drive to work$NL$Park the car$NL$Do your job :-$NL$Find out what your boss wants you to do today$NL$Do what the boss wants you to do$NL$Feedback to the boss on today’s results.$NL$By structuring our list of instructions and considering the overall structure of the day (Get up, go to work, do your job) we can change and improve one section of the instructions without changing the other parts. For example we could improve the instructions for going to work….$NL$Listen to the local traffic and weather report$NL$Decide whether to go by bus or by car$NL$If going by car, get the car and drive to work.$NL$Else walk to the bus station and catch the bus$NL$without worrying about any potential impact this may have on ‘getting up’ or ‘doing your job’. In the same way structuring computer programs can make each part more understandable and make large programs easier to maintain. $NL$Feedback 2$NL$With an address book we would want to be able to perform the following actions :- find out details of a friend i.e. their telephone number, add an address to the address book and, of course, delete an address.$NL$We can create a simple software component to store the data in the address book (i.e. list of names etc) and the operations, things we can do with the address book (i.e add address, find telephone number etc). $NL$By creating a simple software component to store and manage addresses of friends we can reuse this in another software system i.e. it could be used by a business manager to store and find details of customers. It could also become part of a library system to be used by a librarian to store and retrieve details of the users of the library. $NL$Thus in object oriented programming we can create re-usable software components (in this case an address book).$NL$While we can focus our attention on the actual program code we are writing, whatever development methodology is adopted, it is not the creation of the code that is generally the source of most problems. Most problems arise from :-$NL$• poor maintainability: the system is hard to understand and revise when, as is inevitable, requests for change arise.$NL$• Statistics show 70% of the cost of software is not incurred during its initial development phase but is incurred during subsequent years as the software is amended to meet the ever changing needs of the organisation for which it was developed. For this reason it is essential that software engineers do everything possible to ensure that software is easy to maintain during the years after its initial creation.
19;We have seen previously how methods are identified at run time by their signature i.e. the name of the method and the list of parameters the method takes.$NL$Thus we can have two methods with the same name. Shown below are two methods that find a highest value… one finds the highest value given two integer numbers, the other finds the highest value of two double numbers.$NL$Given the following code the CLR engine will invoke the first of these methods then the second as the correct method to implement is identified by its name and the type of its parameters.$NL$Given the following definition of a Student class…$NL$We could even define a version of this method to find the highest student (alphabetically).$NL$And invoke this using ..$NL$In doing this we have created three methods that essentially do exactly the same thing only using different parameter types. This leads us to the idea that it would be nice to create just one method that would work with objects of any type.$NL$The method below is a first attempt to do this :-$NL$This method takes any two ‘Objects’ as parameters. ‘Object’ is the base class of all other classes thus this method can take any two objects of any more specific type and treat these as of the base type ‘Object’. This is polymorphism. $NL$The method above then converts these two ‘Object’s to strings and compares these strings.$NL$Thus this one method can be invoked three times using the following code….$NL$In these cases respectively the CLR engine will treat int, double, and Student objects as the most general type of thing available ‘Object’. It will then convert this object to a string and compare the strings. $NL$This will work however in many situations creating methods which take parameters of type ‘Object’ is flawed or at least very limited. $NL$Inside these methods we do not know what type of object was actually passed as a parameter and hence in the example above we do not know what type of object is actually being returned. When two students object are passed the object returned is a student but we cannot invoke Student specific methods on this object unless we first cast the object returned to a Student.$NL$Assume that we want to invoke an ‘AwardMerit() method on the ‘Student’ returned via the Highest() method. We can do this is we first cast the returned ‘Object’ onto a ‘Student’ object. E.g.$NL$However the compiler cannot be certain that the returned object is a ‘Student’ and the compiler cannot detect the potentially critical error that would occur is we invoked Highest() on two integer numbers and then tried to cast the returning integer onto an object of type ‘Student’.$NL$This leads us to the idea that we would like to be able to create a method that will take parameters of ANY type and return values that are again of ANY type but where we will define these types when we invoke these methods. Such methods do exist and they are called Generic Methods.$NL$Generic methods are methods where the parameter types are not defined until the method is invoked. Parameter types are specified each time the method is invoked and the compiler can thus still check the code is valid.$NL$In other words a generic method uses a parameterized type – a data type that is determined by a parameter. $NL$In the code below a generic method Highest() has been defined as a method that takes two objects as parameters of unspecified type :-$NL$We can use this method and each time we invoke it we define the type of object being compared. If two students are compared the compiler will know that the object being returned is also type student and will therefore know it is legal to invoke student specific methods on this object. $NL$A list of generic data types contained between angle brackets that follow the method’s name is provided. If multiple generic types are used by a method, their names are separated by commas.$NL$In the example above, the identifer T can stand for any data type. So, when T is used within the brackets in the method’s parameter list to describe data, it might be an int, double, ‘Student’ or any other data type. The only requirement is that the method must work no matter what type of object is passed to it. All objects have a ToString() method because every data type inherits the ToString() method form the ‘Object’ class, or contains its own overriding version.$NL$In the case above only one generic data type is specified. This is used to define the type of both parameters and the return value.$NL$The generic data type identifier for a generic method can be any legal C# identifier, but by convention it is usually an upper case letter. “T” is used to stand for “type”.$NL$When a generic method is defined i.e. one that works with any data type the type is specified when the method is used.$NL$Thus in the code below, while Highest() is a generic method, the compiler can see that a ‘Student’ object is being passed as a parameter and thus the object being returned must also be of type ‘Student’.$NL$Given the object being returned is of type ‘Student’ it must be OK to store this in a variable of type Student and it must be OK to invoke and Student methods on the object returned.$NL$Generic methods can therefore that work with any type data but the compiler can still check for type errors (as the type is specified each time the method is invoked).$NL$Generic classes have been created, based on the same mechanisms as generic methods and these are particularly useful because there were used by the creators of the .NET libraries to create generic collections. $NL$While we won’t be making much use of generic methods and generic classes ourselves in this book we will be making significant use of generic collections, but before using these we need a basic understanding of collections themselves.
19;The Unified Modelling Language, UML, is sometimes described as though it was a methodology. It is not!$NL$A methodology is a system of processes in order to achieve a particular outcome e.g. an organised sequence of activities in order to gather user requirements. UML does not describe the procedures a programmer should follow – hence it is not a methodology. It is, on the other hand, a precise diagramming notation that will allow program designs to be represented and discussed. As it is graphical in nature it becomes easy to visualise, understand and discuss the information presented in the diagram. However, as the diagrams represent technical information they must be precise and clear – in order for them to work - therefore there is a precise notation that must be followed.$NL$As UML is not a methodology it is left to the user to follow whatever processes they deem appropriate in order to generate the designs described by the diagrams. UML does not constrain this – it merely allows those designs to be expressed in an easy to use, but precise, graphical notation. $NL$A process will be explained in chapter 6 that will help you to generate good UML designs. Developing good designs is a skill that takes practise to this end the process is repeated in the case study (chapter 11). For now we will just concentrate on the UML notation not these processes.$NL$Classes are the basic components of any object oriented software system and UML class diagrams provide an easy way to represent these. As well as showing individual classes, in detail, class diagrams show multiple classes and how they are related to each other. Thus a class diagram shows the architecture of a system.$NL$A class consists of :- $NL$• a unique name (conventionally starting with an uppercase letter)$NL$• a list of attributes (int, double, boolean, String etc)$NL$• a list of methods$NL$This is shown in a simple box structure…$NL$For attributes and methods visibility modifiers are shown (+ for public access, – for private access). Attributes are normally kept private and methods are normally made public.$NL$Accessor methods are created to provide access to private attributes when required. Thus a public method SetTitle() can be created to change the value of a private attribute ‘title’. $NL$Thus a class Book, with String attributes of title and author, and the following methods SetTitle(), GetTitle(), SetAuthor(), GetAuthor() and ToString() would be shown as ….$NL$Note: String shown above is not a primitive data type but is itself a class. Hence it starts with a capital letter.$NL$Some programmers use words beginning in capitals to denote class names and words beginning in lowercase to represent attributes or methods (thus ToString() would be shown as toString()). This is a common convention when designing and writing programs in Java (another common OO language). However it is not a convention followed by C# programmers – where method names usually start in Uppercase. Method names can be distinguished from class names by the use of (). This in the example above.$NL$‘Book’ is a class$NL$‘title’ is an attribute and$NL$‘SetTitle()’ is a method.$NL$UML diagrams are not language specific thus a software design, communicated via UML diagrams, can be implemented in a range of OO languages. $NL$Furthermore traditional accessor methods, getters and setters, are not required in C# programs as they are replaced by ‘properties’. Properties are in effect hidden accessor methods thus the getter and setter methods shown above, GetTitle(), SetTitle() etc are not required in a C# program. In C# an attribute would be defined called ‘title’ and a property would be defined as ‘Title’. This would allow us to set the ‘title’ directly by using the associated property ‘Title =…..’. $NL$The UML diagrams shown in this book will use the naming convention common among C# programmers … for the simple reason that we will be writing sample code in C# to demonstrate the OO principles discussed here. Though initially we will show conventional assessor methods these will be replaced with properties when coding.$NL$UML allows us to suppress any information we do not wish to highlight in our diagrams – this allows us to suppress irrelevant detail and bring to the readers attention just the information we wish to focus on. Therefore the following are all valid class diagrams…$NL$Firstly with the access modifiers not shown….$NL$Secondly with the access modifiers and the data types not shown…..$NL$And finally with the attributes and methods not shown…..$NL$i.e. there is a class called ‘BankAccount’ but the details of this are not being shown.$NL$Of course virtually all C# programs will be made up of many classes and classes will relate to each other – some classes will make use of other classes. These relationships are shown by arrows. Different type of arrow indicate different relationships (including inheritance and aggregation relationships). $NL$In addition to this class diagrams can make use of keywords, notes and comments.$NL$As we will see in examples that follow, a class diagram can show the following information :-$NL$• Classes$NL$--attributes$NL$--operations$NL$--visibility$NL$• Relationships$NL$--navigability$NL$--multiplicity$NL$--dependency$NL$--aggregation $NL$--composition$NL$• Generalization / specialization$NL$--inheritance$NL$--interfaces$NL$• Keywords$NL$• Notes and Comments$NL$As UML diagrams convey precise information there is a precise syntax that should be followed.$NL$Attributes should be shown as: visibility name : type multiplicity$NL$Where visibility is one of :-$NL$--‘+’ public$NL$--‘-’ private$NL$--‘#’ protected$NL$--‘~’ package$NL$and Multiplicity is one of :-$NL$--‘n’ exactly n$NL$--‘*’ zero or more$NL$--‘m..‘n’ between m and n$NL$The following are examples of attributes correctly specified using UML :-$NL$- custRef : int [1]$NL$a private attribute custRef is a single int value$NL$this would often be shown as - custRef : int However with no multiplicity shown we cannot safely assume a multiplicity of one was intended by the author.$NL$# itemCodes : String [1..*]$NL$a protected attribute itemCodes is one or more String values$NL$validCard : boolean$NL$an attribute validCard, of unspecified visibility, has unspecified multiplicity
19;Within hierarchical classification of animals$NL$Pinky is a pig (species sus scrofa)$NL$Pinky is (also, more generally) a mammal$NL$Pinky is (also, even more generally) an animal$NL$We can specify the type of thing an organism is at different levels of detail:$NL$higher level = less specific$NL$lower level = more specific$NL$If you were asked to give someone a pig you could give them Pinky or any other pig. $NL$If you were asked to give someone a mammal you could give them Pinky, any other pig or any other mammal (e.g. any lion, or any mouse, or any cat). $NL$If you were asked to give someone an animal you could give them Pinky, any other pig, any other mammal, or any other animal (bird, fish, insect etc). $NL$The idea here is that an object in a classification hierarchy has an ‘is a’ relationship with every class from which it is descended and each classification represents a type of animal.$NL$This is true in object oriented programs as well. Every time we define a class we create a new ‘type’. Types determine compatibility between variables, parameters etc. $NL$A subclass type is a subtype of the superclass type and we can substitute a subtype wherever a ‘supertype’ is expected. Following this we can substitute objects of a subtype whenever objects of a supertype are required (as in the example above).$NL$The class diagram below shows a hierarchical relationship of types of object – or classes.$NL$In other words we can ‘substitute’ an object of any subclass where an object of a superclass is required. This is NOT true in reverse!$NL$When designing class/type hierarchies, the type mechanism allows us to place a subclass object where a superclass is specified. However this has implications for the design of subclasses – we need to make sure they are genuinely substitutable for the superclass. If a subclass object is substitutable then clearly it must implement all of the methods of the superclass – this is easy to guarantee as all of the methods defined in the superclass are inherited by the subclass. Thus while a subclass may have additional methods it must at least have all of the methods defined in the superclass and should therefore be substitutable. However what happens if a method is overridden in the subclass?$NL$When overriding methods we must ensure that they are still substitutable for the method being replaced. Therefore when overriding methods, while it is perfectly acceptable to tailor the method to the needs of the subclass a method should not be overridden with functionality which performs an inherently different operation.$NL$For example, RecNewIssue() in DiscMag overrides RecNewIssue() from Magazine but does the same basic job (“fulfils the contract”) as the inherited version with respect to updating the number of copies and the current issue. While it extends that functionality in a way specifically relevant to DiscMags by displaying a reminder to check the cover discs, essentially these two methods perform the same operation.$NL$What do we know about a ‘Publication’?$NL$Answer: It’s an object which supports (at least) the operations:$NL$void SellCopy()$NL$String ToString()$NL$and it has properties that allow us to $NL$set the price,$NL$get the number of copies$NL$set the number of copies.$NL$Inheritance guarantees that objects of any subclass of Publications provides at least these. $NL$Note that a subclass can never remove an operation inherited from its superclass(es) – this would break the guarantee. Because subclasses extend the capabilities of their superclasses, the superclass functionality can be assumed. $NL$It is quite likely that we would choose to override the ToString() method (initially defined within ‘Object’) within Publication and override it again within Magazine so that the String returned provides a better description of Publications and Magazines. However we should not override the ToString() method in order to return the price – this would be changing the functionality of the method so that the method performs an inherently different function. Doing this would break the substitutability principle.$NL$Because an instance of a subclass is an instance of its superclass we can handle subclass objects as if they were superclass objects. Furthermore because a superclass guarantees certain operations in its subclasses we can invoke those operations without caring which subclass the actual object is an instance of.$NL$This characteristic is termed ‘polymorphism’, originally meaning ‘having multiple shapes’. $NL$Thus a Publication comes in various shapes … it could be a Book, Magazine or DiscMag. We can invoke the SellCopy() method on any of these Publications irrespective of their specific details.$NL$Polymorphism is a fancy name for a common idea. Someone who knows how to drive can get into and drive most cars because they have a set of shared key characteristics – steering wheel, gear stick, pedals for clutch, brake and accelerator etc – which the driver knows how to use. There will be lots of differences between any two cars, but you can think of them as subclasses of a superclass which defines these crucial shared ‘operations’.$NL$If ‘p’ ‘is a’ Publication, it might be a Book or a Magazine or a DiscMag.$NL$Whichever it is we know that it has a SellCopy() method.$NL$So we can invoke p.SellCopy() without worrying about what exactly ‘p’ is.$NL$This can make life a lot simpler when we are manipulating objects within an inheritance hierarchy. We can create new types of Publication e.g. a Newspaper and invoke p,SellCopy() on a Newspaper without have to create any functionality within the new class – all the functionality required is already defined in Publication.$NL$Polymorphism makes it very easy to extend the functionality of our programs as we will see now and we will see this again in the case study (in Chapter 11).$NL$Huge sums of money are spent annually creating new computer programs but over the years even more is spent changing and adapting those programs to meet the changing needs of an organisation. Thus as professional software engineers we have a duty to facilitate this and help to make those programs easier to maintain and adapt. Of course the application of good programming standards, commenting and layout etc, have a part to play here but also polymorphism can help as it allows programs to be made that are easily extended.
